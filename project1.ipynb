{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093bae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import cm\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from time import time\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bf6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 48\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 48, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(48)\n",
    "        self.layer1 = self._make_layer(block, 48, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 96, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 192, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 384, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(384, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def project1_model():\n",
    "    return ResNet(BasicBlock, [1, 2, 4, 1])\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999df676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=256, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2738cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4919674\n"
     ]
    }
   ],
   "source": [
    "net = project1_model()\n",
    "\n",
    "num = count_parameters(net)\n",
    "print(num)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "#CosineAnnealingLR(optimizer, T_max=20) \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "#train_dur_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "#test_dur_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a2c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "    t0 = time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        # loss = nll(outputs, targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_index = batch_idx\n",
    "        # train_loss += loss.item()\n",
    "        train_loss = loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        #print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     #% (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        #print(\"\\n\")\n",
    "    train_loss_list.append(train_loss/(batch_index+1))\n",
    "    train_acc_list.append(100.*correct/total)\n",
    "    print(' Train: Loss: %.6f | Acc: %.3f%% | Dur: %.2fS' \n",
    "          % (train_loss/(batch_index+1), 100.*correct/total, time() - t0))\n",
    "\n",
    "def test(epoch):\n",
    "#     global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "    t0 = time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # loss = nll(outputs, targets)\n",
    "            batch_index = batch_idx\n",
    "            # test_loss += loss.item()\n",
    "            test_loss = loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            #print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         #% (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            #print(\"\\n\")\n",
    "    test_loss_list.append(test_loss/(batch_index+1))\n",
    "    test_acc_list.append(100.*correct/total)\n",
    "    print(' Test:  Loss: %.6f | Acc: %.3f%% | Dur: %.2fS' \n",
    "          % (test_loss/(batch_index+1), 100.*correct/total, time() - t0))\n",
    "\n",
    "    # Save checkpoint.\n",
    "#     acc = 100.*correct/total\n",
    "#     if acc > best_acc:\n",
    "#         print('Saving..')\n",
    "#         state = {\n",
    "#             'net': net.state_dict(),\n",
    "#             'acc': acc,\n",
    "#             'epoch': epoch,\n",
    "#             'train_history': (train_loss_list, train_acc_list, test_loss_list, test_acc_list)\n",
    "#         }\n",
    "#         if not os.path.isdir('checkpoint'):\n",
    "#             os.mkdir('checkpoint')\n",
    "#         torch.save(state, './checkpoint/ckpt_rmsprop.pth')\n",
    "#         best_acc = acc\n",
    "\n",
    "\n",
    "def plot_model(train_loss, train_acc, test_loss, test_acc):\n",
    "    length = len(train_acc)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(length), train_acc)\n",
    "    plt.title('Train accuracy vs. epoches')\n",
    "    plt.ylabel('Train accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(length), train_loss)\n",
    "    plt.title('Train loss vs. epoches')\n",
    "    plt.ylabel('Train loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(length), test_acc)\n",
    "    plt.title('Test accuracy vs. epoches')\n",
    "    plt.ylabel('Test accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(length), test_loss)\n",
    "    plt.title('Test loss vs. epoches')\n",
    "    plt.ylabel('Test loss')\n",
    "    plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083e6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " Train: Loss: 0.006547 | Acc: 44.392% | Dur: 15.57S\n",
      " Test:  Loss: 0.147039 | Acc: 51.390% | Dur: 2.51S\n",
      "\n",
      "Epoch: 1\n",
      " Train: Loss: 0.005745 | Acc: 63.706% | Dur: 12.27S\n",
      " Test:  Loss: 0.096508 | Acc: 66.750% | Dur: 2.51S\n",
      "\n",
      "Epoch: 2\n",
      " Train: Loss: 0.003311 | Acc: 72.314% | Dur: 12.21S\n",
      " Test:  Loss: 0.085222 | Acc: 70.250% | Dur: 2.54S\n",
      "\n",
      "Epoch: 3\n",
      " Train: Loss: 0.002849 | Acc: 76.890% | Dur: 12.34S\n",
      " Test:  Loss: 0.069871 | Acc: 75.280% | Dur: 2.55S\n",
      "\n",
      "Epoch: 4\n",
      " Train: Loss: 0.002409 | Acc: 80.220% | Dur: 12.04S\n",
      " Test:  Loss: 0.067921 | Acc: 76.900% | Dur: 2.50S\n",
      "\n",
      "Epoch: 5\n",
      " Train: Loss: 0.002452 | Acc: 82.562% | Dur: 12.24S\n",
      " Test:  Loss: 0.057878 | Acc: 79.570% | Dur: 2.54S\n",
      "\n",
      "Epoch: 6\n",
      " Train: Loss: 0.001640 | Acc: 84.216% | Dur: 12.22S\n",
      " Test:  Loss: 0.056883 | Acc: 81.000% | Dur: 2.64S\n",
      "\n",
      "Epoch: 7\n",
      " Train: Loss: 0.002619 | Acc: 85.364% | Dur: 12.24S\n",
      " Test:  Loss: 0.044514 | Acc: 83.070% | Dur: 2.52S\n",
      "\n",
      "Epoch: 8\n",
      " Train: Loss: 0.002114 | Acc: 86.660% | Dur: 12.21S\n",
      " Test:  Loss: 0.055944 | Acc: 80.300% | Dur: 2.60S\n",
      "\n",
      "Epoch: 9\n",
      " Train: Loss: 0.001799 | Acc: 87.642% | Dur: 12.25S\n",
      " Test:  Loss: 0.045614 | Acc: 84.700% | Dur: 2.57S\n",
      "\n",
      "Epoch: 10\n",
      " Train: Loss: 0.002604 | Acc: 88.664% | Dur: 12.09S\n",
      " Test:  Loss: 0.039212 | Acc: 84.880% | Dur: 2.60S\n",
      "\n",
      "Epoch: 11\n",
      " Train: Loss: 0.002485 | Acc: 89.494% | Dur: 12.13S\n",
      " Test:  Loss: 0.048588 | Acc: 84.220% | Dur: 2.58S\n",
      "\n",
      "Epoch: 12\n",
      " Train: Loss: 0.001268 | Acc: 90.340% | Dur: 12.50S\n",
      " Test:  Loss: 0.045124 | Acc: 83.850% | Dur: 2.62S\n",
      "\n",
      "Epoch: 13\n",
      " Train: Loss: 0.001508 | Acc: 91.024% | Dur: 12.27S\n",
      " Test:  Loss: 0.044239 | Acc: 84.940% | Dur: 2.56S\n",
      "\n",
      "Epoch: 14\n",
      " Train: Loss: 0.001467 | Acc: 91.482% | Dur: 12.50S\n",
      " Test:  Loss: 0.039305 | Acc: 85.250% | Dur: 2.59S\n",
      "\n",
      "Epoch: 15\n",
      " Train: Loss: 0.001077 | Acc: 92.164% | Dur: 12.49S\n",
      " Test:  Loss: 0.041090 | Acc: 86.490% | Dur: 2.57S\n",
      "\n",
      "Epoch: 16\n",
      " Train: Loss: 0.001103 | Acc: 92.612% | Dur: 12.25S\n",
      " Test:  Loss: 0.040575 | Acc: 85.830% | Dur: 2.58S\n",
      "\n",
      "Epoch: 17\n",
      " Train: Loss: 0.001050 | Acc: 93.162% | Dur: 12.43S\n",
      " Test:  Loss: 0.039584 | Acc: 85.370% | Dur: 2.55S\n",
      "\n",
      "Epoch: 18\n",
      " Train: Loss: 0.001199 | Acc: 93.750% | Dur: 12.37S\n",
      " Test:  Loss: 0.040642 | Acc: 85.970% | Dur: 2.59S\n",
      "\n",
      "Epoch: 19\n",
      " Train: Loss: 0.000927 | Acc: 94.106% | Dur: 12.20S\n",
      " Test:  Loss: 0.034654 | Acc: 88.410% | Dur: 2.55S\n",
      "\n",
      "Epoch: 20\n",
      " Train: Loss: 0.001394 | Acc: 94.502% | Dur: 12.22S\n",
      " Test:  Loss: 0.032071 | Acc: 88.380% | Dur: 2.58S\n",
      "\n",
      "Epoch: 21\n",
      " Train: Loss: 0.001065 | Acc: 95.114% | Dur: 12.28S\n",
      " Test:  Loss: 0.038835 | Acc: 88.460% | Dur: 2.55S\n",
      "\n",
      "Epoch: 22\n",
      " Train: Loss: 0.000812 | Acc: 95.132% | Dur: 12.25S\n",
      " Test:  Loss: 0.031850 | Acc: 89.030% | Dur: 2.53S\n",
      "\n",
      "Epoch: 23\n",
      " Train: Loss: 0.000862 | Acc: 95.738% | Dur: 12.27S\n",
      " Test:  Loss: 0.044437 | Acc: 87.260% | Dur: 2.56S\n",
      "\n",
      "Epoch: 24\n",
      " Train: Loss: 0.000471 | Acc: 96.114% | Dur: 12.06S\n",
      " Test:  Loss: 0.035271 | Acc: 89.110% | Dur: 2.52S\n",
      "\n",
      "Epoch: 25\n",
      " Train: Loss: 0.000336 | Acc: 96.376% | Dur: 12.41S\n",
      " Test:  Loss: 0.038069 | Acc: 88.290% | Dur: 2.58S\n",
      "\n",
      "Epoch: 26\n",
      " Train: Loss: 0.000748 | Acc: 96.624% | Dur: 12.25S\n",
      " Test:  Loss: 0.039592 | Acc: 88.660% | Dur: 2.52S\n",
      "\n",
      "Epoch: 27\n",
      " Train: Loss: 0.000773 | Acc: 96.806% | Dur: 12.36S\n",
      " Test:  Loss: 0.040518 | Acc: 89.010% | Dur: 2.62S\n",
      "\n",
      "Epoch: 28\n",
      " Train: Loss: 0.000489 | Acc: 97.160% | Dur: 12.39S\n",
      " Test:  Loss: 0.041654 | Acc: 89.120% | Dur: 2.59S\n",
      "\n",
      "Epoch: 29\n",
      " Train: Loss: 0.000119 | Acc: 97.438% | Dur: 12.40S\n",
      " Test:  Loss: 0.035702 | Acc: 89.440% | Dur: 2.51S\n",
      "\n",
      "Epoch: 30\n",
      " Train: Loss: 0.000487 | Acc: 97.634% | Dur: 12.18S\n",
      " Test:  Loss: 0.038681 | Acc: 88.840% | Dur: 2.52S\n",
      "\n",
      "Epoch: 31\n",
      " Train: Loss: 0.000453 | Acc: 97.798% | Dur: 12.38S\n",
      " Test:  Loss: 0.036720 | Acc: 89.850% | Dur: 2.55S\n",
      "\n",
      "Epoch: 32\n",
      " Train: Loss: 0.000666 | Acc: 97.920% | Dur: 12.28S\n",
      " Test:  Loss: 0.037302 | Acc: 89.760% | Dur: 2.63S\n",
      "\n",
      "Epoch: 33\n",
      " Train: Loss: 0.000248 | Acc: 98.186% | Dur: 12.50S\n",
      " Test:  Loss: 0.040108 | Acc: 90.090% | Dur: 2.56S\n",
      "\n",
      "Epoch: 34\n",
      " Train: Loss: 0.000499 | Acc: 98.438% | Dur: 12.56S\n",
      " Test:  Loss: 0.036687 | Acc: 90.690% | Dur: 2.56S\n",
      "\n",
      "Epoch: 35\n",
      " Train: Loss: 0.000285 | Acc: 98.528% | Dur: 12.34S\n",
      " Test:  Loss: 0.042037 | Acc: 89.580% | Dur: 2.52S\n",
      "\n",
      "Epoch: 36\n",
      " Train: Loss: 0.000154 | Acc: 98.652% | Dur: 12.09S\n",
      " Test:  Loss: 0.040450 | Acc: 89.930% | Dur: 2.52S\n",
      "\n",
      "Epoch: 37\n",
      " Train: Loss: 0.000472 | Acc: 98.898% | Dur: 12.33S\n",
      " Test:  Loss: 0.040227 | Acc: 90.270% | Dur: 2.53S\n",
      "\n",
      "Epoch: 38\n",
      " Train: Loss: 0.000064 | Acc: 98.934% | Dur: 12.24S\n",
      " Test:  Loss: 0.042463 | Acc: 89.980% | Dur: 2.56S\n",
      "\n",
      "Epoch: 39\n",
      " Train: Loss: 0.000363 | Acc: 99.066% | Dur: 12.30S\n",
      " Test:  Loss: 0.044780 | Acc: 89.980% | Dur: 2.61S\n",
      "\n",
      "Epoch: 40\n",
      " Train: Loss: 0.000124 | Acc: 99.088% | Dur: 12.49S\n",
      " Test:  Loss: 0.042667 | Acc: 90.380% | Dur: 2.55S\n",
      "\n",
      "Epoch: 41\n",
      " Train: Loss: 0.000058 | Acc: 99.174% | Dur: 12.20S\n",
      " Test:  Loss: 0.042232 | Acc: 90.190% | Dur: 2.58S\n",
      "\n",
      "Epoch: 42\n",
      " Train: Loss: 0.000292 | Acc: 99.306% | Dur: 12.30S\n",
      " Test:  Loss: 0.042649 | Acc: 90.650% | Dur: 2.57S\n",
      "\n",
      "Epoch: 43\n",
      " Train: Loss: 0.000148 | Acc: 99.350% | Dur: 12.47S\n",
      " Test:  Loss: 0.041863 | Acc: 90.310% | Dur: 2.51S\n",
      "\n",
      "Epoch: 44\n",
      " Train: Loss: 0.000087 | Acc: 99.392% | Dur: 12.31S\n",
      " Test:  Loss: 0.040687 | Acc: 90.650% | Dur: 2.53S\n",
      "\n",
      "Epoch: 45\n",
      " Train: Loss: 0.000079 | Acc: 99.460% | Dur: 12.06S\n",
      " Test:  Loss: 0.042181 | Acc: 90.530% | Dur: 2.52S\n",
      "\n",
      "Epoch: 46\n",
      " Train: Loss: 0.000090 | Acc: 99.536% | Dur: 12.48S\n",
      " Test:  Loss: 0.043073 | Acc: 90.520% | Dur: 2.55S\n",
      "\n",
      "Epoch: 47\n",
      " Train: Loss: 0.000061 | Acc: 99.542% | Dur: 12.13S\n",
      " Test:  Loss: 0.042379 | Acc: 90.760% | Dur: 2.51S\n",
      "\n",
      "Epoch: 48\n",
      " Train: Loss: 0.000095 | Acc: 99.514% | Dur: 12.27S\n",
      " Test:  Loss: 0.042346 | Acc: 90.810% | Dur: 2.55S\n",
      "\n",
      "Epoch: 49\n",
      " Train: Loss: 0.000122 | Acc: 99.630% | Dur: 12.28S\n",
      " Test:  Loss: 0.042053 | Acc: 90.700% | Dur: 2.57S\n",
      "\n",
      "Epoch: 50\n",
      " Train: Loss: 0.000273 | Acc: 99.656% | Dur: 12.24S\n",
      " Test:  Loss: 0.040300 | Acc: 90.750% | Dur: 2.53S\n",
      "\n",
      "Epoch: 51\n",
      " Train: Loss: 0.000432 | Acc: 99.644% | Dur: 12.25S\n",
      " Test:  Loss: 0.041415 | Acc: 90.870% | Dur: 2.59S\n",
      "\n",
      "Epoch: 52\n",
      " Train: Loss: 0.000104 | Acc: 99.632% | Dur: 12.43S\n",
      " Test:  Loss: 0.040127 | Acc: 90.830% | Dur: 2.59S\n",
      "\n",
      "Epoch: 53\n",
      " Train: Loss: 0.000065 | Acc: 99.722% | Dur: 11.99S\n",
      " Test:  Loss: 0.040747 | Acc: 91.030% | Dur: 2.61S\n",
      "\n",
      "Epoch: 54\n",
      " Train: Loss: 0.000260 | Acc: 99.758% | Dur: 12.15S\n",
      " Test:  Loss: 0.041085 | Acc: 90.930% | Dur: 2.52S\n",
      "\n",
      "Epoch: 55\n",
      " Train: Loss: 0.000019 | Acc: 99.726% | Dur: 12.24S\n",
      " Test:  Loss: 0.042674 | Acc: 90.980% | Dur: 2.55S\n",
      "\n",
      "Epoch: 56\n",
      " Train: Loss: 0.000075 | Acc: 99.720% | Dur: 12.39S\n",
      " Test:  Loss: 0.042115 | Acc: 90.800% | Dur: 2.58S\n",
      "\n",
      "Epoch: 57\n",
      " Train: Loss: 0.000044 | Acc: 99.772% | Dur: 12.27S\n",
      " Test:  Loss: 0.042230 | Acc: 90.850% | Dur: 2.55S\n",
      "\n",
      "Epoch: 58\n",
      " Train: Loss: 0.000044 | Acc: 99.746% | Dur: 12.05S\n",
      " Test:  Loss: 0.041302 | Acc: 90.980% | Dur: 2.51S\n",
      "\n",
      "Epoch: 59\n",
      " Train: Loss: 0.000079 | Acc: 99.784% | Dur: 12.41S\n",
      " Test:  Loss: 0.042346 | Acc: 90.980% | Dur: 2.52S\n",
      "\n",
      "Epoch: 60\n",
      " Train: Loss: 0.000201 | Acc: 99.748% | Dur: 12.22S\n",
      " Test:  Loss: 0.042869 | Acc: 90.890% | Dur: 2.52S\n",
      "\n",
      "Epoch: 61\n",
      " Train: Loss: 0.000139 | Acc: 99.810% | Dur: 12.26S\n",
      " Test:  Loss: 0.042261 | Acc: 90.880% | Dur: 2.53S\n",
      "\n",
      "Epoch: 62\n",
      " Train: Loss: 0.000078 | Acc: 99.822% | Dur: 12.28S\n",
      " Test:  Loss: 0.041317 | Acc: 90.990% | Dur: 2.52S\n",
      "\n",
      "Epoch: 63\n",
      " Train: Loss: 0.000159 | Acc: 99.822% | Dur: 12.29S\n",
      " Test:  Loss: 0.041476 | Acc: 91.060% | Dur: 2.55S\n",
      "\n",
      "Epoch: 64\n",
      " Train: Loss: 0.000076 | Acc: 99.856% | Dur: 12.31S\n",
      " Test:  Loss: 0.042093 | Acc: 91.030% | Dur: 2.51S\n",
      "\n",
      "Epoch: 65\n",
      " Train: Loss: 0.000109 | Acc: 99.828% | Dur: 12.42S\n",
      " Test:  Loss: 0.042190 | Acc: 90.990% | Dur: 2.50S\n",
      "\n",
      "Epoch: 66\n",
      " Train: Loss: 0.000023 | Acc: 99.862% | Dur: 12.04S\n",
      " Test:  Loss: 0.041955 | Acc: 91.030% | Dur: 2.56S\n",
      "\n",
      "Epoch: 67\n",
      " Train: Loss: 0.000055 | Acc: 99.832% | Dur: 12.20S\n",
      " Test:  Loss: 0.043052 | Acc: 91.090% | Dur: 2.51S\n",
      "\n",
      "Epoch: 68\n",
      " Train: Loss: 0.000017 | Acc: 99.866% | Dur: 12.16S\n",
      " Test:  Loss: 0.042799 | Acc: 90.940% | Dur: 2.53S\n",
      "\n",
      "Epoch: 69\n",
      " Train: Loss: 0.000098 | Acc: 99.854% | Dur: 12.18S\n",
      " Test:  Loss: 0.042576 | Acc: 91.010% | Dur: 2.51S\n",
      "\n",
      "Epoch: 70\n",
      " Train: Loss: 0.000014 | Acc: 99.836% | Dur: 12.20S\n",
      " Test:  Loss: 0.042540 | Acc: 91.020% | Dur: 2.52S\n",
      "\n",
      "Epoch: 71\n",
      " Train: Loss: 0.000038 | Acc: 99.876% | Dur: 11.96S\n",
      " Test:  Loss: 0.042622 | Acc: 91.170% | Dur: 2.53S\n",
      "\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Loss: 0.000068 | Acc: 99.872% | Dur: 12.14S\n",
      " Test:  Loss: 0.042884 | Acc: 91.060% | Dur: 2.51S\n",
      "\n",
      "Epoch: 73\n",
      " Train: Loss: 0.000202 | Acc: 99.852% | Dur: 12.22S\n",
      " Test:  Loss: 0.042504 | Acc: 91.190% | Dur: 2.54S\n",
      "\n",
      "Epoch: 74\n",
      " Train: Loss: 0.000175 | Acc: 99.874% | Dur: 12.04S\n",
      " Test:  Loss: 0.042480 | Acc: 91.330% | Dur: 2.58S\n",
      "\n",
      "Epoch: 75\n",
      " Train: Loss: 0.000044 | Acc: 99.878% | Dur: 12.28S\n",
      " Test:  Loss: 0.043042 | Acc: 91.020% | Dur: 2.55S\n",
      "\n",
      "Epoch: 76\n",
      " Train: Loss: 0.000070 | Acc: 99.886% | Dur: 12.04S\n",
      " Test:  Loss: 0.043389 | Acc: 91.110% | Dur: 2.59S\n",
      "\n",
      "Epoch: 77\n",
      " Train: Loss: 0.000089 | Acc: 99.854% | Dur: 12.36S\n",
      " Test:  Loss: 0.043307 | Acc: 91.150% | Dur: 2.52S\n",
      "\n",
      "Epoch: 78\n",
      " Train: Loss: 0.000047 | Acc: 99.874% | Dur: 12.36S\n",
      " Test:  Loss: 0.043510 | Acc: 91.160% | Dur: 2.59S\n",
      "\n",
      "Epoch: 79\n",
      " Train: Loss: 0.000040 | Acc: 99.884% | Dur: 12.23S\n",
      " Test:  Loss: 0.042915 | Acc: 91.100% | Dur: 2.62S\n",
      "\n",
      "Epoch: 80\n",
      " Train: Loss: 0.000067 | Acc: 99.894% | Dur: 12.31S\n",
      " Test:  Loss: 0.042533 | Acc: 91.180% | Dur: 2.56S\n",
      "\n",
      "Epoch: 81\n",
      " Train: Loss: 0.000082 | Acc: 99.900% | Dur: 12.34S\n",
      " Test:  Loss: 0.042885 | Acc: 91.080% | Dur: 2.54S\n",
      "\n",
      "Epoch: 82\n",
      " Train: Loss: 0.000095 | Acc: 99.888% | Dur: 12.25S\n",
      " Test:  Loss: 0.042245 | Acc: 91.080% | Dur: 2.54S\n",
      "\n",
      "Epoch: 83\n",
      " Train: Loss: 0.000162 | Acc: 99.906% | Dur: 12.01S\n",
      " Test:  Loss: 0.043079 | Acc: 91.210% | Dur: 2.50S\n",
      "\n",
      "Epoch: 84\n",
      " Train: Loss: 0.000043 | Acc: 99.896% | Dur: 12.29S\n",
      " Test:  Loss: 0.042629 | Acc: 91.120% | Dur: 2.51S\n",
      "\n",
      "Epoch: 85\n",
      " Train: Loss: 0.000054 | Acc: 99.908% | Dur: 12.39S\n",
      " Test:  Loss: 0.042290 | Acc: 90.890% | Dur: 2.52S\n",
      "\n",
      "Epoch: 86\n",
      " Train: Loss: 0.000040 | Acc: 99.898% | Dur: 12.34S\n",
      " Test:  Loss: 0.042987 | Acc: 90.940% | Dur: 2.60S\n",
      "\n",
      "Epoch: 87\n",
      " Train: Loss: 0.000022 | Acc: 99.918% | Dur: 12.43S\n",
      " Test:  Loss: 0.042875 | Acc: 91.070% | Dur: 2.53S\n",
      "\n",
      "Epoch: 88\n",
      " Train: Loss: 0.000024 | Acc: 99.908% | Dur: 12.13S\n",
      " Test:  Loss: 0.042489 | Acc: 91.090% | Dur: 2.53S\n",
      "\n",
      "Epoch: 89\n",
      " Train: Loss: 0.000015 | Acc: 99.884% | Dur: 12.16S\n",
      " Test:  Loss: 0.042884 | Acc: 90.950% | Dur: 2.55S\n",
      "\n",
      "Epoch: 90\n",
      " Train: Loss: 0.000179 | Acc: 99.926% | Dur: 12.13S\n",
      " Test:  Loss: 0.043151 | Acc: 91.120% | Dur: 2.57S\n",
      "\n",
      "Epoch: 91\n",
      " Train: Loss: 0.000082 | Acc: 99.898% | Dur: 12.20S\n",
      " Test:  Loss: 0.043322 | Acc: 91.040% | Dur: 2.57S\n",
      "\n",
      "Epoch: 92\n",
      " Train: Loss: 0.000019 | Acc: 99.884% | Dur: 12.32S\n",
      " Test:  Loss: 0.043762 | Acc: 91.010% | Dur: 2.61S\n",
      "\n",
      "Epoch: 93\n",
      " Train: Loss: 0.000103 | Acc: 99.906% | Dur: 12.41S\n",
      " Test:  Loss: 0.043687 | Acc: 91.140% | Dur: 2.64S\n",
      "\n",
      "Epoch: 94\n",
      " Train: Loss: 0.000165 | Acc: 99.900% | Dur: 12.96S\n",
      " Test:  Loss: 0.043401 | Acc: 91.000% | Dur: 2.63S\n",
      "\n",
      "Epoch: 95\n",
      " Train: Loss: 0.000024 | Acc: 99.910% | Dur: 12.89S\n",
      " Test:  Loss: 0.043567 | Acc: 91.060% | Dur: 2.63S\n",
      "\n",
      "Epoch: 96\n",
      " Train: Loss: 0.000014 | Acc: 99.914% | Dur: 12.88S\n",
      " Test:  Loss: 0.042877 | Acc: 91.120% | Dur: 2.63S\n",
      "\n",
      "Epoch: 97\n",
      " Train: Loss: 0.000043 | Acc: 99.902% | Dur: 12.54S\n",
      " Test:  Loss: 0.043660 | Acc: 91.090% | Dur: 2.64S\n",
      "\n",
      "Epoch: 98\n",
      " Train: Loss: 0.000031 | Acc: 99.916% | Dur: 12.73S\n",
      " Test:  Loss: 0.043529 | Acc: 91.010% | Dur: 2.60S\n",
      "\n",
      "Epoch: 99\n",
      " Train: Loss: 0.000070 | Acc: 99.918% | Dur: 12.65S\n",
      " Test:  Loss: 0.043558 | Acc: 91.040% | Dur: 2.60S\n",
      "\n",
      "Epoch: 100\n",
      " Train: Loss: 0.000023 | Acc: 99.918% | Dur: 12.64S\n",
      " Test:  Loss: 0.043262 | Acc: 91.090% | Dur: 2.62S\n",
      "\n",
      "Epoch: 101\n",
      " Train: Loss: 0.000027 | Acc: 99.894% | Dur: 12.78S\n",
      " Test:  Loss: 0.043230 | Acc: 91.030% | Dur: 2.59S\n",
      "\n",
      "Epoch: 102\n",
      " Train: Loss: 0.000009 | Acc: 99.888% | Dur: 12.90S\n",
      " Test:  Loss: 0.043468 | Acc: 91.060% | Dur: 2.60S\n",
      "\n",
      "Epoch: 103\n",
      " Train: Loss: 0.000027 | Acc: 99.908% | Dur: 12.35S\n",
      " Test:  Loss: 0.043282 | Acc: 91.100% | Dur: 2.60S\n",
      "\n",
      "Epoch: 104\n",
      " Train: Loss: 0.000016 | Acc: 99.884% | Dur: 12.84S\n",
      " Test:  Loss: 0.043227 | Acc: 91.100% | Dur: 2.59S\n",
      "\n",
      "Epoch: 105\n",
      " Train: Loss: 0.000012 | Acc: 99.908% | Dur: 12.58S\n",
      " Test:  Loss: 0.043237 | Acc: 91.120% | Dur: 2.63S\n",
      "\n",
      "Epoch: 106\n",
      " Train: Loss: 0.000018 | Acc: 99.908% | Dur: 12.84S\n",
      " Test:  Loss: 0.043596 | Acc: 91.100% | Dur: 2.71S\n",
      "\n",
      "Epoch: 107\n",
      " Train: Loss: 0.000098 | Acc: 99.910% | Dur: 12.51S\n",
      " Test:  Loss: 0.043453 | Acc: 91.050% | Dur: 2.59S\n",
      "\n",
      "Epoch: 108\n",
      " Train: Loss: 0.000186 | Acc: 99.918% | Dur: 12.69S\n",
      " Test:  Loss: 0.043593 | Acc: 91.190% | Dur: 2.62S\n",
      "\n",
      "Epoch: 109\n",
      " Train: Loss: 0.000036 | Acc: 99.912% | Dur: 12.69S\n",
      " Test:  Loss: 0.043303 | Acc: 91.190% | Dur: 2.65S\n",
      "\n",
      "Epoch: 110\n",
      " Train: Loss: 0.000054 | Acc: 99.946% | Dur: 12.69S\n",
      " Test:  Loss: 0.043401 | Acc: 91.100% | Dur: 2.60S\n",
      "\n",
      "Epoch: 111\n",
      " Train: Loss: 0.000076 | Acc: 99.916% | Dur: 12.73S\n",
      " Test:  Loss: 0.043421 | Acc: 91.180% | Dur: 2.60S\n",
      "\n",
      "Epoch: 112\n",
      " Train: Loss: 0.000037 | Acc: 99.916% | Dur: 12.77S\n",
      " Test:  Loss: 0.043684 | Acc: 91.230% | Dur: 2.63S\n",
      "\n",
      "Epoch: 113\n",
      " Train: Loss: 0.000016 | Acc: 99.910% | Dur: 13.13S\n",
      " Test:  Loss: 0.043546 | Acc: 91.110% | Dur: 2.59S\n",
      "\n",
      "Epoch: 114\n",
      " Train: Loss: 0.000026 | Acc: 99.934% | Dur: 12.38S\n",
      " Test:  Loss: 0.043771 | Acc: 91.180% | Dur: 2.62S\n",
      "\n",
      "Epoch: 115\n",
      " Train: Loss: 0.000031 | Acc: 99.908% | Dur: 12.70S\n",
      " Test:  Loss: 0.043749 | Acc: 91.190% | Dur: 2.56S\n",
      "\n",
      "Epoch: 116\n",
      " Train: Loss: 0.000111 | Acc: 99.926% | Dur: 12.84S\n",
      " Test:  Loss: 0.044009 | Acc: 91.060% | Dur: 2.59S\n",
      "\n",
      "Epoch: 117\n",
      " Train: Loss: 0.000189 | Acc: 99.906% | Dur: 12.61S\n",
      " Test:  Loss: 0.043514 | Acc: 91.080% | Dur: 2.63S\n",
      "\n",
      "Epoch: 118\n",
      " Train: Loss: 0.000066 | Acc: 99.918% | Dur: 12.65S\n",
      " Test:  Loss: 0.043704 | Acc: 91.100% | Dur: 2.58S\n",
      "\n",
      "Epoch: 119\n",
      " Train: Loss: 0.000019 | Acc: 99.926% | Dur: 12.57S\n",
      " Test:  Loss: 0.043779 | Acc: 91.120% | Dur: 2.63S\n",
      "\n",
      "Epoch: 120\n",
      " Train: Loss: 0.000033 | Acc: 99.912% | Dur: 12.70S\n",
      " Test:  Loss: 0.043357 | Acc: 91.030% | Dur: 2.58S\n",
      "\n",
      "Epoch: 121\n",
      " Train: Loss: 0.000018 | Acc: 99.914% | Dur: 12.56S\n",
      " Test:  Loss: 0.043295 | Acc: 91.090% | Dur: 2.58S\n",
      "\n",
      "Epoch: 122\n",
      " Train: Loss: 0.000017 | Acc: 99.924% | Dur: 12.45S\n",
      " Test:  Loss: 0.043257 | Acc: 91.110% | Dur: 2.57S\n",
      "\n",
      "Epoch: 123\n",
      " Train: Loss: 0.000014 | Acc: 99.940% | Dur: 12.61S\n",
      " Test:  Loss: 0.043600 | Acc: 91.080% | Dur: 2.59S\n",
      "\n",
      "Epoch: 124\n",
      " Train: Loss: 0.000040 | Acc: 99.914% | Dur: 12.84S\n",
      " Test:  Loss: 0.043868 | Acc: 91.160% | Dur: 2.57S\n",
      "\n",
      "Epoch: 125\n",
      " Train: Loss: 0.000031 | Acc: 99.924% | Dur: 12.61S\n",
      " Test:  Loss: 0.043597 | Acc: 91.120% | Dur: 2.59S\n",
      "\n",
      "Epoch: 126\n",
      " Train: Loss: 0.000097 | Acc: 99.932% | Dur: 12.65S\n",
      " Test:  Loss: 0.043664 | Acc: 91.190% | Dur: 2.63S\n",
      "\n",
      "Epoch: 127\n",
      " Train: Loss: 0.000058 | Acc: 99.922% | Dur: 12.68S\n",
      " Test:  Loss: 0.044200 | Acc: 91.050% | Dur: 2.61S\n",
      "\n",
      "Epoch: 128\n",
      " Train: Loss: 0.000041 | Acc: 99.926% | Dur: 12.62S\n",
      " Test:  Loss: 0.043612 | Acc: 91.160% | Dur: 2.57S\n",
      "\n",
      "Epoch: 129\n",
      " Train: Loss: 0.000072 | Acc: 99.920% | Dur: 12.60S\n",
      " Test:  Loss: 0.043571 | Acc: 91.160% | Dur: 2.57S\n",
      "\n",
      "Epoch: 130\n",
      " Train: Loss: 0.000144 | Acc: 99.916% | Dur: 12.64S\n",
      " Test:  Loss: 0.043778 | Acc: 91.200% | Dur: 2.57S\n",
      "\n",
      "Epoch: 131\n",
      " Train: Loss: 0.000010 | Acc: 99.934% | Dur: 12.59S\n",
      " Test:  Loss: 0.043492 | Acc: 91.130% | Dur: 2.59S\n",
      "\n",
      "Epoch: 132\n",
      " Train: Loss: 0.000097 | Acc: 99.912% | Dur: 12.64S\n",
      " Test:  Loss: 0.043513 | Acc: 91.090% | Dur: 2.58S\n",
      "\n",
      "Epoch: 133\n",
      " Train: Loss: 0.000083 | Acc: 99.912% | Dur: 12.64S\n",
      " Test:  Loss: 0.043797 | Acc: 91.100% | Dur: 2.57S\n",
      "\n",
      "Epoch: 134\n",
      " Train: Loss: 0.000214 | Acc: 99.928% | Dur: 12.57S\n",
      " Test:  Loss: 0.043652 | Acc: 91.080% | Dur: 2.59S\n",
      "\n",
      "Epoch: 135\n",
      " Train: Loss: 0.000031 | Acc: 99.932% | Dur: 12.65S\n",
      " Test:  Loss: 0.043567 | Acc: 91.100% | Dur: 2.60S\n",
      "\n",
      "Epoch: 136\n",
      " Train: Loss: 0.000032 | Acc: 99.912% | Dur: 12.64S\n",
      " Test:  Loss: 0.043569 | Acc: 91.130% | Dur: 2.57S\n",
      "\n",
      "Epoch: 137\n",
      " Train: Loss: 0.000025 | Acc: 99.930% | Dur: 12.62S\n",
      " Test:  Loss: 0.043652 | Acc: 91.130% | Dur: 2.60S\n",
      "\n",
      "Epoch: 138\n",
      " Train: Loss: 0.000045 | Acc: 99.928% | Dur: 12.76S\n",
      " Test:  Loss: 0.043775 | Acc: 91.130% | Dur: 2.56S\n",
      "\n",
      "Epoch: 139\n",
      " Train: Loss: 0.000107 | Acc: 99.930% | Dur: 12.64S\n",
      " Test:  Loss: 0.043875 | Acc: 91.120% | Dur: 2.57S\n",
      "\n",
      "Epoch: 140\n",
      " Train: Loss: 0.000055 | Acc: 99.946% | Dur: 12.52S\n",
      " Test:  Loss: 0.043840 | Acc: 91.060% | Dur: 2.57S\n",
      "\n",
      "Epoch: 141\n",
      " Train: Loss: 0.000015 | Acc: 99.916% | Dur: 12.56S\n",
      " Test:  Loss: 0.043938 | Acc: 91.110% | Dur: 2.56S\n",
      "\n",
      "Epoch: 142\n",
      " Train: Loss: 0.000109 | Acc: 99.912% | Dur: 12.63S\n",
      " Test:  Loss: 0.043908 | Acc: 91.210% | Dur: 2.59S\n",
      "\n",
      "Epoch: 143\n",
      " Train: Loss: 0.000036 | Acc: 99.936% | Dur: 12.60S\n",
      " Test:  Loss: 0.043410 | Acc: 91.140% | Dur: 2.58S\n",
      "\n",
      "Epoch: 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Loss: 0.000045 | Acc: 99.916% | Dur: 12.63S\n",
      " Test:  Loss: 0.043618 | Acc: 91.140% | Dur: 2.59S\n",
      "\n",
      "Epoch: 145\n",
      " Train: Loss: 0.000041 | Acc: 99.926% | Dur: 12.60S\n",
      " Test:  Loss: 0.043992 | Acc: 91.170% | Dur: 2.63S\n",
      "\n",
      "Epoch: 146\n",
      " Train: Loss: 0.000194 | Acc: 99.908% | Dur: 12.64S\n",
      " Test:  Loss: 0.043661 | Acc: 91.030% | Dur: 2.58S\n",
      "\n",
      "Epoch: 147\n",
      " Train: Loss: 0.000048 | Acc: 99.922% | Dur: 12.64S\n",
      " Test:  Loss: 0.043707 | Acc: 91.180% | Dur: 2.62S\n",
      "\n",
      "Epoch: 148\n",
      " Train: Loss: 0.000053 | Acc: 99.912% | Dur: 12.65S\n",
      " Test:  Loss: 0.044049 | Acc: 91.170% | Dur: 2.57S\n",
      "\n",
      "Epoch: 149\n",
      " Train: Loss: 0.000041 | Acc: 99.908% | Dur: 12.62S\n",
      " Test:  Loss: 0.043749 | Acc: 91.100% | Dur: 2.60S\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(150):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_list)\n",
    "print(train_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81189086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc_list)\n",
    "print(test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68658828",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(train_loss_list,train_acc_list,test_loss_list,test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82018dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './project1_model.pt'\n",
    "torch.save(net.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
