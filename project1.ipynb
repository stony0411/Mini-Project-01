{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093bae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import cm\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from time import time\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bf6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def project1_model():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# def test():\n",
    "#     net = ResNet18()\n",
    "#     y = net(torch.randn(1, 3, 32, 32))\n",
    "#     print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999df676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=2000, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2738cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = project1_model()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "#train_dur_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "#test_dur_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a2c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "    t0 = time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        # loss = nll(outputs, targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_index = batch_idx\n",
    "        # train_loss += loss.item()\n",
    "        train_loss = loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        #print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     #% (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        #print(\"\\n\")\n",
    "    train_loss_list.append(train_loss/(batch_index+1))\n",
    "    train_acc_list.append(100.*correct/total)\n",
    "    print(' Train: Loss: %.6f | Acc: %.3f%% | Dur: %.2fS' \n",
    "          % (train_loss/(batch_index+1), 100.*correct/total, time() - t0))\n",
    "\n",
    "def test(epoch):\n",
    "#     global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "    t0 = time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # loss = nll(outputs, targets)\n",
    "            batch_index = batch_idx\n",
    "            # test_loss += loss.item()\n",
    "            test_loss = loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            #print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         #% (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            #print(\"\\n\")\n",
    "    test_loss_list.append(test_loss/(batch_index+1))\n",
    "    test_acc_list.append(100.*correct/total)\n",
    "    print(' Test:  Loss: %.6f | Acc: %.3f%% | Dur: %.2fS' \n",
    "          % (test_loss/(batch_index+1), 100.*correct/total, time() - t0))\n",
    "\n",
    "    # Save checkpoint.\n",
    "#     acc = 100.*correct/total\n",
    "#     if acc > best_acc:\n",
    "#         print('Saving..')\n",
    "#         state = {\n",
    "#             'net': net.state_dict(),\n",
    "#             'acc': acc,\n",
    "#             'epoch': epoch,\n",
    "#             'train_history': (train_loss_list, train_acc_list, test_loss_list, test_acc_list)\n",
    "#         }\n",
    "#         if not os.path.isdir('checkpoint'):\n",
    "#             os.mkdir('checkpoint')\n",
    "#         torch.save(state, './checkpoint/ckpt_rmsprop.pth')\n",
    "#         best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083e6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " Train: Loss: 0.078632 | Acc: 23.000% | Dur: 22.16S\n",
      " Test:  Loss: 0.247890 | Acc: 10.470% | Dur: 2.76S\n",
      "\n",
      "Epoch: 1\n",
      " Train: Loss: 0.070706 | Acc: 32.062% | Dur: 14.19S\n",
      " Test:  Loss: 0.177739 | Acc: 33.970% | Dur: 2.85S\n",
      "\n",
      "Epoch: 2\n",
      " Train: Loss: 0.067986 | Acc: 35.880% | Dur: 14.20S\n",
      " Test:  Loss: 0.186274 | Acc: 33.710% | Dur: 3.43S\n",
      "\n",
      "Epoch: 3\n",
      " Train: Loss: 0.065586 | Acc: 38.674% | Dur: 34.12S\n",
      " Test:  Loss: 0.162675 | Acc: 40.180% | Dur: 3.66S\n",
      "\n",
      "Epoch: 4\n",
      " Train: Loss: 0.064680 | Acc: 40.974% | Dur: 28.66S\n",
      " Test:  Loss: 0.154118 | Acc: 41.940% | Dur: 2.98S\n",
      "\n",
      "Epoch: 5\n",
      " Train: Loss: 0.061038 | Acc: 43.374% | Dur: 23.86S\n",
      " Test:  Loss: 0.171472 | Acc: 39.810% | Dur: 4.10S\n",
      "\n",
      "Epoch: 6\n",
      " Train: Loss: 0.057266 | Acc: 45.082% | Dur: 22.70S\n",
      " Test:  Loss: 0.148321 | Acc: 45.230% | Dur: 2.92S\n",
      "\n",
      "Epoch: 7\n",
      " Train: Loss: 0.056645 | Acc: 46.856% | Dur: 14.33S\n",
      " Test:  Loss: 0.156389 | Acc: 45.070% | Dur: 2.80S\n",
      "\n",
      "Epoch: 8\n",
      " Train: Loss: 0.055760 | Acc: 48.698% | Dur: 14.29S\n",
      " Test:  Loss: 0.147283 | Acc: 48.130% | Dur: 2.87S\n",
      "\n",
      "Epoch: 9\n",
      " Train: Loss: 0.054987 | Acc: 50.268% | Dur: 14.29S\n",
      " Test:  Loss: 0.137273 | Acc: 49.380% | Dur: 2.89S\n",
      "\n",
      "Epoch: 10\n",
      " Train: Loss: 0.052645 | Acc: 51.932% | Dur: 14.27S\n",
      " Test:  Loss: 0.134538 | Acc: 51.010% | Dur: 2.88S\n",
      "\n",
      "Epoch: 11\n",
      " Train: Loss: 0.052570 | Acc: 52.678% | Dur: 14.29S\n",
      " Test:  Loss: 0.172263 | Acc: 43.990% | Dur: 2.87S\n",
      "\n",
      "Epoch: 12\n",
      " Train: Loss: 0.049748 | Acc: 54.838% | Dur: 14.30S\n",
      " Test:  Loss: 0.134306 | Acc: 51.950% | Dur: 2.79S\n",
      "\n",
      "Epoch: 13\n",
      " Train: Loss: 0.048576 | Acc: 55.492% | Dur: 14.26S\n",
      " Test:  Loss: 0.130101 | Acc: 54.530% | Dur: 2.80S\n",
      "\n",
      "Epoch: 14\n",
      " Train: Loss: 0.047531 | Acc: 56.464% | Dur: 14.30S\n",
      " Test:  Loss: 0.124789 | Acc: 55.060% | Dur: 2.84S\n",
      "\n",
      "Epoch: 15\n",
      " Train: Loss: 0.048348 | Acc: 57.178% | Dur: 14.27S\n",
      " Test:  Loss: 0.125769 | Acc: 53.900% | Dur: 2.84S\n",
      "\n",
      "Epoch: 16\n",
      " Train: Loss: 0.043675 | Acc: 58.190% | Dur: 14.30S\n",
      " Test:  Loss: 0.114933 | Acc: 57.840% | Dur: 2.86S\n",
      "\n",
      "Epoch: 17\n",
      " Train: Loss: 0.044621 | Acc: 59.394% | Dur: 14.26S\n",
      " Test:  Loss: 0.119396 | Acc: 55.980% | Dur: 2.86S\n",
      "\n",
      "Epoch: 18\n",
      " Train: Loss: 0.044874 | Acc: 59.654% | Dur: 14.32S\n",
      " Test:  Loss: 0.115925 | Acc: 57.950% | Dur: 2.82S\n",
      "\n",
      "Epoch: 19\n",
      " Train: Loss: 0.042280 | Acc: 61.142% | Dur: 14.27S\n",
      " Test:  Loss: 0.120109 | Acc: 56.880% | Dur: 2.83S\n",
      "\n",
      "Epoch: 20\n",
      " Train: Loss: 0.040500 | Acc: 61.326% | Dur: 14.26S\n",
      " Test:  Loss: 0.107093 | Acc: 60.610% | Dur: 2.79S\n",
      "\n",
      "Epoch: 21\n",
      " Train: Loss: 0.040643 | Acc: 62.036% | Dur: 14.32S\n",
      " Test:  Loss: 0.103115 | Acc: 61.950% | Dur: 2.84S\n",
      "\n",
      "Epoch: 22\n",
      " Train: Loss: 0.041815 | Acc: 62.260% | Dur: 14.26S\n",
      " Test:  Loss: 0.105359 | Acc: 61.240% | Dur: 2.82S\n",
      "\n",
      "Epoch: 23\n",
      " Train: Loss: 0.039557 | Acc: 63.474% | Dur: 14.29S\n",
      " Test:  Loss: 0.107665 | Acc: 61.190% | Dur: 2.81S\n",
      "\n",
      "Epoch: 24\n",
      " Train: Loss: 0.038516 | Acc: 64.452% | Dur: 14.28S\n",
      " Test:  Loss: 0.102643 | Acc: 62.330% | Dur: 2.78S\n",
      "\n",
      "Epoch: 25\n",
      " Train: Loss: 0.038495 | Acc: 64.942% | Dur: 14.26S\n",
      " Test:  Loss: 0.099099 | Acc: 63.500% | Dur: 2.82S\n",
      "\n",
      "Epoch: 26\n",
      " Train: Loss: 0.037936 | Acc: 65.542% | Dur: 14.25S\n",
      " Test:  Loss: 0.094142 | Acc: 65.140% | Dur: 2.83S\n",
      "\n",
      "Epoch: 27\n",
      " Train: Loss: 0.037525 | Acc: 66.142% | Dur: 14.26S\n",
      " Test:  Loss: 0.109557 | Acc: 61.190% | Dur: 2.85S\n",
      "\n",
      "Epoch: 28\n",
      " Train: Loss: 0.038404 | Acc: 66.562% | Dur: 14.29S\n",
      " Test:  Loss: 0.095189 | Acc: 65.770% | Dur: 2.84S\n",
      "\n",
      "Epoch: 29\n",
      " Train: Loss: 0.037106 | Acc: 67.372% | Dur: 14.28S\n",
      " Test:  Loss: 0.093492 | Acc: 65.590% | Dur: 2.82S\n",
      "\n",
      "Epoch: 30\n",
      " Train: Loss: 0.036998 | Acc: 67.508% | Dur: 14.28S\n",
      " Test:  Loss: 0.095122 | Acc: 65.180% | Dur: 2.85S\n",
      "\n",
      "Epoch: 31\n",
      " Train: Loss: 0.036386 | Acc: 68.338% | Dur: 14.30S\n",
      " Test:  Loss: 0.102600 | Acc: 64.240% | Dur: 2.80S\n",
      "\n",
      "Epoch: 32\n",
      " Train: Loss: 0.036912 | Acc: 68.474% | Dur: 14.29S\n",
      " Test:  Loss: 0.117015 | Acc: 59.650% | Dur: 2.80S\n",
      "\n",
      "Epoch: 33\n",
      " Train: Loss: 0.035104 | Acc: 69.294% | Dur: 14.28S\n",
      " Test:  Loss: 0.114619 | Acc: 61.630% | Dur: 2.83S\n",
      "\n",
      "Epoch: 34\n",
      " Train: Loss: 0.035551 | Acc: 69.736% | Dur: 14.28S\n",
      " Test:  Loss: 0.099083 | Acc: 63.670% | Dur: 2.78S\n",
      "\n",
      "Epoch: 35\n",
      " Train: Loss: 0.033559 | Acc: 70.222% | Dur: 14.34S\n",
      " Test:  Loss: 0.092426 | Acc: 66.980% | Dur: 2.81S\n",
      "\n",
      "Epoch: 36\n",
      " Train: Loss: 0.033506 | Acc: 70.702% | Dur: 14.27S\n",
      " Test:  Loss: 0.095996 | Acc: 66.230% | Dur: 2.83S\n",
      "\n",
      "Epoch: 37\n",
      " Train: Loss: 0.032567 | Acc: 71.172% | Dur: 14.30S\n",
      " Test:  Loss: 0.087395 | Acc: 67.730% | Dur: 2.76S\n",
      "\n",
      "Epoch: 38\n",
      " Train: Loss: 0.034063 | Acc: 71.332% | Dur: 14.28S\n",
      " Test:  Loss: 0.089021 | Acc: 67.450% | Dur: 2.88S\n",
      "\n",
      "Epoch: 39\n",
      " Train: Loss: 0.029875 | Acc: 72.040% | Dur: 14.30S\n",
      " Test:  Loss: 0.094991 | Acc: 66.380% | Dur: 2.83S\n",
      "\n",
      "Epoch: 40\n",
      " Train: Loss: 0.030240 | Acc: 72.244% | Dur: 14.29S\n",
      " Test:  Loss: 0.087578 | Acc: 68.320% | Dur: 2.83S\n",
      "\n",
      "Epoch: 41\n",
      " Train: Loss: 0.030204 | Acc: 72.746% | Dur: 14.49S\n",
      " Test:  Loss: 0.090136 | Acc: 67.760% | Dur: 2.94S\n",
      "\n",
      "Epoch: 42\n",
      " Train: Loss: 0.029678 | Acc: 73.222% | Dur: 14.68S\n",
      " Test:  Loss: 0.086292 | Acc: 69.040% | Dur: 2.80S\n",
      "\n",
      "Epoch: 43\n",
      " Train: Loss: 0.028791 | Acc: 73.110% | Dur: 14.29S\n",
      " Test:  Loss: 0.077589 | Acc: 71.430% | Dur: 2.83S\n",
      "\n",
      "Epoch: 44\n",
      " Train: Loss: 0.029061 | Acc: 73.974% | Dur: 14.25S\n",
      " Test:  Loss: 0.079769 | Acc: 70.550% | Dur: 2.81S\n",
      "\n",
      "Epoch: 45\n",
      " Train: Loss: 0.027568 | Acc: 74.826% | Dur: 14.27S\n",
      " Test:  Loss: 0.078394 | Acc: 71.780% | Dur: 2.80S\n",
      "\n",
      "Epoch: 46\n",
      " Train: Loss: 0.027398 | Acc: 74.744% | Dur: 14.30S\n",
      " Test:  Loss: 0.093579 | Acc: 67.690% | Dur: 2.90S\n",
      "\n",
      "Epoch: 47\n",
      " Train: Loss: 0.027682 | Acc: 75.014% | Dur: 14.27S\n",
      " Test:  Loss: 0.090743 | Acc: 68.690% | Dur: 2.85S\n",
      "\n",
      "Epoch: 48\n",
      " Train: Loss: 0.025199 | Acc: 75.358% | Dur: 14.28S\n",
      " Test:  Loss: 0.084789 | Acc: 71.080% | Dur: 2.83S\n",
      "\n",
      "Epoch: 49\n",
      " Train: Loss: 0.025716 | Acc: 75.640% | Dur: 14.34S\n",
      " Test:  Loss: 0.082572 | Acc: 70.940% | Dur: 2.84S\n",
      "\n",
      "Epoch: 50\n",
      " Train: Loss: 0.026360 | Acc: 76.028% | Dur: 14.25S\n",
      " Test:  Loss: 0.073846 | Acc: 73.650% | Dur: 2.83S\n",
      "\n",
      "Epoch: 51\n",
      " Train: Loss: 0.025908 | Acc: 76.328% | Dur: 14.29S\n",
      " Test:  Loss: 0.077635 | Acc: 72.500% | Dur: 2.81S\n",
      "\n",
      "Epoch: 52\n",
      " Train: Loss: 0.027236 | Acc: 76.434% | Dur: 14.25S\n",
      " Test:  Loss: 0.093379 | Acc: 69.460% | Dur: 2.78S\n",
      "\n",
      "Epoch: 53\n",
      " Train: Loss: 0.025874 | Acc: 77.178% | Dur: 14.26S\n",
      " Test:  Loss: 0.074194 | Acc: 73.720% | Dur: 2.93S\n",
      "\n",
      "Epoch: 54\n",
      " Train: Loss: 0.025875 | Acc: 77.054% | Dur: 14.26S\n",
      " Test:  Loss: 0.078348 | Acc: 73.450% | Dur: 2.81S\n",
      "\n",
      "Epoch: 55\n",
      " Train: Loss: 0.024533 | Acc: 77.986% | Dur: 14.28S\n",
      " Test:  Loss: 0.075567 | Acc: 73.610% | Dur: 2.90S\n",
      "\n",
      "Epoch: 56\n",
      " Train: Loss: 0.026128 | Acc: 77.746% | Dur: 14.33S\n",
      " Test:  Loss: 0.076565 | Acc: 72.890% | Dur: 2.85S\n",
      "\n",
      "Epoch: 57\n",
      " Train: Loss: 0.024119 | Acc: 78.894% | Dur: 14.29S\n",
      " Test:  Loss: 0.078520 | Acc: 72.870% | Dur: 2.84S\n",
      "\n",
      "Epoch: 58\n",
      " Train: Loss: 0.022102 | Acc: 78.536% | Dur: 14.25S\n",
      " Test:  Loss: 0.067929 | Acc: 76.420% | Dur: 2.81S\n",
      "\n",
      "Epoch: 59\n",
      " Train: Loss: 0.023830 | Acc: 79.120% | Dur: 14.27S\n",
      " Test:  Loss: 0.073978 | Acc: 74.020% | Dur: 2.88S\n",
      "\n",
      "Epoch: 60\n",
      " Train: Loss: 0.021835 | Acc: 79.098% | Dur: 14.27S\n",
      " Test:  Loss: 0.067535 | Acc: 76.480% | Dur: 2.80S\n",
      "\n",
      "Epoch: 61\n",
      " Train: Loss: 0.021777 | Acc: 79.412% | Dur: 14.30S\n",
      " Test:  Loss: 0.067682 | Acc: 75.720% | Dur: 2.82S\n",
      "\n",
      "Epoch: 62\n",
      " Train: Loss: 0.022093 | Acc: 79.684% | Dur: 14.27S\n",
      " Test:  Loss: 0.072950 | Acc: 75.010% | Dur: 2.81S\n",
      "\n",
      "Epoch: 63\n",
      " Train: Loss: 0.024202 | Acc: 79.976% | Dur: 14.30S\n",
      " Test:  Loss: 0.070271 | Acc: 76.050% | Dur: 2.79S\n",
      "\n",
      "Epoch: 64\n",
      " Train: Loss: 0.022089 | Acc: 80.204% | Dur: 14.27S\n",
      " Test:  Loss: 0.064343 | Acc: 77.220% | Dur: 2.88S\n",
      "\n",
      "Epoch: 65\n",
      " Train: Loss: 0.021226 | Acc: 80.548% | Dur: 14.25S\n",
      " Test:  Loss: 0.064293 | Acc: 77.230% | Dur: 2.85S\n",
      "\n",
      "Epoch: 66\n",
      " Train: Loss: 0.022498 | Acc: 80.826% | Dur: 14.27S\n",
      " Test:  Loss: 0.070668 | Acc: 76.040% | Dur: 2.82S\n",
      "\n",
      "Epoch: 67\n",
      " Train: Loss: 0.021575 | Acc: 80.964% | Dur: 14.26S\n",
      " Test:  Loss: 0.068872 | Acc: 76.190% | Dur: 2.85S\n",
      "\n",
      "Epoch: 68\n",
      " Train: Loss: 0.021420 | Acc: 81.266% | Dur: 14.27S\n",
      " Test:  Loss: 0.076729 | Acc: 75.190% | Dur: 2.82S\n",
      "\n",
      "Epoch: 69\n",
      " Train: Loss: 0.021068 | Acc: 81.096% | Dur: 14.28S\n",
      " Test:  Loss: 0.067373 | Acc: 76.650% | Dur: 2.83S\n",
      "\n",
      "Epoch: 70\n",
      " Train: Loss: 0.022199 | Acc: 81.564% | Dur: 14.34S\n",
      " Test:  Loss: 0.065659 | Acc: 77.290% | Dur: 2.80S\n",
      "\n",
      "Epoch: 71\n",
      " Train: Loss: 0.019724 | Acc: 81.840% | Dur: 14.29S\n",
      " Test:  Loss: 0.066431 | Acc: 77.490% | Dur: 2.82S\n",
      "\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Loss: 0.019656 | Acc: 82.026% | Dur: 14.25S\n",
      " Test:  Loss: 0.060232 | Acc: 78.730% | Dur: 2.79S\n",
      "\n",
      "Epoch: 73\n",
      " Train: Loss: 0.018534 | Acc: 82.514% | Dur: 14.24S\n",
      " Test:  Loss: 0.070551 | Acc: 76.860% | Dur: 2.89S\n",
      "\n",
      "Epoch: 74\n",
      " Train: Loss: 0.020764 | Acc: 82.078% | Dur: 14.30S\n",
      " Test:  Loss: 0.066978 | Acc: 77.470% | Dur: 2.79S\n",
      "\n",
      "Epoch: 75\n",
      " Train: Loss: 0.018775 | Acc: 82.594% | Dur: 14.28S\n",
      " Test:  Loss: 0.064005 | Acc: 78.620% | Dur: 2.83S\n",
      "\n",
      "Epoch: 76\n",
      " Train: Loss: 0.019220 | Acc: 82.852% | Dur: 14.29S\n",
      " Test:  Loss: 0.063749 | Acc: 77.570% | Dur: 2.80S\n",
      "\n",
      "Epoch: 77\n",
      " Train: Loss: 0.019222 | Acc: 83.144% | Dur: 14.36S\n",
      " Test:  Loss: 0.062767 | Acc: 78.640% | Dur: 2.83S\n",
      "\n",
      "Epoch: 78\n",
      " Train: Loss: 0.019233 | Acc: 83.360% | Dur: 14.27S\n",
      " Test:  Loss: 0.061030 | Acc: 79.180% | Dur: 2.83S\n",
      "\n",
      "Epoch: 79\n",
      " Train: Loss: 0.018866 | Acc: 83.274% | Dur: 14.26S\n",
      " Test:  Loss: 0.058249 | Acc: 79.420% | Dur: 2.84S\n",
      "\n",
      "Epoch: 80\n",
      " Train: Loss: 0.018120 | Acc: 83.338% | Dur: 14.29S\n",
      " Test:  Loss: 0.057283 | Acc: 79.630% | Dur: 2.88S\n",
      "\n",
      "Epoch: 81\n",
      " Train: Loss: 0.019286 | Acc: 83.618% | Dur: 14.30S\n",
      " Test:  Loss: 0.061387 | Acc: 78.950% | Dur: 2.82S\n",
      "\n",
      "Epoch: 82\n",
      " Train: Loss: 0.018859 | Acc: 83.926% | Dur: 14.64S\n",
      " Test:  Loss: 0.064547 | Acc: 78.140% | Dur: 2.86S\n",
      "\n",
      "Epoch: 83\n",
      " Train: Loss: 0.018340 | Acc: 84.078% | Dur: 14.71S\n",
      " Test:  Loss: 0.066756 | Acc: 77.990% | Dur: 2.91S\n",
      "\n",
      "Epoch: 84\n",
      " Train: Loss: 0.017413 | Acc: 84.470% | Dur: 15.69S\n",
      " Test:  Loss: 0.059960 | Acc: 79.660% | Dur: 3.59S\n",
      "\n",
      "Epoch: 85\n",
      " Train: Loss: 0.017631 | Acc: 84.716% | Dur: 17.06S\n",
      " Test:  Loss: 0.057999 | Acc: 80.110% | Dur: 3.01S\n",
      "\n",
      "Epoch: 86\n",
      " Train: Loss: 0.017134 | Acc: 84.288% | Dur: 17.21S\n",
      " Test:  Loss: 0.061394 | Acc: 79.830% | Dur: 3.00S\n",
      "\n",
      "Epoch: 87\n",
      " Train: Loss: 0.016745 | Acc: 84.810% | Dur: 16.78S\n",
      " Test:  Loss: 0.063498 | Acc: 78.990% | Dur: 2.96S\n",
      "\n",
      "Epoch: 88\n",
      " Train: Loss: 0.016359 | Acc: 85.100% | Dur: 16.88S\n",
      " Test:  Loss: 0.061275 | Acc: 78.550% | Dur: 2.97S\n",
      "\n",
      "Epoch: 89\n",
      " Train: Loss: 0.017611 | Acc: 85.092% | Dur: 16.46S\n",
      " Test:  Loss: 0.062498 | Acc: 79.580% | Dur: 2.86S\n",
      "\n",
      "Epoch: 90\n",
      " Train: Loss: 0.016312 | Acc: 85.136% | Dur: 15.47S\n",
      " Test:  Loss: 0.054193 | Acc: 81.010% | Dur: 2.85S\n",
      "\n",
      "Epoch: 91\n",
      " Train: Loss: 0.016817 | Acc: 85.774% | Dur: 15.48S\n",
      " Test:  Loss: 0.062711 | Acc: 79.060% | Dur: 2.85S\n",
      "\n",
      "Epoch: 92\n",
      " Train: Loss: 0.016832 | Acc: 85.670% | Dur: 16.03S\n",
      " Test:  Loss: 0.078752 | Acc: 76.250% | Dur: 2.93S\n",
      "\n",
      "Epoch: 93\n",
      " Train: Loss: 0.015245 | Acc: 85.574% | Dur: 15.68S\n",
      " Test:  Loss: 0.057772 | Acc: 80.910% | Dur: 2.90S\n",
      "\n",
      "Epoch: 94\n",
      " Train: Loss: 0.015494 | Acc: 85.768% | Dur: 15.63S\n",
      " Test:  Loss: 0.058651 | Acc: 79.460% | Dur: 2.89S\n",
      "\n",
      "Epoch: 95\n",
      " Train: Loss: 0.016355 | Acc: 85.956% | Dur: 15.93S\n",
      " Test:  Loss: 0.056666 | Acc: 80.710% | Dur: 2.88S\n",
      "\n",
      "Epoch: 96\n",
      " Train: Loss: 0.017266 | Acc: 86.322% | Dur: 15.70S\n",
      " Test:  Loss: 0.067368 | Acc: 79.100% | Dur: 2.88S\n",
      "\n",
      "Epoch: 97\n",
      " Train: Loss: 0.016439 | Acc: 86.272% | Dur: 15.88S\n",
      " Test:  Loss: 0.057300 | Acc: 79.750% | Dur: 2.90S\n",
      "\n",
      "Epoch: 98\n",
      " Train: Loss: 0.014936 | Acc: 86.396% | Dur: 15.74S\n",
      " Test:  Loss: 0.054788 | Acc: 80.850% | Dur: 2.94S\n",
      "\n",
      "Epoch: 99\n",
      " Train: Loss: 0.015535 | Acc: 86.358% | Dur: 15.57S\n",
      " Test:  Loss: 0.055988 | Acc: 80.270% | Dur: 2.93S\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c8ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.0, 32.062, 35.88, 38.674, 40.974, 43.374, 45.082, 46.856, 48.698, 50.268, 51.932, 52.678, 54.838, 55.492, 56.464, 57.178, 58.19, 59.394, 59.654, 61.142, 61.326, 62.036, 62.26, 63.474, 64.452, 64.942, 65.542, 66.142, 66.562, 67.372, 67.508, 68.338, 68.474, 69.294, 69.736, 70.222, 70.702, 71.172, 71.332, 72.04, 72.244, 72.746, 73.222, 73.11, 73.974, 74.826, 74.744, 75.014, 75.358, 75.64, 76.028, 76.328, 76.434, 77.178, 77.054, 77.986, 77.746, 78.894, 78.536, 79.12, 79.098, 79.412, 79.684, 79.976, 80.204, 80.548, 80.826, 80.964, 81.266, 81.096, 81.564, 81.84, 82.026, 82.514, 82.078, 82.594, 82.852, 83.144, 83.36, 83.274, 83.338, 83.618, 83.926, 84.078, 84.47, 84.716, 84.288, 84.81, 85.1, 85.092, 85.136, 85.774, 85.67, 85.574, 85.768, 85.956, 86.322, 86.272, 86.396, 86.358]\n",
      "[0.07863185882568359, 0.0707058334350586, 0.06798649787902832, 0.06558572769165039, 0.06468032360076904, 0.061038212776184084, 0.057265644073486326, 0.056644749641418454, 0.05576033592224121, 0.05498651027679444, 0.05264454841613769, 0.05257030963897705, 0.04974825382232666, 0.04857631683349609, 0.04753073215484619, 0.048348093032836915, 0.0436748456954956, 0.04462134838104248, 0.04487419128417969, 0.04228043079376221, 0.04050019264221191, 0.040642805099487304, 0.04181492328643799, 0.03955710411071777, 0.03851609706878662, 0.03849510431289673, 0.03793619632720947, 0.037524662017822265, 0.03840357303619385, 0.037106380462646485, 0.03699802160263062, 0.03638580322265625, 0.036912460327148434, 0.03510436773300171, 0.035551464557647704, 0.03355903625488281, 0.033505640029907226, 0.032566561698913574, 0.03406266450881958, 0.029874682426452637, 0.030240447521209718, 0.03020378589630127, 0.029678256511688234, 0.02879125118255615, 0.02906115770339966, 0.02756828784942627, 0.02739781379699707, 0.027682363986968994, 0.025198912620544432, 0.02571577787399292, 0.026360325813293457, 0.025908119678497314, 0.02723557472229004, 0.02587371826171875, 0.02587486743927002, 0.02453281879425049, 0.026128392219543457, 0.024118516445159912, 0.022101948261260985, 0.02383044481277466, 0.021834821701049806, 0.021776952743530274, 0.022092773914337158, 0.02420212507247925, 0.0220889949798584, 0.021226089000701904, 0.022498111724853515, 0.021575477123260498, 0.02141986131668091, 0.021067802906036378, 0.022199227809906005, 0.019723931550979613, 0.01965645670890808, 0.0185335636138916, 0.020763792991638184, 0.018775198459625244, 0.019220219850540163, 0.01922159790992737, 0.019232780933380128, 0.018866034746170043, 0.018120028972625733, 0.019286104440689088, 0.01885881781578064, 0.01834018111228943, 0.01741308331489563, 0.017631022930145263, 0.01713442325592041, 0.016744831800460814, 0.01635852813720703, 0.01761107683181763, 0.01631205916404724, 0.01681686520576477, 0.016832023859024048, 0.015244742631912231, 0.015493625402450561, 0.016354949474334718, 0.017266392707824707, 0.01643863081932068, 0.014935929775238038, 0.01553526520729065]\n"
     ]
    }
   ],
   "source": [
    "print(train_acc_list)\n",
    "print(train_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81189086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.47, 33.97, 33.71, 40.18, 41.94, 39.81, 45.23, 45.07, 48.13, 49.38, 51.01, 43.99, 51.95, 54.53, 55.06, 53.9, 57.84, 55.98, 57.95, 56.88, 60.61, 61.95, 61.24, 61.19, 62.33, 63.5, 65.14, 61.19, 65.77, 65.59, 65.18, 64.24, 59.65, 61.63, 63.67, 66.98, 66.23, 67.73, 67.45, 66.38, 68.32, 67.76, 69.04, 71.43, 70.55, 71.78, 67.69, 68.69, 71.08, 70.94, 73.65, 72.5, 69.46, 73.72, 73.45, 73.61, 72.89, 72.87, 76.42, 74.02, 76.48, 75.72, 75.01, 76.05, 77.22, 77.23, 76.04, 76.19, 75.19, 76.65, 77.29, 77.49, 78.73, 76.86, 77.47, 78.62, 77.57, 78.64, 79.18, 79.42, 79.63, 78.95, 78.14, 77.99, 79.66, 80.11, 79.83, 78.99, 78.55, 79.58, 81.01, 79.06, 76.25, 80.91, 79.46, 80.71, 79.1, 79.75, 80.85, 80.27]\n",
      "[0.24788956642150878, 0.17773879766464235, 0.1862744927406311, 0.16267482042312623, 0.15411751270294188, 0.17147183418273926, 0.14832090139389037, 0.15638885498046876, 0.1472830057144165, 0.13727262020111083, 0.13453842401504518, 0.17226266860961914, 0.13430585861206054, 0.13010144233703613, 0.12478911876678467, 0.12576918601989745, 0.1149333357810974, 0.11939616203308105, 0.11592494249343872, 0.12010912895202637, 0.1070931077003479, 0.10311522483825683, 0.10535897016525268, 0.10766539573669434, 0.10264328718185425, 0.09909887909889221, 0.09414167404174804, 0.10955705642700195, 0.09518908262252808, 0.0934921681880951, 0.09512221217155456, 0.10260002613067627, 0.11701548099517822, 0.11461900472640991, 0.09908261895179749, 0.0924258828163147, 0.09599618911743164, 0.08739525675773621, 0.08902118802070617, 0.09499109983444214, 0.08757801651954651, 0.09013639688491822, 0.08629249334335327, 0.07758926153182984, 0.07976871132850646, 0.07839421033859253, 0.09357890486717224, 0.0907427191734314, 0.08478874564170838, 0.08257243037223816, 0.07384616732597352, 0.07763486504554748, 0.09337939620018006, 0.07419365048408508, 0.07834781408309936, 0.07556652426719665, 0.07656546235084534, 0.07852023243904113, 0.0679287850856781, 0.07397759556770325, 0.06753472089767457, 0.06768189072608947, 0.07295039296150208, 0.07027146220207214, 0.06434341669082641, 0.06429337859153747, 0.07066797018051148, 0.06887170076370239, 0.07672905325889587, 0.06737334728240967, 0.06565931439399719, 0.06643133759498596, 0.06023153066635132, 0.07055106163024902, 0.06697786450386048, 0.064005047082901, 0.06374862194061279, 0.06276743412017823, 0.06103031635284424, 0.05824946165084839, 0.05728302001953125, 0.061387276649475096, 0.06454740762710572, 0.06675615310668945, 0.05996033549308777, 0.0579990029335022, 0.061393517255783084, 0.06349796652793885, 0.06127515435218811, 0.06249776482582092, 0.054193198680877686, 0.06271069049835205, 0.0787518322467804, 0.0577720046043396, 0.05865069031715393, 0.0566656231880188, 0.06736825704574585, 0.05729954838752747, 0.05478777289390564, 0.055987733602523806]\n"
     ]
    }
   ],
   "source": [
    "print(test_acc_list)\n",
    "print(test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68658828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
