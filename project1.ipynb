{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093bae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import cm\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "#from torchtoolbox.tools import mixup_data, mixup_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bf6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 48\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 48, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(48)\n",
    "        self.layer1 = self._make_layer(block, 48, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 96, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 192, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 384, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(384, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def project1_model():\n",
    "    return ResNet(BasicBlock, [1, 2, 4, 1])\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d4a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999df676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    #Cutout(n_holes=1, length=16),  \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=256, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2738cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4919674\n"
     ]
    }
   ],
   "source": [
    "net = project1_model()\n",
    "\n",
    "num = count_parameters(net)\n",
    "print(num)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) \n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "#train_dur_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "#test_dur_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a2c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "    t0 = time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets,1.)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        # loss = nll(outputs, targets)\n",
    "        \n",
    "        #loss = criterion(outputs, targets)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_index = batch_idx\n",
    "        # train_loss += loss.item()\n",
    "        train_loss = loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += (lam * predicted.eq(targets_a.data).sum().float()\n",
    "                    + (1 - lam) * predicted.eq(targets_b.data).sum().float()).item()\n",
    "\n",
    "        #print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     #% (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        #print(\"\\n\")\n",
    "    train_loss_list.append(train_loss/(batch_index+1))\n",
    "    train_acc_list.append(100.*correct/total)\n",
    "    print(' Train: Loss: %.6f | Acc: %.3f%% | Dur: %.2fS' \n",
    "          % (train_loss/(batch_index+1), 100.*correct/total, time() - t0))\n",
    "\n",
    "def test(epoch):\n",
    "#     global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "    t0 = time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # loss = nll(outputs, targets)\n",
    "            batch_index = batch_idx\n",
    "            # test_loss += loss.item()\n",
    "            test_loss = loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            #print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         #% (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            #print(\"\\n\")\n",
    "    test_loss_list.append(test_loss/(batch_index+1))\n",
    "    test_acc_list.append(100.*correct/total)\n",
    "    print(' Test:  Loss: %.6f | Acc: %.3f%% | Dur: %.2fS' \n",
    "          % (test_loss/(batch_index+1), 100.*correct/total, time() - t0))\n",
    "\n",
    "    # Save checkpoint.\n",
    "#     acc = 100.*correct/total\n",
    "#     if acc > best_acc:\n",
    "#         print('Saving..')\n",
    "#         state = {\n",
    "#             'net': net.state_dict(),\n",
    "#             'acc': acc,\n",
    "#             'epoch': epoch,\n",
    "#             'train_history': (train_loss_list, train_acc_list, test_loss_list, test_acc_list)\n",
    "#         }\n",
    "#         if not os.path.isdir('checkpoint'):\n",
    "#             os.mkdir('checkpoint')\n",
    "#         torch.save(state, './checkpoint/ckpt_rmsprop.pth')\n",
    "#         best_acc = acc\n",
    "\n",
    "\n",
    "def plot_model(train_loss, train_acc, test_loss, test_acc):\n",
    "    length = len(train_acc)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(length), train_acc)\n",
    "    plt.title('Train accuracy vs. epoches')\n",
    "    plt.ylabel('Train accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(length), train_loss)\n",
    "    plt.title('Train loss vs. epoches')\n",
    "    plt.ylabel('Train loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(length), test_acc)\n",
    "    plt.title('Test accuracy vs. epoches')\n",
    "    plt.ylabel('Test accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(length), test_loss)\n",
    "    plt.title('Test loss vs. epoches')\n",
    "    plt.ylabel('Test loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.6, \n",
    "                    hspace=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083e6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " Train: Loss: 0.009293 | Acc: 31.759% | Dur: 22.19S\n",
      " Test:  Loss: 0.132873 | Acc: 50.730% | Dur: 3.23S\n",
      "\n",
      "Epoch: 1\n",
      " Train: Loss: 0.008121 | Acc: 44.282% | Dur: 15.59S\n",
      " Test:  Loss: 0.130401 | Acc: 55.030% | Dur: 3.32S\n",
      "\n",
      "Epoch: 2\n",
      " Train: Loss: 0.007897 | Acc: 50.066% | Dur: 15.83S\n",
      " Test:  Loss: 0.113369 | Acc: 61.390% | Dur: 3.42S\n",
      "\n",
      "Epoch: 3\n",
      " Train: Loss: 0.008758 | Acc: 54.132% | Dur: 15.55S\n",
      " Test:  Loss: 0.090684 | Acc: 70.760% | Dur: 3.19S\n",
      "\n",
      "Epoch: 4\n",
      " Train: Loss: 0.004556 | Acc: 55.683% | Dur: 15.54S\n",
      " Test:  Loss: 0.083128 | Acc: 71.020% | Dur: 3.34S\n",
      "\n",
      "Epoch: 5\n",
      " Train: Loss: 0.005762 | Acc: 57.883% | Dur: 15.97S\n",
      " Test:  Loss: 0.087087 | Acc: 72.120% | Dur: 3.41S\n",
      "\n",
      "Epoch: 6\n",
      " Train: Loss: 0.008645 | Acc: 59.248% | Dur: 16.15S\n",
      " Test:  Loss: 0.075798 | Acc: 75.960% | Dur: 3.42S\n",
      "\n",
      "Epoch: 7\n",
      " Train: Loss: 0.005212 | Acc: 61.216% | Dur: 21.15S\n",
      " Test:  Loss: 0.072581 | Acc: 76.230% | Dur: 3.49S\n",
      "\n",
      "Epoch: 8\n",
      " Train: Loss: 0.008462 | Acc: 61.608% | Dur: 20.54S\n",
      " Test:  Loss: 0.070692 | Acc: 78.980% | Dur: 3.51S\n",
      "\n",
      "Epoch: 9\n",
      " Train: Loss: 0.006942 | Acc: 59.855% | Dur: 23.14S\n",
      " Test:  Loss: 0.066945 | Acc: 79.800% | Dur: 3.42S\n",
      "\n",
      "Epoch: 10\n",
      " Train: Loss: 0.007213 | Acc: 62.106% | Dur: 20.82S\n",
      " Test:  Loss: 0.058937 | Acc: 82.220% | Dur: 3.51S\n",
      "\n",
      "Epoch: 11\n",
      " Train: Loss: 0.005950 | Acc: 61.107% | Dur: 20.42S\n",
      " Test:  Loss: 0.062400 | Acc: 82.390% | Dur: 3.50S\n",
      "\n",
      "Epoch: 12\n",
      " Train: Loss: 0.003671 | Acc: 64.274% | Dur: 20.60S\n",
      " Test:  Loss: 0.069637 | Acc: 79.520% | Dur: 3.43S\n",
      "\n",
      "Epoch: 13\n",
      " Train: Loss: 0.003009 | Acc: 65.859% | Dur: 20.06S\n",
      " Test:  Loss: 0.052856 | Acc: 82.980% | Dur: 3.42S\n",
      "\n",
      "Epoch: 14\n",
      " Train: Loss: 0.006690 | Acc: 63.431% | Dur: 21.41S\n",
      " Test:  Loss: 0.057531 | Acc: 84.540% | Dur: 3.45S\n",
      "\n",
      "Epoch: 15\n",
      " Train: Loss: 0.007662 | Acc: 65.840% | Dur: 20.85S\n",
      " Test:  Loss: 0.054434 | Acc: 82.910% | Dur: 3.47S\n",
      "\n",
      "Epoch: 16\n",
      " Train: Loss: 0.002339 | Acc: 67.347% | Dur: 20.39S\n",
      " Test:  Loss: 0.056437 | Acc: 82.990% | Dur: 3.51S\n",
      "\n",
      "Epoch: 17\n",
      " Train: Loss: 0.003615 | Acc: 66.438% | Dur: 19.67S\n",
      " Test:  Loss: 0.057384 | Acc: 84.240% | Dur: 2.89S\n",
      "\n",
      "Epoch: 18\n",
      " Train: Loss: 0.002166 | Acc: 68.373% | Dur: 13.39S\n",
      " Test:  Loss: 0.050559 | Acc: 85.620% | Dur: 2.78S\n",
      "\n",
      "Epoch: 19\n",
      " Train: Loss: 0.005544 | Acc: 66.643% | Dur: 13.98S\n",
      " Test:  Loss: 0.054230 | Acc: 85.600% | Dur: 3.08S\n",
      "\n",
      "Epoch: 20\n",
      " Train: Loss: 0.006051 | Acc: 66.777% | Dur: 14.23S\n",
      " Test:  Loss: 0.051666 | Acc: 85.440% | Dur: 2.90S\n",
      "\n",
      "Epoch: 21\n",
      " Train: Loss: 0.005271 | Acc: 65.430% | Dur: 13.31S\n",
      " Test:  Loss: 0.050874 | Acc: 86.440% | Dur: 2.97S\n",
      "\n",
      "Epoch: 22\n",
      " Train: Loss: 0.006980 | Acc: 67.776% | Dur: 14.23S\n",
      " Test:  Loss: 0.051005 | Acc: 87.110% | Dur: 2.96S\n",
      "\n",
      "Epoch: 23\n",
      " Train: Loss: 0.005564 | Acc: 68.502% | Dur: 14.04S\n",
      " Test:  Loss: 0.045756 | Acc: 87.810% | Dur: 3.04S\n",
      "\n",
      "Epoch: 24\n",
      " Train: Loss: 0.007687 | Acc: 67.929% | Dur: 13.94S\n",
      " Test:  Loss: 0.047909 | Acc: 87.920% | Dur: 2.90S\n",
      "\n",
      "Epoch: 25\n",
      " Train: Loss: 0.007196 | Acc: 70.028% | Dur: 13.25S\n",
      " Test:  Loss: 0.044336 | Acc: 87.070% | Dur: 2.94S\n",
      "\n",
      "Epoch: 26\n",
      " Train: Loss: 0.001982 | Acc: 65.830% | Dur: 13.03S\n",
      " Test:  Loss: 0.045995 | Acc: 88.740% | Dur: 2.92S\n",
      "\n",
      "Epoch: 27\n",
      " Train: Loss: 0.002277 | Acc: 71.843% | Dur: 14.22S\n",
      " Test:  Loss: 0.044860 | Acc: 87.620% | Dur: 2.96S\n",
      "\n",
      "Epoch: 28\n",
      " Train: Loss: 0.006000 | Acc: 66.978% | Dur: 13.56S\n",
      " Test:  Loss: 0.047762 | Acc: 87.720% | Dur: 2.95S\n",
      "\n",
      "Epoch: 29\n",
      " Train: Loss: 0.001677 | Acc: 70.675% | Dur: 13.86S\n",
      " Test:  Loss: 0.040572 | Acc: 88.830% | Dur: 2.96S\n",
      "\n",
      "Epoch: 30\n",
      " Train: Loss: 0.006434 | Acc: 68.838% | Dur: 13.86S\n",
      " Test:  Loss: 0.045938 | Acc: 88.370% | Dur: 3.00S\n",
      "\n",
      "Epoch: 31\n",
      " Train: Loss: 0.004449 | Acc: 68.568% | Dur: 14.03S\n",
      " Test:  Loss: 0.041032 | Acc: 89.210% | Dur: 3.01S\n",
      "\n",
      "Epoch: 32\n",
      " Train: Loss: 0.007524 | Acc: 70.726% | Dur: 13.80S\n",
      " Test:  Loss: 0.046838 | Acc: 90.020% | Dur: 2.98S\n",
      "\n",
      "Epoch: 33\n",
      " Train: Loss: 0.004830 | Acc: 70.363% | Dur: 13.69S\n",
      " Test:  Loss: 0.045022 | Acc: 89.470% | Dur: 2.98S\n",
      "\n",
      "Epoch: 34\n",
      " Train: Loss: 0.007068 | Acc: 70.617% | Dur: 13.74S\n",
      " Test:  Loss: 0.043493 | Acc: 90.050% | Dur: 3.00S\n",
      "\n",
      "Epoch: 35\n",
      " Train: Loss: 0.007450 | Acc: 68.928% | Dur: 13.78S\n",
      " Test:  Loss: 0.043888 | Acc: 90.300% | Dur: 2.98S\n",
      "\n",
      "Epoch: 36\n",
      " Train: Loss: 0.001048 | Acc: 70.600% | Dur: 13.93S\n",
      " Test:  Loss: 0.037200 | Acc: 90.620% | Dur: 2.97S\n",
      "\n",
      "Epoch: 37\n",
      " Train: Loss: 0.007040 | Acc: 71.492% | Dur: 13.80S\n",
      " Test:  Loss: 0.038996 | Acc: 90.490% | Dur: 2.99S\n",
      "\n",
      "Epoch: 38\n",
      " Train: Loss: 0.002554 | Acc: 70.486% | Dur: 13.92S\n",
      " Test:  Loss: 0.034021 | Acc: 91.120% | Dur: 3.00S\n",
      "\n",
      "Epoch: 39\n",
      " Train: Loss: 0.006424 | Acc: 71.163% | Dur: 14.14S\n",
      " Test:  Loss: 0.041398 | Acc: 90.900% | Dur: 2.98S\n",
      "\n",
      "Epoch: 40\n",
      " Train: Loss: 0.005240 | Acc: 70.184% | Dur: 14.00S\n",
      " Test:  Loss: 0.040174 | Acc: 90.960% | Dur: 2.96S\n",
      "\n",
      "Epoch: 41\n",
      " Train: Loss: 0.007410 | Acc: 71.736% | Dur: 13.96S\n",
      " Test:  Loss: 0.036539 | Acc: 90.940% | Dur: 2.97S\n",
      "\n",
      "Epoch: 42\n",
      " Train: Loss: 0.003082 | Acc: 69.452% | Dur: 13.66S\n",
      " Test:  Loss: 0.032991 | Acc: 91.560% | Dur: 3.02S\n",
      "\n",
      "Epoch: 43\n",
      " Train: Loss: 0.006191 | Acc: 69.572% | Dur: 14.07S\n",
      " Test:  Loss: 0.037355 | Acc: 91.570% | Dur: 2.98S\n",
      "\n",
      "Epoch: 44\n",
      " Train: Loss: 0.007449 | Acc: 70.303% | Dur: 13.92S\n",
      " Test:  Loss: 0.044353 | Acc: 90.940% | Dur: 3.02S\n",
      "\n",
      "Epoch: 45\n",
      " Train: Loss: 0.006367 | Acc: 70.759% | Dur: 13.73S\n",
      " Test:  Loss: 0.038547 | Acc: 91.280% | Dur: 2.98S\n",
      "\n",
      "Epoch: 46\n",
      " Train: Loss: 0.006542 | Acc: 70.541% | Dur: 13.77S\n",
      " Test:  Loss: 0.034894 | Acc: 91.530% | Dur: 2.95S\n",
      "\n",
      "Epoch: 47\n",
      " Train: Loss: 0.002721 | Acc: 70.313% | Dur: 14.15S\n",
      " Test:  Loss: 0.038539 | Acc: 91.300% | Dur: 3.03S\n",
      "\n",
      "Epoch: 48\n",
      " Train: Loss: 0.006493 | Acc: 72.588% | Dur: 13.90S\n",
      " Test:  Loss: 0.038586 | Acc: 91.170% | Dur: 2.99S\n",
      "\n",
      "Epoch: 49\n",
      " Train: Loss: 0.005245 | Acc: 72.385% | Dur: 13.78S\n",
      " Test:  Loss: 0.038154 | Acc: 91.310% | Dur: 2.97S\n",
      "\n",
      "Epoch: 50\n",
      " Train: Loss: 0.003199 | Acc: 71.026% | Dur: 13.75S\n",
      " Test:  Loss: 0.037584 | Acc: 91.370% | Dur: 2.94S\n",
      "\n",
      "Epoch: 51\n",
      " Train: Loss: 0.006152 | Acc: 72.284% | Dur: 14.02S\n",
      " Test:  Loss: 0.037613 | Acc: 91.370% | Dur: 2.97S\n",
      "\n",
      "Epoch: 52\n",
      " Train: Loss: 0.006601 | Acc: 68.934% | Dur: 13.94S\n",
      " Test:  Loss: 0.042465 | Acc: 91.110% | Dur: 3.06S\n",
      "\n",
      "Epoch: 53\n",
      " Train: Loss: 0.001535 | Acc: 72.093% | Dur: 14.22S\n",
      " Test:  Loss: 0.035130 | Acc: 91.540% | Dur: 3.01S\n",
      "\n",
      "Epoch: 54\n",
      " Train: Loss: 0.005609 | Acc: 71.477% | Dur: 13.50S\n",
      " Test:  Loss: 0.035217 | Acc: 91.630% | Dur: 3.00S\n",
      "\n",
      "Epoch: 55\n",
      " Train: Loss: 0.007151 | Acc: 71.583% | Dur: 13.87S\n",
      " Test:  Loss: 0.036581 | Acc: 91.490% | Dur: 2.98S\n",
      "\n",
      "Epoch: 56\n",
      " Train: Loss: 0.005724 | Acc: 69.871% | Dur: 13.94S\n",
      " Test:  Loss: 0.042487 | Acc: 91.110% | Dur: 2.99S\n",
      "\n",
      "Epoch: 57\n",
      " Train: Loss: 0.004409 | Acc: 72.315% | Dur: 14.02S\n",
      " Test:  Loss: 0.037567 | Acc: 91.430% | Dur: 3.00S\n",
      "\n",
      "Epoch: 58\n",
      " Train: Loss: 0.006903 | Acc: 72.448% | Dur: 13.85S\n",
      " Test:  Loss: 0.040490 | Acc: 91.150% | Dur: 2.99S\n",
      "\n",
      "Epoch: 59\n",
      " Train: Loss: 0.002843 | Acc: 69.760% | Dur: 13.87S\n",
      " Test:  Loss: 0.038444 | Acc: 91.400% | Dur: 2.97S\n",
      "\n",
      "Epoch: 60\n",
      " Train: Loss: 0.001902 | Acc: 70.040% | Dur: 14.07S\n",
      " Test:  Loss: 0.039650 | Acc: 90.760% | Dur: 2.98S\n",
      "\n",
      "Epoch: 61\n",
      " Train: Loss: 0.006423 | Acc: 71.853% | Dur: 14.01S\n",
      " Test:  Loss: 0.039382 | Acc: 91.100% | Dur: 2.97S\n",
      "\n",
      "Epoch: 62\n",
      " Train: Loss: 0.006820 | Acc: 69.354% | Dur: 13.92S\n",
      " Test:  Loss: 0.035242 | Acc: 91.090% | Dur: 3.05S\n",
      "\n",
      "Epoch: 63\n",
      " Train: Loss: 0.003749 | Acc: 71.880% | Dur: 14.11S\n",
      " Test:  Loss: 0.039311 | Acc: 90.810% | Dur: 2.94S\n",
      "\n",
      "Epoch: 64\n",
      " Train: Loss: 0.007395 | Acc: 71.464% | Dur: 14.22S\n",
      " Test:  Loss: 0.040410 | Acc: 90.520% | Dur: 3.01S\n",
      "\n",
      "Epoch: 65\n",
      " Train: Loss: 0.003212 | Acc: 71.510% | Dur: 14.08S\n",
      " Test:  Loss: 0.044226 | Acc: 89.890% | Dur: 2.99S\n",
      "\n",
      "Epoch: 66\n",
      " Train: Loss: 0.006927 | Acc: 71.428% | Dur: 14.32S\n",
      " Test:  Loss: 0.040884 | Acc: 90.030% | Dur: 2.92S\n",
      "\n",
      "Epoch: 67\n",
      " Train: Loss: 0.006487 | Acc: 68.581% | Dur: 13.37S\n",
      " Test:  Loss: 0.039453 | Acc: 90.150% | Dur: 2.87S\n",
      "\n",
      "Epoch: 68\n",
      " Train: Loss: 0.003712 | Acc: 74.355% | Dur: 13.74S\n",
      " Test:  Loss: 0.038854 | Acc: 89.120% | Dur: 2.94S\n",
      "\n",
      "Epoch: 69\n",
      " Train: Loss: 0.001015 | Acc: 70.392% | Dur: 13.85S\n",
      " Test:  Loss: 0.039709 | Acc: 90.140% | Dur: 2.90S\n",
      "\n",
      "Epoch: 70\n",
      " Train: Loss: 0.005970 | Acc: 71.528% | Dur: 13.59S\n",
      " Test:  Loss: 0.043492 | Acc: 88.620% | Dur: 3.10S\n",
      "\n",
      "Epoch: 71\n",
      " Train: Loss: 0.005942 | Acc: 70.417% | Dur: 13.95S\n",
      " Test:  Loss: 0.042192 | Acc: 89.030% | Dur: 3.03S\n",
      "\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Loss: 0.007289 | Acc: 68.315% | Dur: 13.77S\n",
      " Test:  Loss: 0.047882 | Acc: 88.510% | Dur: 2.97S\n",
      "\n",
      "Epoch: 73\n",
      " Train: Loss: 0.002384 | Acc: 70.999% | Dur: 13.95S\n",
      " Test:  Loss: 0.038816 | Acc: 89.710% | Dur: 3.04S\n",
      "\n",
      "Epoch: 74\n",
      " Train: Loss: 0.005524 | Acc: 70.748% | Dur: 13.88S\n",
      " Test:  Loss: 0.040355 | Acc: 90.170% | Dur: 2.97S\n",
      "\n",
      "Epoch: 75\n",
      " Train: Loss: 0.005147 | Acc: 70.681% | Dur: 14.15S\n",
      " Test:  Loss: 0.045923 | Acc: 89.950% | Dur: 3.02S\n",
      "\n",
      "Epoch: 76\n",
      " Train: Loss: 0.006888 | Acc: 69.126% | Dur: 14.40S\n",
      " Test:  Loss: 0.055088 | Acc: 85.810% | Dur: 3.06S\n",
      "\n",
      "Epoch: 77\n",
      " Train: Loss: 0.006577 | Acc: 69.184% | Dur: 14.06S\n",
      " Test:  Loss: 0.046314 | Acc: 90.050% | Dur: 3.05S\n",
      "\n",
      "Epoch: 78\n",
      " Train: Loss: 0.007257 | Acc: 68.744% | Dur: 14.33S\n",
      " Test:  Loss: 0.041282 | Acc: 90.120% | Dur: 3.00S\n",
      "\n",
      "Epoch: 79\n",
      " Train: Loss: 0.006659 | Acc: 71.216% | Dur: 14.16S\n",
      " Test:  Loss: 0.048538 | Acc: 88.500% | Dur: 3.04S\n",
      "\n",
      "Epoch: 80\n",
      " Train: Loss: 0.007231 | Acc: 70.927% | Dur: 13.94S\n",
      " Test:  Loss: 0.045739 | Acc: 87.410% | Dur: 2.91S\n",
      "\n",
      "Epoch: 81\n",
      " Train: Loss: 0.002335 | Acc: 69.670% | Dur: 13.59S\n",
      " Test:  Loss: 0.043218 | Acc: 88.770% | Dur: 2.96S\n",
      "\n",
      "Epoch: 82\n",
      " Train: Loss: 0.005764 | Acc: 70.116% | Dur: 13.81S\n",
      " Test:  Loss: 0.043414 | Acc: 89.070% | Dur: 2.97S\n",
      "\n",
      "Epoch: 83\n",
      " Train: Loss: 0.006921 | Acc: 71.498% | Dur: 14.05S\n",
      " Test:  Loss: 0.046949 | Acc: 87.510% | Dur: 2.90S\n",
      "\n",
      "Epoch: 84\n",
      " Train: Loss: 0.007560 | Acc: 71.099% | Dur: 14.10S\n",
      " Test:  Loss: 0.050085 | Acc: 87.720% | Dur: 2.90S\n",
      "\n",
      "Epoch: 85\n",
      " Train: Loss: 0.006399 | Acc: 70.131% | Dur: 13.77S\n",
      " Test:  Loss: 0.056715 | Acc: 83.980% | Dur: 3.00S\n",
      "\n",
      "Epoch: 86\n",
      " Train: Loss: 0.004323 | Acc: 69.728% | Dur: 13.87S\n",
      " Test:  Loss: 0.047283 | Acc: 88.900% | Dur: 3.01S\n",
      "\n",
      "Epoch: 87\n",
      " Train: Loss: 0.006736 | Acc: 69.902% | Dur: 13.97S\n",
      " Test:  Loss: 0.044164 | Acc: 89.520% | Dur: 3.04S\n",
      "\n",
      "Epoch: 88\n",
      " Train: Loss: 0.002356 | Acc: 69.794% | Dur: 13.79S\n",
      " Test:  Loss: 0.044947 | Acc: 87.990% | Dur: 2.99S\n",
      "\n",
      "Epoch: 89\n",
      " Train: Loss: 0.006043 | Acc: 70.352% | Dur: 15.83S\n",
      " Test:  Loss: 0.043113 | Acc: 89.390% | Dur: 2.87S\n",
      "\n",
      "Epoch: 90\n",
      " Train: Loss: 0.007369 | Acc: 69.297% | Dur: 13.14S\n",
      " Test:  Loss: 0.048252 | Acc: 89.380% | Dur: 2.85S\n",
      "\n",
      "Epoch: 91\n",
      " Train: Loss: 0.007170 | Acc: 69.349% | Dur: 13.49S\n",
      " Test:  Loss: 0.049386 | Acc: 87.860% | Dur: 2.80S\n",
      "\n",
      "Epoch: 92\n",
      " Train: Loss: 0.005801 | Acc: 71.561% | Dur: 13.46S\n",
      " Test:  Loss: 0.051250 | Acc: 86.040% | Dur: 2.83S\n",
      "\n",
      "Epoch: 93\n",
      " Train: Loss: 0.005470 | Acc: 69.773% | Dur: 13.20S\n",
      " Test:  Loss: 0.043164 | Acc: 89.180% | Dur: 2.78S\n",
      "\n",
      "Epoch: 94\n",
      " Train: Loss: 0.002175 | Acc: 72.551% | Dur: 13.09S\n",
      " Test:  Loss: 0.046915 | Acc: 89.410% | Dur: 2.83S\n",
      "\n",
      "Epoch: 95\n",
      " Train: Loss: 0.007417 | Acc: 71.374% | Dur: 13.08S\n",
      " Test:  Loss: 0.048947 | Acc: 86.930% | Dur: 2.80S\n",
      "\n",
      "Epoch: 96\n",
      " Train: Loss: 0.004392 | Acc: 71.248% | Dur: 13.66S\n",
      " Test:  Loss: 0.042834 | Acc: 90.280% | Dur: 2.91S\n",
      "\n",
      "Epoch: 97\n",
      " Train: Loss: 0.006741 | Acc: 69.008% | Dur: 13.63S\n",
      " Test:  Loss: 0.052086 | Acc: 88.880% | Dur: 2.85S\n",
      "\n",
      "Epoch: 98\n",
      " Train: Loss: 0.005159 | Acc: 69.602% | Dur: 13.34S\n",
      " Test:  Loss: 0.045277 | Acc: 88.860% | Dur: 2.79S\n",
      "\n",
      "Epoch: 99\n",
      " Train: Loss: 0.007166 | Acc: 67.563% | Dur: 13.42S\n",
      " Test:  Loss: 0.048919 | Acc: 88.100% | Dur: 2.80S\n",
      "\n",
      "Epoch: 100\n",
      " Train: Loss: 0.004683 | Acc: 68.724% | Dur: 13.17S\n",
      " Test:  Loss: 0.049384 | Acc: 87.870% | Dur: 2.81S\n",
      "\n",
      "Epoch: 101\n",
      " Train: Loss: 0.004275 | Acc: 70.613% | Dur: 13.19S\n",
      " Test:  Loss: 0.047784 | Acc: 88.540% | Dur: 2.84S\n",
      "\n",
      "Epoch: 102\n",
      " Train: Loss: 0.005911 | Acc: 70.808% | Dur: 13.03S\n",
      " Test:  Loss: 0.041895 | Acc: 90.390% | Dur: 2.79S\n",
      "\n",
      "Epoch: 103\n",
      " Train: Loss: 0.006566 | Acc: 70.958% | Dur: 13.38S\n",
      " Test:  Loss: 0.047198 | Acc: 89.780% | Dur: 2.79S\n",
      "\n",
      "Epoch: 104\n",
      " Train: Loss: 0.005535 | Acc: 70.958% | Dur: 13.15S\n",
      " Test:  Loss: 0.043687 | Acc: 89.190% | Dur: 2.78S\n",
      "\n",
      "Epoch: 105\n",
      " Train: Loss: 0.004841 | Acc: 71.726% | Dur: 13.53S\n",
      " Test:  Loss: 0.043005 | Acc: 89.940% | Dur: 2.93S\n",
      "\n",
      "Epoch: 106\n",
      " Train: Loss: 0.006904 | Acc: 70.655% | Dur: 13.15S\n",
      " Test:  Loss: 0.040771 | Acc: 90.820% | Dur: 2.75S\n",
      "\n",
      "Epoch: 107\n",
      " Train: Loss: 0.005591 | Acc: 70.823% | Dur: 13.55S\n",
      " Test:  Loss: 0.047880 | Acc: 88.880% | Dur: 2.84S\n",
      "\n",
      "Epoch: 108\n",
      " Train: Loss: 0.005164 | Acc: 71.711% | Dur: 13.23S\n",
      " Test:  Loss: 0.046260 | Acc: 91.180% | Dur: 2.73S\n",
      "\n",
      "Epoch: 109\n",
      " Train: Loss: 0.004895 | Acc: 71.732% | Dur: 13.06S\n",
      " Test:  Loss: 0.047800 | Acc: 88.520% | Dur: 2.82S\n",
      "\n",
      "Epoch: 110\n",
      " Train: Loss: 0.006220 | Acc: 70.631% | Dur: 13.27S\n",
      " Test:  Loss: 0.043125 | Acc: 91.620% | Dur: 2.83S\n",
      "\n",
      "Epoch: 111\n",
      " Train: Loss: 0.006769 | Acc: 71.622% | Dur: 13.20S\n",
      " Test:  Loss: 0.042224 | Acc: 91.350% | Dur: 2.80S\n",
      "\n",
      "Epoch: 112\n",
      " Train: Loss: 0.007299 | Acc: 72.433% | Dur: 13.32S\n",
      " Test:  Loss: 0.045727 | Acc: 89.860% | Dur: 2.96S\n",
      "\n",
      "Epoch: 113\n",
      " Train: Loss: 0.005157 | Acc: 74.349% | Dur: 13.00S\n",
      " Test:  Loss: 0.037811 | Acc: 91.370% | Dur: 2.86S\n",
      "\n",
      "Epoch: 114\n",
      " Train: Loss: 0.005828 | Acc: 70.387% | Dur: 13.18S\n",
      " Test:  Loss: 0.044432 | Acc: 90.210% | Dur: 2.79S\n",
      "\n",
      "Epoch: 115\n",
      " Train: Loss: 0.001783 | Acc: 72.431% | Dur: 13.47S\n",
      " Test:  Loss: 0.037852 | Acc: 91.790% | Dur: 2.87S\n",
      "\n",
      "Epoch: 116\n",
      " Train: Loss: 0.006260 | Acc: 72.809% | Dur: 13.71S\n",
      " Test:  Loss: 0.044937 | Acc: 90.130% | Dur: 2.92S\n",
      "\n",
      "Epoch: 117\n",
      " Train: Loss: 0.006457 | Acc: 72.957% | Dur: 13.30S\n",
      " Test:  Loss: 0.042638 | Acc: 90.990% | Dur: 2.77S\n",
      "\n",
      "Epoch: 118\n",
      " Train: Loss: 0.002886 | Acc: 72.256% | Dur: 13.22S\n",
      " Test:  Loss: 0.037818 | Acc: 92.070% | Dur: 2.85S\n",
      "\n",
      "Epoch: 119\n",
      " Train: Loss: 0.005899 | Acc: 72.158% | Dur: 13.66S\n",
      " Test:  Loss: 0.041451 | Acc: 91.960% | Dur: 2.91S\n",
      "\n",
      "Epoch: 120\n",
      " Train: Loss: 0.006234 | Acc: 71.607% | Dur: 13.40S\n",
      " Test:  Loss: 0.040943 | Acc: 92.050% | Dur: 2.83S\n",
      "\n",
      "Epoch: 121\n",
      " Train: Loss: 0.002182 | Acc: 72.259% | Dur: 13.67S\n",
      " Test:  Loss: 0.039076 | Acc: 92.170% | Dur: 2.83S\n",
      "\n",
      "Epoch: 122\n",
      " Train: Loss: 0.005857 | Acc: 71.479% | Dur: 13.23S\n",
      " Test:  Loss: 0.039074 | Acc: 92.540% | Dur: 2.75S\n",
      "\n",
      "Epoch: 123\n",
      " Train: Loss: 0.003279 | Acc: 73.201% | Dur: 13.83S\n",
      " Test:  Loss: 0.039755 | Acc: 92.600% | Dur: 3.33S\n",
      "\n",
      "Epoch: 124\n",
      " Train: Loss: 0.006536 | Acc: 73.064% | Dur: 16.77S\n",
      " Test:  Loss: 0.039934 | Acc: 92.660% | Dur: 2.80S\n",
      "\n",
      "Epoch: 125\n",
      " Train: Loss: 0.005584 | Acc: 72.176% | Dur: 13.47S\n",
      " Test:  Loss: 0.036867 | Acc: 92.750% | Dur: 2.76S\n",
      "\n",
      "Epoch: 126\n",
      " Train: Loss: 0.006625 | Acc: 72.775% | Dur: 13.81S\n",
      " Test:  Loss: 0.040951 | Acc: 92.770% | Dur: 2.86S\n",
      "\n",
      "Epoch: 127\n",
      " Train: Loss: 0.006245 | Acc: 76.402% | Dur: 13.46S\n",
      " Test:  Loss: 0.036863 | Acc: 92.660% | Dur: 2.89S\n",
      "\n",
      "Epoch: 128\n",
      " Train: Loss: 0.006127 | Acc: 74.816% | Dur: 14.07S\n",
      " Test:  Loss: 0.036452 | Acc: 92.460% | Dur: 3.07S\n",
      "\n",
      "Epoch: 129\n",
      " Train: Loss: 0.007017 | Acc: 73.732% | Dur: 13.73S\n",
      " Test:  Loss: 0.038457 | Acc: 93.200% | Dur: 2.98S\n",
      "\n",
      "Epoch: 130\n",
      " Train: Loss: 0.002839 | Acc: 73.269% | Dur: 13.82S\n",
      " Test:  Loss: 0.037664 | Acc: 93.240% | Dur: 3.10S\n",
      "\n",
      "Epoch: 131\n",
      " Train: Loss: 0.005924 | Acc: 73.373% | Dur: 13.60S\n",
      " Test:  Loss: 0.037133 | Acc: 93.260% | Dur: 3.11S\n",
      "\n",
      "Epoch: 132\n",
      " Train: Loss: 0.000501 | Acc: 75.194% | Dur: 15.51S\n",
      " Test:  Loss: 0.030894 | Acc: 93.230% | Dur: 2.96S\n",
      "\n",
      "Epoch: 133\n",
      " Train: Loss: 0.006896 | Acc: 73.154% | Dur: 14.36S\n",
      " Test:  Loss: 0.036748 | Acc: 93.030% | Dur: 3.04S\n",
      "\n",
      "Epoch: 134\n",
      " Train: Loss: 0.005275 | Acc: 72.618% | Dur: 14.00S\n",
      " Test:  Loss: 0.037344 | Acc: 93.380% | Dur: 2.98S\n",
      "\n",
      "Epoch: 135\n",
      " Train: Loss: 0.003570 | Acc: 75.752% | Dur: 15.04S\n",
      " Test:  Loss: 0.035935 | Acc: 93.480% | Dur: 3.00S\n",
      "\n",
      "Epoch: 136\n",
      " Train: Loss: 0.003719 | Acc: 72.811% | Dur: 13.75S\n",
      " Test:  Loss: 0.035688 | Acc: 93.370% | Dur: 2.96S\n",
      "\n",
      "Epoch: 137\n",
      " Train: Loss: 0.002912 | Acc: 74.938% | Dur: 13.60S\n",
      " Test:  Loss: 0.033293 | Acc: 93.980% | Dur: 2.97S\n",
      "\n",
      "Epoch: 138\n",
      " Train: Loss: 0.006716 | Acc: 73.057% | Dur: 13.80S\n",
      " Test:  Loss: 0.036876 | Acc: 93.840% | Dur: 2.96S\n",
      "\n",
      "Epoch: 139\n",
      " Train: Loss: 0.006122 | Acc: 75.404% | Dur: 13.77S\n",
      " Test:  Loss: 0.035242 | Acc: 93.580% | Dur: 2.98S\n",
      "\n",
      "Epoch: 140\n",
      " Train: Loss: 0.005826 | Acc: 74.100% | Dur: 13.47S\n",
      " Test:  Loss: 0.035185 | Acc: 93.820% | Dur: 2.98S\n",
      "\n",
      "Epoch: 141\n",
      " Train: Loss: 0.001346 | Acc: 74.447% | Dur: 14.03S\n",
      " Test:  Loss: 0.031722 | Acc: 94.010% | Dur: 3.00S\n",
      "\n",
      "Epoch: 142\n",
      " Train: Loss: 0.005708 | Acc: 75.543% | Dur: 13.77S\n",
      " Test:  Loss: 0.033470 | Acc: 93.970% | Dur: 3.06S\n",
      "\n",
      "Epoch: 143\n",
      " Train: Loss: 0.005117 | Acc: 75.254% | Dur: 13.99S\n",
      " Test:  Loss: 0.033691 | Acc: 93.920% | Dur: 3.00S\n",
      "\n",
      "Epoch: 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Loss: 0.005142 | Acc: 72.691% | Dur: 14.04S\n",
      " Test:  Loss: 0.037732 | Acc: 93.690% | Dur: 2.97S\n",
      "\n",
      "Epoch: 145\n",
      " Train: Loss: 0.002004 | Acc: 75.518% | Dur: 13.67S\n",
      " Test:  Loss: 0.030579 | Acc: 94.110% | Dur: 3.02S\n",
      "\n",
      "Epoch: 146\n",
      " Train: Loss: 0.004062 | Acc: 74.316% | Dur: 13.89S\n",
      " Test:  Loss: 0.035136 | Acc: 93.920% | Dur: 3.02S\n",
      "\n",
      "Epoch: 147\n",
      " Train: Loss: 0.002066 | Acc: 74.967% | Dur: 13.70S\n",
      " Test:  Loss: 0.031135 | Acc: 93.970% | Dur: 2.97S\n",
      "\n",
      "Epoch: 148\n",
      " Train: Loss: 0.001460 | Acc: 73.003% | Dur: 13.63S\n",
      " Test:  Loss: 0.035135 | Acc: 94.060% | Dur: 2.96S\n",
      "\n",
      "Epoch: 149\n",
      " Train: Loss: 0.003446 | Acc: 75.957% | Dur: 14.15S\n",
      " Test:  Loss: 0.032720 | Acc: 93.980% | Dur: 3.01S\n",
      "\n",
      "Epoch: 150\n",
      " Train: Loss: 0.004054 | Acc: 73.597% | Dur: 13.99S\n",
      " Test:  Loss: 0.033232 | Acc: 93.930% | Dur: 3.01S\n",
      "\n",
      "Epoch: 151\n",
      " Train: Loss: 0.006132 | Acc: 74.549% | Dur: 13.76S\n",
      " Test:  Loss: 0.037159 | Acc: 93.920% | Dur: 2.98S\n",
      "\n",
      "Epoch: 152\n",
      " Train: Loss: 0.001555 | Acc: 73.495% | Dur: 13.81S\n",
      " Test:  Loss: 0.032695 | Acc: 94.010% | Dur: 3.01S\n",
      "\n",
      "Epoch: 153\n",
      " Train: Loss: 0.004810 | Acc: 74.159% | Dur: 13.76S\n",
      " Test:  Loss: 0.034739 | Acc: 94.020% | Dur: 2.99S\n",
      "\n",
      "Epoch: 154\n",
      " Train: Loss: 0.004518 | Acc: 74.925% | Dur: 13.75S\n",
      " Test:  Loss: 0.031543 | Acc: 94.070% | Dur: 2.99S\n",
      "\n",
      "Epoch: 155\n",
      " Train: Loss: 0.006030 | Acc: 72.624% | Dur: 13.93S\n",
      " Test:  Loss: 0.036393 | Acc: 93.920% | Dur: 2.99S\n",
      "\n",
      "Epoch: 156\n",
      " Train: Loss: 0.004081 | Acc: 74.882% | Dur: 13.71S\n",
      " Test:  Loss: 0.032143 | Acc: 94.040% | Dur: 2.97S\n",
      "\n",
      "Epoch: 157\n",
      " Train: Loss: 0.006292 | Acc: 75.111% | Dur: 14.10S\n",
      " Test:  Loss: 0.030997 | Acc: 94.010% | Dur: 3.00S\n",
      "\n",
      "Epoch: 158\n",
      " Train: Loss: 0.004710 | Acc: 73.821% | Dur: 13.76S\n",
      " Test:  Loss: 0.034137 | Acc: 93.960% | Dur: 2.97S\n",
      "\n",
      "Epoch: 159\n",
      " Train: Loss: 0.005846 | Acc: 74.379% | Dur: 13.59S\n",
      " Test:  Loss: 0.031915 | Acc: 94.180% | Dur: 2.85S\n",
      "\n",
      "Epoch: 160\n",
      " Train: Loss: 0.005780 | Acc: 71.906% | Dur: 13.77S\n",
      " Test:  Loss: 0.036720 | Acc: 93.740% | Dur: 2.94S\n",
      "\n",
      "Epoch: 161\n",
      " Train: Loss: 0.006430 | Acc: 73.647% | Dur: 13.75S\n",
      " Test:  Loss: 0.035052 | Acc: 93.700% | Dur: 2.99S\n",
      "\n",
      "Epoch: 162\n",
      " Train: Loss: 0.005885 | Acc: 71.778% | Dur: 13.96S\n",
      " Test:  Loss: 0.037136 | Acc: 93.560% | Dur: 3.01S\n",
      "\n",
      "Epoch: 163\n",
      " Train: Loss: 0.004276 | Acc: 74.900% | Dur: 13.74S\n",
      " Test:  Loss: 0.034105 | Acc: 93.530% | Dur: 3.06S\n",
      "\n",
      "Epoch: 164\n",
      " Train: Loss: 0.006501 | Acc: 74.245% | Dur: 13.82S\n",
      " Test:  Loss: 0.033541 | Acc: 93.800% | Dur: 2.96S\n",
      "\n",
      "Epoch: 165\n",
      " Train: Loss: 0.005650 | Acc: 74.952% | Dur: 13.83S\n",
      " Test:  Loss: 0.034265 | Acc: 93.570% | Dur: 3.02S\n",
      "\n",
      "Epoch: 166\n",
      " Train: Loss: 0.003319 | Acc: 71.686% | Dur: 13.97S\n",
      " Test:  Loss: 0.034199 | Acc: 93.450% | Dur: 2.94S\n",
      "\n",
      "Epoch: 167\n",
      " Train: Loss: 0.005889 | Acc: 73.862% | Dur: 13.77S\n",
      " Test:  Loss: 0.035460 | Acc: 93.440% | Dur: 3.03S\n",
      "\n",
      "Epoch: 168\n",
      " Train: Loss: 0.006611 | Acc: 74.622% | Dur: 13.98S\n",
      " Test:  Loss: 0.040160 | Acc: 92.570% | Dur: 3.02S\n",
      "\n",
      "Epoch: 169\n",
      " Train: Loss: 0.004472 | Acc: 73.304% | Dur: 13.88S\n",
      " Test:  Loss: 0.032141 | Acc: 93.560% | Dur: 2.97S\n",
      "\n",
      "Epoch: 170\n",
      " Train: Loss: 0.002353 | Acc: 73.854% | Dur: 14.02S\n",
      " Test:  Loss: 0.035952 | Acc: 93.080% | Dur: 2.96S\n",
      "\n",
      "Epoch: 171\n",
      " Train: Loss: 0.006155 | Acc: 74.198% | Dur: 13.86S\n",
      " Test:  Loss: 0.036236 | Acc: 92.490% | Dur: 3.00S\n",
      "\n",
      "Epoch: 172\n",
      " Train: Loss: 0.006673 | Acc: 72.967% | Dur: 13.95S\n",
      " Test:  Loss: 0.037018 | Acc: 92.490% | Dur: 2.91S\n",
      "\n",
      "Epoch: 173\n",
      " Train: Loss: 0.003029 | Acc: 74.018% | Dur: 13.85S\n",
      " Test:  Loss: 0.034671 | Acc: 92.940% | Dur: 2.99S\n",
      "\n",
      "Epoch: 174\n",
      " Train: Loss: 0.004121 | Acc: 76.434% | Dur: 13.94S\n",
      " Test:  Loss: 0.036342 | Acc: 92.250% | Dur: 2.98S\n",
      "\n",
      "Epoch: 175\n",
      " Train: Loss: 0.003522 | Acc: 74.259% | Dur: 13.83S\n",
      " Test:  Loss: 0.039771 | Acc: 92.630% | Dur: 2.97S\n",
      "\n",
      "Epoch: 176\n",
      " Train: Loss: 0.002035 | Acc: 72.864% | Dur: 13.67S\n",
      " Test:  Loss: 0.034076 | Acc: 92.470% | Dur: 3.09S\n",
      "\n",
      "Epoch: 177\n",
      " Train: Loss: 0.004282 | Acc: 72.335% | Dur: 13.96S\n",
      " Test:  Loss: 0.040284 | Acc: 92.120% | Dur: 2.95S\n",
      "\n",
      "Epoch: 178\n",
      " Train: Loss: 0.004673 | Acc: 73.948% | Dur: 13.74S\n",
      " Test:  Loss: 0.041223 | Acc: 92.320% | Dur: 3.03S\n",
      "\n",
      "Epoch: 179\n",
      " Train: Loss: 0.006367 | Acc: 71.407% | Dur: 14.13S\n",
      " Test:  Loss: 0.039154 | Acc: 91.650% | Dur: 3.08S\n",
      "\n",
      "Epoch: 180\n",
      " Train: Loss: 0.006558 | Acc: 71.256% | Dur: 14.04S\n",
      " Test:  Loss: 0.038802 | Acc: 92.560% | Dur: 2.95S\n",
      "\n",
      "Epoch: 181\n",
      " Train: Loss: 0.004121 | Acc: 72.696% | Dur: 13.90S\n",
      " Test:  Loss: 0.042129 | Acc: 91.900% | Dur: 3.06S\n",
      "\n",
      "Epoch: 182\n",
      " Train: Loss: 0.003336 | Acc: 72.529% | Dur: 14.23S\n",
      " Test:  Loss: 0.040570 | Acc: 91.700% | Dur: 3.07S\n",
      "\n",
      "Epoch: 183\n",
      " Train: Loss: 0.006344 | Acc: 73.891% | Dur: 14.35S\n",
      " Test:  Loss: 0.043600 | Acc: 92.480% | Dur: 2.99S\n",
      "\n",
      "Epoch: 184\n",
      " Train: Loss: 0.005768 | Acc: 73.135% | Dur: 13.81S\n",
      " Test:  Loss: 0.041149 | Acc: 91.440% | Dur: 3.01S\n",
      "\n",
      "Epoch: 185\n",
      " Train: Loss: 0.004238 | Acc: 73.696% | Dur: 14.07S\n",
      " Test:  Loss: 0.038141 | Acc: 91.890% | Dur: 3.01S\n",
      "\n",
      "Epoch: 186\n",
      " Train: Loss: 0.004528 | Acc: 72.999% | Dur: 13.74S\n",
      " Test:  Loss: 0.041669 | Acc: 89.920% | Dur: 2.91S\n",
      "\n",
      "Epoch: 187\n",
      " Train: Loss: 0.006660 | Acc: 72.160% | Dur: 13.60S\n",
      " Test:  Loss: 0.038408 | Acc: 91.560% | Dur: 2.97S\n",
      "\n",
      "Epoch: 188\n",
      " Train: Loss: 0.007184 | Acc: 72.156% | Dur: 14.05S\n",
      " Test:  Loss: 0.038268 | Acc: 91.540% | Dur: 3.01S\n",
      "\n",
      "Epoch: 189\n",
      " Train: Loss: 0.004775 | Acc: 71.038% | Dur: 14.05S\n",
      " Test:  Loss: 0.043398 | Acc: 91.700% | Dur: 2.98S\n",
      "\n",
      "Epoch: 190\n",
      " Train: Loss: 0.005867 | Acc: 72.205% | Dur: 13.96S\n",
      " Test:  Loss: 0.045918 | Acc: 90.230% | Dur: 2.98S\n",
      "\n",
      "Epoch: 191\n",
      " Train: Loss: 0.005364 | Acc: 71.587% | Dur: 13.69S\n",
      " Test:  Loss: 0.040890 | Acc: 92.150% | Dur: 3.03S\n",
      "\n",
      "Epoch: 192\n",
      " Train: Loss: 0.006197 | Acc: 73.779% | Dur: 14.15S\n",
      " Test:  Loss: 0.041920 | Acc: 91.550% | Dur: 3.11S\n",
      "\n",
      "Epoch: 193\n",
      " Train: Loss: 0.002465 | Acc: 71.121% | Dur: 13.59S\n",
      " Test:  Loss: 0.043415 | Acc: 90.740% | Dur: 2.99S\n",
      "\n",
      "Epoch: 194\n",
      " Train: Loss: 0.006290 | Acc: 74.553% | Dur: 13.72S\n",
      " Test:  Loss: 0.041164 | Acc: 91.650% | Dur: 2.95S\n",
      "\n",
      "Epoch: 195\n",
      " Train: Loss: 0.004006 | Acc: 72.306% | Dur: 14.09S\n",
      " Test:  Loss: 0.041632 | Acc: 91.940% | Dur: 3.06S\n",
      "\n",
      "Epoch: 196\n",
      " Train: Loss: 0.006169 | Acc: 71.099% | Dur: 13.87S\n",
      " Test:  Loss: 0.043381 | Acc: 91.910% | Dur: 2.97S\n",
      "\n",
      "Epoch: 197\n",
      " Train: Loss: 0.006549 | Acc: 72.644% | Dur: 14.44S\n",
      " Test:  Loss: 0.045716 | Acc: 90.750% | Dur: 3.07S\n",
      "\n",
      "Epoch: 198\n",
      " Train: Loss: 0.005169 | Acc: 71.300% | Dur: 13.75S\n",
      " Test:  Loss: 0.039335 | Acc: 91.810% | Dur: 2.84S\n",
      "\n",
      "Epoch: 199\n",
      " Train: Loss: 0.006155 | Acc: 73.924% | Dur: 14.05S\n",
      " Test:  Loss: 0.042506 | Acc: 90.790% | Dur: 3.05S\n",
      "\n",
      "Epoch: 200\n",
      " Train: Loss: 0.002279 | Acc: 72.172% | Dur: 13.79S\n",
      " Test:  Loss: 0.040684 | Acc: 91.520% | Dur: 3.15S\n",
      "\n",
      "Epoch: 201\n",
      " Train: Loss: 0.006210 | Acc: 71.460% | Dur: 16.85S\n",
      " Test:  Loss: 0.047573 | Acc: 89.610% | Dur: 3.24S\n",
      "\n",
      "Epoch: 202\n",
      " Train: Loss: 0.006494 | Acc: 72.071% | Dur: 16.47S\n",
      " Test:  Loss: 0.047265 | Acc: 90.310% | Dur: 3.16S\n",
      "\n",
      "Epoch: 203\n",
      " Train: Loss: 0.006401 | Acc: 73.449% | Dur: 16.67S\n",
      " Test:  Loss: 0.040548 | Acc: 91.330% | Dur: 2.75S\n",
      "\n",
      "Epoch: 204\n",
      " Train: Loss: 0.006448 | Acc: 70.957% | Dur: 13.11S\n",
      " Test:  Loss: 0.041744 | Acc: 90.970% | Dur: 2.75S\n",
      "\n",
      "Epoch: 205\n",
      " Train: Loss: 0.001050 | Acc: 73.206% | Dur: 12.89S\n",
      " Test:  Loss: 0.039933 | Acc: 91.920% | Dur: 2.79S\n",
      "\n",
      "Epoch: 206\n",
      " Train: Loss: 0.004150 | Acc: 73.711% | Dur: 13.14S\n",
      " Test:  Loss: 0.034912 | Acc: 91.680% | Dur: 2.75S\n",
      "\n",
      "Epoch: 207\n",
      " Train: Loss: 0.005022 | Acc: 70.899% | Dur: 13.33S\n",
      " Test:  Loss: 0.042219 | Acc: 90.630% | Dur: 2.76S\n",
      "\n",
      "Epoch: 208\n",
      " Train: Loss: 0.006117 | Acc: 72.180% | Dur: 13.22S\n",
      " Test:  Loss: 0.039460 | Acc: 91.960% | Dur: 2.76S\n",
      "\n",
      "Epoch: 209\n",
      " Train: Loss: 0.004533 | Acc: 72.854% | Dur: 13.15S\n",
      " Test:  Loss: 0.038381 | Acc: 91.900% | Dur: 2.81S\n",
      "\n",
      "Epoch: 210\n",
      " Train: Loss: 0.005755 | Acc: 73.039% | Dur: 12.95S\n",
      " Test:  Loss: 0.043155 | Acc: 92.400% | Dur: 2.73S\n",
      "\n",
      "Epoch: 211\n",
      " Train: Loss: 0.003503 | Acc: 70.359% | Dur: 13.05S\n",
      " Test:  Loss: 0.041303 | Acc: 90.860% | Dur: 2.76S\n",
      "\n",
      "Epoch: 212\n",
      " Train: Loss: 0.006174 | Acc: 72.230% | Dur: 13.07S\n",
      " Test:  Loss: 0.041454 | Acc: 91.790% | Dur: 2.74S\n",
      "\n",
      "Epoch: 213\n",
      " Train: Loss: 0.006505 | Acc: 72.869% | Dur: 13.22S\n",
      " Test:  Loss: 0.034276 | Acc: 92.200% | Dur: 2.77S\n",
      "\n",
      "Epoch: 214\n",
      " Train: Loss: 0.004403 | Acc: 74.082% | Dur: 13.12S\n",
      " Test:  Loss: 0.039140 | Acc: 92.570% | Dur: 2.79S\n",
      "\n",
      "Epoch: 215\n",
      " Train: Loss: 0.001233 | Acc: 74.682% | Dur: 13.02S\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test:  Loss: 0.033769 | Acc: 92.410% | Dur: 2.75S\n",
      "\n",
      "Epoch: 216\n",
      " Train: Loss: 0.006039 | Acc: 73.278% | Dur: 12.80S\n",
      " Test:  Loss: 0.043943 | Acc: 92.490% | Dur: 2.76S\n",
      "\n",
      "Epoch: 217\n",
      " Train: Loss: 0.004079 | Acc: 74.387% | Dur: 13.23S\n",
      " Test:  Loss: 0.037846 | Acc: 92.930% | Dur: 2.79S\n",
      "\n",
      "Epoch: 218\n",
      " Train: Loss: 0.001576 | Acc: 74.027% | Dur: 13.28S\n",
      " Test:  Loss: 0.037731 | Acc: 92.450% | Dur: 2.74S\n",
      "\n",
      "Epoch: 219\n",
      " Train: Loss: 0.005720 | Acc: 74.528% | Dur: 13.03S\n",
      " Test:  Loss: 0.039850 | Acc: 92.830% | Dur: 2.83S\n",
      "\n",
      "Epoch: 220\n",
      " Train: Loss: 0.006564 | Acc: 72.662% | Dur: 13.06S\n",
      " Test:  Loss: 0.040091 | Acc: 92.410% | Dur: 2.74S\n",
      "\n",
      "Epoch: 221\n",
      " Train: Loss: 0.005028 | Acc: 73.240% | Dur: 13.14S\n",
      " Test:  Loss: 0.032790 | Acc: 93.610% | Dur: 2.80S\n",
      "\n",
      "Epoch: 222\n",
      " Train: Loss: 0.005831 | Acc: 72.016% | Dur: 12.92S\n",
      " Test:  Loss: 0.037082 | Acc: 92.690% | Dur: 2.74S\n",
      "\n",
      "Epoch: 223\n",
      " Train: Loss: 0.004482 | Acc: 72.864% | Dur: 12.98S\n",
      " Test:  Loss: 0.042090 | Acc: 92.680% | Dur: 2.75S\n",
      "\n",
      "Epoch: 224\n",
      " Train: Loss: 0.005605 | Acc: 72.654% | Dur: 13.07S\n",
      " Test:  Loss: 0.039831 | Acc: 92.860% | Dur: 2.81S\n",
      "\n",
      "Epoch: 225\n",
      " Train: Loss: 0.002178 | Acc: 71.692% | Dur: 13.07S\n",
      " Test:  Loss: 0.034965 | Acc: 93.400% | Dur: 2.74S\n",
      "\n",
      "Epoch: 226\n",
      " Train: Loss: 0.005983 | Acc: 74.689% | Dur: 13.07S\n",
      " Test:  Loss: 0.035791 | Acc: 93.680% | Dur: 2.79S\n",
      "\n",
      "Epoch: 227\n",
      " Train: Loss: 0.001277 | Acc: 72.758% | Dur: 12.88S\n",
      " Test:  Loss: 0.035940 | Acc: 93.320% | Dur: 2.80S\n",
      "\n",
      "Epoch: 228\n",
      " Train: Loss: 0.004343 | Acc: 75.125% | Dur: 13.07S\n",
      " Test:  Loss: 0.033036 | Acc: 93.880% | Dur: 2.74S\n",
      "\n",
      "Epoch: 229\n",
      " Train: Loss: 0.004039 | Acc: 73.207% | Dur: 13.07S\n",
      " Test:  Loss: 0.035293 | Acc: 93.850% | Dur: 2.75S\n",
      "\n",
      "Epoch: 230\n",
      " Train: Loss: 0.006547 | Acc: 75.198% | Dur: 13.04S\n",
      " Test:  Loss: 0.036255 | Acc: 93.640% | Dur: 2.75S\n",
      "\n",
      "Epoch: 231\n",
      " Train: Loss: 0.005458 | Acc: 74.813% | Dur: 12.86S\n",
      " Test:  Loss: 0.038534 | Acc: 93.700% | Dur: 2.72S\n",
      "\n",
      "Epoch: 232\n",
      " Train: Loss: 0.003828 | Acc: 73.447% | Dur: 13.07S\n",
      " Test:  Loss: 0.034103 | Acc: 93.930% | Dur: 2.75S\n",
      "\n",
      "Epoch: 233\n",
      " Train: Loss: 0.000307 | Acc: 74.599% | Dur: 12.98S\n",
      " Test:  Loss: 0.034755 | Acc: 94.030% | Dur: 2.77S\n",
      "\n",
      "Epoch: 234\n",
      " Train: Loss: 0.001087 | Acc: 74.703% | Dur: 13.00S\n",
      " Test:  Loss: 0.030887 | Acc: 94.290% | Dur: 2.75S\n",
      "\n",
      "Epoch: 235\n",
      " Train: Loss: 0.004125 | Acc: 73.836% | Dur: 13.22S\n",
      " Test:  Loss: 0.035147 | Acc: 94.140% | Dur: 2.72S\n",
      "\n",
      "Epoch: 236\n",
      " Train: Loss: 0.000729 | Acc: 74.918% | Dur: 13.03S\n",
      " Test:  Loss: 0.031627 | Acc: 94.470% | Dur: 2.79S\n",
      "\n",
      "Epoch: 237\n",
      " Train: Loss: 0.005440 | Acc: 74.625% | Dur: 13.11S\n",
      " Test:  Loss: 0.035386 | Acc: 94.280% | Dur: 2.78S\n",
      "\n",
      "Epoch: 238\n",
      " Train: Loss: 0.001220 | Acc: 74.808% | Dur: 13.16S\n",
      " Test:  Loss: 0.032605 | Acc: 94.470% | Dur: 2.73S\n",
      "\n",
      "Epoch: 239\n",
      " Train: Loss: 0.005727 | Acc: 76.340% | Dur: 13.03S\n",
      " Test:  Loss: 0.034223 | Acc: 94.510% | Dur: 2.81S\n",
      "\n",
      "Epoch: 240\n",
      " Train: Loss: 0.005688 | Acc: 73.784% | Dur: 13.51S\n",
      " Test:  Loss: 0.033067 | Acc: 94.440% | Dur: 2.83S\n",
      "\n",
      "Epoch: 241\n",
      " Train: Loss: 0.001568 | Acc: 71.653% | Dur: 13.24S\n",
      " Test:  Loss: 0.034666 | Acc: 94.490% | Dur: 2.85S\n",
      "\n",
      "Epoch: 242\n",
      " Train: Loss: 0.003689 | Acc: 75.290% | Dur: 13.16S\n",
      " Test:  Loss: 0.030596 | Acc: 94.700% | Dur: 2.80S\n",
      "\n",
      "Epoch: 243\n",
      " Train: Loss: 0.006089 | Acc: 75.680% | Dur: 13.14S\n",
      " Test:  Loss: 0.029663 | Acc: 94.670% | Dur: 2.75S\n",
      "\n",
      "Epoch: 244\n",
      " Train: Loss: 0.004702 | Acc: 75.749% | Dur: 13.07S\n",
      " Test:  Loss: 0.036964 | Acc: 94.510% | Dur: 2.75S\n",
      "\n",
      "Epoch: 245\n",
      " Train: Loss: 0.004958 | Acc: 75.055% | Dur: 13.41S\n",
      " Test:  Loss: 0.032892 | Acc: 94.640% | Dur: 2.73S\n",
      "\n",
      "Epoch: 246\n",
      " Train: Loss: 0.005272 | Acc: 73.924% | Dur: 13.04S\n",
      " Test:  Loss: 0.035770 | Acc: 94.480% | Dur: 2.78S\n",
      "\n",
      "Epoch: 247\n",
      " Train: Loss: 0.004686 | Acc: 75.449% | Dur: 12.88S\n",
      " Test:  Loss: 0.032977 | Acc: 94.520% | Dur: 2.78S\n",
      "\n",
      "Epoch: 248\n",
      " Train: Loss: 0.004579 | Acc: 75.608% | Dur: 13.24S\n",
      " Test:  Loss: 0.030026 | Acc: 94.780% | Dur: 2.75S\n",
      "\n",
      "Epoch: 249\n",
      " Train: Loss: 0.002773 | Acc: 74.659% | Dur: 13.04S\n",
      " Test:  Loss: 0.030963 | Acc: 94.720% | Dur: 2.78S\n",
      "\n",
      "Epoch: 250\n",
      " Train: Loss: 0.005117 | Acc: 75.059% | Dur: 13.15S\n",
      " Test:  Loss: 0.032956 | Acc: 94.650% | Dur: 2.80S\n",
      "\n",
      "Epoch: 251\n",
      " Train: Loss: 0.003887 | Acc: 75.009% | Dur: 13.06S\n",
      " Test:  Loss: 0.031718 | Acc: 94.650% | Dur: 2.77S\n",
      "\n",
      "Epoch: 252\n",
      " Train: Loss: 0.003697 | Acc: 76.514% | Dur: 13.49S\n",
      " Test:  Loss: 0.031058 | Acc: 94.600% | Dur: 2.75S\n",
      "\n",
      "Epoch: 253\n",
      " Train: Loss: 0.006267 | Acc: 73.591% | Dur: 13.11S\n",
      " Test:  Loss: 0.036463 | Acc: 94.460% | Dur: 2.77S\n",
      "\n",
      "Epoch: 254\n",
      " Train: Loss: 0.005117 | Acc: 75.838% | Dur: 13.29S\n",
      " Test:  Loss: 0.035061 | Acc: 94.440% | Dur: 2.75S\n",
      "\n",
      "Epoch: 255\n",
      " Train: Loss: 0.005502 | Acc: 74.368% | Dur: 13.55S\n",
      " Test:  Loss: 0.033522 | Acc: 94.620% | Dur: 2.84S\n",
      "\n",
      "Epoch: 256\n",
      " Train: Loss: 0.001700 | Acc: 74.563% | Dur: 13.41S\n",
      " Test:  Loss: 0.031223 | Acc: 94.870% | Dur: 2.77S\n",
      "\n",
      "Epoch: 257\n",
      " Train: Loss: 0.002296 | Acc: 74.398% | Dur: 13.12S\n",
      " Test:  Loss: 0.028543 | Acc: 94.670% | Dur: 2.79S\n",
      "\n",
      "Epoch: 258\n",
      " Train: Loss: 0.004058 | Acc: 76.236% | Dur: 13.25S\n",
      " Test:  Loss: 0.035181 | Acc: 94.640% | Dur: 2.85S\n",
      "\n",
      "Epoch: 259\n",
      " Train: Loss: 0.005879 | Acc: 74.761% | Dur: 13.40S\n",
      " Test:  Loss: 0.032446 | Acc: 94.860% | Dur: 2.77S\n",
      "\n",
      "Epoch: 260\n",
      " Train: Loss: 0.001995 | Acc: 76.179% | Dur: 13.24S\n",
      " Test:  Loss: 0.030775 | Acc: 94.710% | Dur: 2.78S\n",
      "\n",
      "Epoch: 261\n",
      " Train: Loss: 0.005494 | Acc: 73.458% | Dur: 13.29S\n",
      " Test:  Loss: 0.037178 | Acc: 94.470% | Dur: 2.79S\n",
      "\n",
      "Epoch: 262\n",
      " Train: Loss: 0.004768 | Acc: 74.583% | Dur: 13.08S\n",
      " Test:  Loss: 0.035710 | Acc: 94.330% | Dur: 2.77S\n",
      "\n",
      "Epoch: 263\n",
      " Train: Loss: 0.002516 | Acc: 76.722% | Dur: 13.08S\n",
      " Test:  Loss: 0.033232 | Acc: 94.470% | Dur: 2.79S\n",
      "\n",
      "Epoch: 264\n",
      " Train: Loss: 0.005623 | Acc: 71.611% | Dur: 13.28S\n",
      " Test:  Loss: 0.035833 | Acc: 94.210% | Dur: 2.76S\n",
      "\n",
      "Epoch: 265\n",
      " Train: Loss: 0.003049 | Acc: 75.784% | Dur: 13.25S\n",
      " Test:  Loss: 0.038273 | Acc: 94.070% | Dur: 2.78S\n",
      "\n",
      "Epoch: 266\n",
      " Train: Loss: 0.005739 | Acc: 74.547% | Dur: 13.06S\n",
      " Test:  Loss: 0.035333 | Acc: 94.310% | Dur: 2.77S\n",
      "\n",
      "Epoch: 267\n",
      " Train: Loss: 0.005529 | Acc: 74.540% | Dur: 13.12S\n",
      " Test:  Loss: 0.035207 | Acc: 94.110% | Dur: 2.75S\n",
      "\n",
      "Epoch: 268\n",
      " Train: Loss: 0.002868 | Acc: 73.108% | Dur: 13.21S\n",
      " Test:  Loss: 0.036272 | Acc: 93.930% | Dur: 2.78S\n",
      "\n",
      "Epoch: 269\n",
      " Train: Loss: 0.005530 | Acc: 75.381% | Dur: 13.06S\n",
      " Test:  Loss: 0.033996 | Acc: 93.930% | Dur: 2.74S\n",
      "\n",
      "Epoch: 270\n",
      " Train: Loss: 0.003486 | Acc: 73.868% | Dur: 13.27S\n",
      " Test:  Loss: 0.036885 | Acc: 94.040% | Dur: 2.71S\n",
      "\n",
      "Epoch: 271\n",
      " Train: Loss: 0.002518 | Acc: 74.649% | Dur: 13.01S\n",
      " Test:  Loss: 0.033337 | Acc: 93.670% | Dur: 2.73S\n",
      "\n",
      "Epoch: 272\n",
      " Train: Loss: 0.004359 | Acc: 74.496% | Dur: 12.98S\n",
      " Test:  Loss: 0.032158 | Acc: 93.760% | Dur: 2.71S\n",
      "\n",
      "Epoch: 273\n",
      " Train: Loss: 0.003481 | Acc: 74.507% | Dur: 12.90S\n",
      " Test:  Loss: 0.035996 | Acc: 93.660% | Dur: 2.69S\n",
      "\n",
      "Epoch: 274\n",
      " Train: Loss: 0.006073 | Acc: 74.919% | Dur: 13.01S\n",
      " Test:  Loss: 0.035184 | Acc: 93.330% | Dur: 2.73S\n",
      "\n",
      "Epoch: 275\n",
      " Train: Loss: 0.005058 | Acc: 72.643% | Dur: 13.25S\n",
      " Test:  Loss: 0.040347 | Acc: 93.140% | Dur: 2.81S\n",
      "\n",
      "Epoch: 276\n",
      " Train: Loss: 0.005763 | Acc: 73.863% | Dur: 13.10S\n",
      " Test:  Loss: 0.037078 | Acc: 92.920% | Dur: 2.77S\n",
      "\n",
      "Epoch: 277\n",
      " Train: Loss: 0.005504 | Acc: 73.656% | Dur: 12.66S\n",
      " Test:  Loss: 0.038418 | Acc: 92.790% | Dur: 2.74S\n",
      "\n",
      "Epoch: 278\n",
      " Train: Loss: 0.003600 | Acc: 73.096% | Dur: 12.82S\n",
      " Test:  Loss: 0.041623 | Acc: 90.720% | Dur: 2.79S\n",
      "\n",
      "Epoch: 279\n",
      " Train: Loss: 0.002452 | Acc: 72.674% | Dur: 13.21S\n",
      " Test:  Loss: 0.039603 | Acc: 92.600% | Dur: 2.77S\n",
      "\n",
      "Epoch: 280\n",
      " Train: Loss: 0.003125 | Acc: 71.706% | Dur: 12.94S\n",
      " Test:  Loss: 0.037344 | Acc: 93.120% | Dur: 2.80S\n",
      "\n",
      "Epoch: 281\n",
      " Train: Loss: 0.004625 | Acc: 73.801% | Dur: 13.12S\n",
      " Test:  Loss: 0.041558 | Acc: 90.640% | Dur: 2.72S\n",
      "\n",
      "Epoch: 282\n",
      " Train: Loss: 0.004766 | Acc: 72.556% | Dur: 13.43S\n",
      " Test:  Loss: 0.040722 | Acc: 91.800% | Dur: 2.82S\n",
      "\n",
      "Epoch: 283\n",
      " Train: Loss: 0.005569 | Acc: 74.384% | Dur: 13.27S\n",
      " Test:  Loss: 0.039462 | Acc: 92.000% | Dur: 2.75S\n",
      "\n",
      "Epoch: 284\n",
      " Train: Loss: 0.005496 | Acc: 72.269% | Dur: 13.19S\n",
      " Test:  Loss: 0.042491 | Acc: 92.110% | Dur: 2.80S\n",
      "\n",
      "Epoch: 285\n",
      " Train: Loss: 0.006162 | Acc: 74.460% | Dur: 13.20S\n",
      " Test:  Loss: 0.036888 | Acc: 92.120% | Dur: 2.74S\n",
      "\n",
      "Epoch: 286\n",
      " Train: Loss: 0.006450 | Acc: 71.704% | Dur: 13.03S\n",
      " Test:  Loss: 0.037262 | Acc: 92.380% | Dur: 2.79S\n",
      "\n",
      "Epoch: 287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Loss: 0.005995 | Acc: 72.925% | Dur: 13.20S\n",
      " Test:  Loss: 0.039111 | Acc: 92.850% | Dur: 2.76S\n",
      "\n",
      "Epoch: 288\n",
      " Train: Loss: 0.005435 | Acc: 73.609% | Dur: 13.02S\n",
      " Test:  Loss: 0.037621 | Acc: 92.920% | Dur: 2.79S\n",
      "\n",
      "Epoch: 289\n",
      " Train: Loss: 0.001619 | Acc: 72.012% | Dur: 13.20S\n",
      " Test:  Loss: 0.043174 | Acc: 92.010% | Dur: 2.79S\n",
      "\n",
      "Epoch: 290\n",
      " Train: Loss: 0.005578 | Acc: 75.890% | Dur: 13.04S\n",
      " Test:  Loss: 0.039668 | Acc: 91.640% | Dur: 2.80S\n",
      "\n",
      "Epoch: 291\n",
      " Train: Loss: 0.001348 | Acc: 73.890% | Dur: 12.97S\n",
      " Test:  Loss: 0.042233 | Acc: 92.540% | Dur: 2.74S\n",
      "\n",
      "Epoch: 292\n",
      " Train: Loss: 0.001897 | Acc: 72.624% | Dur: 13.10S\n",
      " Test:  Loss: 0.038836 | Acc: 92.270% | Dur: 2.79S\n",
      "\n",
      "Epoch: 293\n",
      " Train: Loss: 0.006316 | Acc: 73.059% | Dur: 13.08S\n",
      " Test:  Loss: 0.042070 | Acc: 91.540% | Dur: 2.77S\n",
      "\n",
      "Epoch: 294\n",
      " Train: Loss: 0.004214 | Acc: 72.070% | Dur: 13.00S\n",
      " Test:  Loss: 0.043386 | Acc: 91.460% | Dur: 2.73S\n",
      "\n",
      "Epoch: 295\n",
      " Train: Loss: 0.001261 | Acc: 71.973% | Dur: 13.02S\n",
      " Test:  Loss: 0.037596 | Acc: 91.880% | Dur: 2.79S\n",
      "\n",
      "Epoch: 296\n",
      " Train: Loss: 0.005561 | Acc: 74.315% | Dur: 13.16S\n",
      " Test:  Loss: 0.042464 | Acc: 91.170% | Dur: 2.79S\n",
      "\n",
      "Epoch: 297\n",
      " Train: Loss: 0.003301 | Acc: 74.047% | Dur: 13.12S\n",
      " Test:  Loss: 0.034711 | Acc: 92.300% | Dur: 2.79S\n",
      "\n",
      "Epoch: 298\n",
      " Train: Loss: 0.006686 | Acc: 72.264% | Dur: 13.03S\n",
      " Test:  Loss: 0.040277 | Acc: 91.030% | Dur: 2.72S\n",
      "\n",
      "Epoch: 299\n",
      " Train: Loss: 0.006675 | Acc: 71.892% | Dur: 12.87S\n",
      " Test:  Loss: 0.043538 | Acc: 91.540% | Dur: 2.76S\n",
      "\n",
      "Epoch: 300\n",
      " Train: Loss: 0.000923 | Acc: 73.411% | Dur: 13.06S\n",
      " Test:  Loss: 0.034472 | Acc: 92.420% | Dur: 2.76S\n",
      "\n",
      "Epoch: 301\n",
      " Train: Loss: 0.005462 | Acc: 74.000% | Dur: 13.29S\n",
      " Test:  Loss: 0.040884 | Acc: 91.840% | Dur: 2.90S\n",
      "\n",
      "Epoch: 302\n",
      " Train: Loss: 0.004278 | Acc: 72.813% | Dur: 13.22S\n",
      " Test:  Loss: 0.038273 | Acc: 92.230% | Dur: 2.79S\n",
      "\n",
      "Epoch: 303\n",
      " Train: Loss: 0.005406 | Acc: 71.833% | Dur: 13.34S\n",
      " Test:  Loss: 0.042712 | Acc: 92.380% | Dur: 2.77S\n",
      "\n",
      "Epoch: 304\n",
      " Train: Loss: 0.003987 | Acc: 73.329% | Dur: 13.30S\n",
      " Test:  Loss: 0.037326 | Acc: 91.290% | Dur: 2.79S\n",
      "\n",
      "Epoch: 305\n",
      " Train: Loss: 0.004902 | Acc: 71.535% | Dur: 13.06S\n",
      " Test:  Loss: 0.044516 | Acc: 90.520% | Dur: 2.85S\n",
      "\n",
      "Epoch: 306\n",
      " Train: Loss: 0.001124 | Acc: 72.602% | Dur: 13.48S\n",
      " Test:  Loss: 0.040302 | Acc: 91.740% | Dur: 2.76S\n",
      "\n",
      "Epoch: 307\n",
      " Train: Loss: 0.004802 | Acc: 73.022% | Dur: 12.97S\n",
      " Test:  Loss: 0.040967 | Acc: 92.220% | Dur: 2.78S\n",
      "\n",
      "Epoch: 308\n",
      " Train: Loss: 0.007197 | Acc: 73.602% | Dur: 13.33S\n",
      " Test:  Loss: 0.040234 | Acc: 92.300% | Dur: 2.80S\n",
      "\n",
      "Epoch: 309\n",
      " Train: Loss: 0.006157 | Acc: 72.118% | Dur: 13.26S\n",
      " Test:  Loss: 0.040701 | Acc: 91.910% | Dur: 2.78S\n",
      "\n",
      "Epoch: 310\n",
      " Train: Loss: 0.001605 | Acc: 73.082% | Dur: 13.08S\n",
      " Test:  Loss: 0.039555 | Acc: 92.510% | Dur: 2.82S\n",
      "\n",
      "Epoch: 311\n",
      " Train: Loss: 0.005761 | Acc: 74.258% | Dur: 13.00S\n",
      " Test:  Loss: 0.040891 | Acc: 92.790% | Dur: 2.69S\n",
      "\n",
      "Epoch: 312\n",
      " Train: Loss: 0.006241 | Acc: 71.190% | Dur: 12.84S\n",
      " Test:  Loss: 0.046043 | Acc: 91.570% | Dur: 2.78S\n",
      "\n",
      "Epoch: 313\n",
      " Train: Loss: 0.000546 | Acc: 71.247% | Dur: 13.08S\n",
      " Test:  Loss: 0.035848 | Acc: 92.660% | Dur: 2.73S\n",
      "\n",
      "Epoch: 314\n",
      " Train: Loss: 0.006184 | Acc: 71.874% | Dur: 13.10S\n",
      " Test:  Loss: 0.040971 | Acc: 91.970% | Dur: 2.75S\n",
      "\n",
      "Epoch: 315\n",
      " Train: Loss: 0.005647 | Acc: 73.887% | Dur: 12.94S\n",
      " Test:  Loss: 0.035218 | Acc: 93.260% | Dur: 2.75S\n",
      "\n",
      "Epoch: 316\n",
      " Train: Loss: 0.003725 | Acc: 72.338% | Dur: 13.04S\n",
      " Test:  Loss: 0.038938 | Acc: 92.300% | Dur: 2.78S\n",
      "\n",
      "Epoch: 317\n",
      " Train: Loss: 0.000394 | Acc: 76.054% | Dur: 13.53S\n",
      " Test:  Loss: 0.038139 | Acc: 93.260% | Dur: 2.81S\n",
      "\n",
      "Epoch: 318\n",
      " Train: Loss: 0.006045 | Acc: 74.146% | Dur: 13.12S\n",
      " Test:  Loss: 0.035369 | Acc: 93.120% | Dur: 2.80S\n",
      "\n",
      "Epoch: 319\n",
      " Train: Loss: 0.005885 | Acc: 73.521% | Dur: 13.16S\n",
      " Test:  Loss: 0.036412 | Acc: 93.410% | Dur: 2.79S\n",
      "\n",
      "Epoch: 320\n",
      " Train: Loss: 0.006186 | Acc: 73.258% | Dur: 13.10S\n",
      " Test:  Loss: 0.035133 | Acc: 93.750% | Dur: 2.75S\n",
      "\n",
      "Epoch: 321\n",
      " Train: Loss: 0.002777 | Acc: 73.020% | Dur: 13.09S\n",
      " Test:  Loss: 0.037530 | Acc: 93.290% | Dur: 2.78S\n",
      "\n",
      "Epoch: 322\n",
      " Train: Loss: 0.006107 | Acc: 74.190% | Dur: 13.04S\n",
      " Test:  Loss: 0.037400 | Acc: 93.520% | Dur: 2.75S\n",
      "\n",
      "Epoch: 323\n",
      " Train: Loss: 0.006499 | Acc: 73.851% | Dur: 13.13S\n",
      " Test:  Loss: 0.038498 | Acc: 93.290% | Dur: 2.75S\n",
      "\n",
      "Epoch: 324\n",
      " Train: Loss: 0.004705 | Acc: 73.216% | Dur: 12.79S\n",
      " Test:  Loss: 0.032849 | Acc: 93.720% | Dur: 2.72S\n",
      "\n",
      "Epoch: 325\n",
      " Train: Loss: 0.003661 | Acc: 74.974% | Dur: 13.18S\n",
      " Test:  Loss: 0.033450 | Acc: 94.020% | Dur: 2.80S\n",
      "\n",
      "Epoch: 326\n",
      " Train: Loss: 0.006439 | Acc: 75.351% | Dur: 12.95S\n",
      " Test:  Loss: 0.033138 | Acc: 93.940% | Dur: 2.78S\n",
      "\n",
      "Epoch: 327\n",
      " Train: Loss: 0.005218 | Acc: 74.607% | Dur: 13.03S\n",
      " Test:  Loss: 0.037502 | Acc: 94.130% | Dur: 2.75S\n",
      "\n",
      "Epoch: 328\n",
      " Train: Loss: 0.006429 | Acc: 73.781% | Dur: 13.04S\n",
      " Test:  Loss: 0.035113 | Acc: 94.070% | Dur: 2.80S\n",
      "\n",
      "Epoch: 329\n",
      " Train: Loss: 0.002521 | Acc: 72.712% | Dur: 13.15S\n",
      " Test:  Loss: 0.034557 | Acc: 93.930% | Dur: 2.74S\n",
      "\n",
      "Epoch: 330\n",
      " Train: Loss: 0.005179 | Acc: 71.999% | Dur: 13.00S\n",
      " Test:  Loss: 0.034999 | Acc: 94.050% | Dur: 2.76S\n",
      "\n",
      "Epoch: 331\n",
      " Train: Loss: 0.005502 | Acc: 73.336% | Dur: 13.27S\n",
      " Test:  Loss: 0.035546 | Acc: 93.910% | Dur: 2.74S\n",
      "\n",
      "Epoch: 332\n",
      " Train: Loss: 0.000386 | Acc: 73.068% | Dur: 13.23S\n",
      " Test:  Loss: 0.030939 | Acc: 94.460% | Dur: 2.75S\n",
      "\n",
      "Epoch: 333\n",
      " Train: Loss: 0.002981 | Acc: 71.787% | Dur: 13.03S\n",
      " Test:  Loss: 0.035142 | Acc: 94.320% | Dur: 2.78S\n",
      "\n",
      "Epoch: 334\n",
      " Train: Loss: 0.003815 | Acc: 73.222% | Dur: 13.33S\n",
      " Test:  Loss: 0.036771 | Acc: 94.310% | Dur: 2.81S\n",
      "\n",
      "Epoch: 335\n",
      " Train: Loss: 0.001833 | Acc: 73.764% | Dur: 13.11S\n",
      " Test:  Loss: 0.031457 | Acc: 94.700% | Dur: 2.81S\n",
      "\n",
      "Epoch: 336\n",
      " Train: Loss: 0.005852 | Acc: 73.574% | Dur: 12.75S\n",
      " Test:  Loss: 0.037326 | Acc: 94.250% | Dur: 2.76S\n",
      "\n",
      "Epoch: 337\n",
      " Train: Loss: 0.000823 | Acc: 74.364% | Dur: 12.87S\n",
      " Test:  Loss: 0.033965 | Acc: 94.610% | Dur: 2.77S\n",
      "\n",
      "Epoch: 338\n",
      " Train: Loss: 0.005335 | Acc: 75.706% | Dur: 13.08S\n",
      " Test:  Loss: 0.037417 | Acc: 94.280% | Dur: 2.77S\n",
      "\n",
      "Epoch: 339\n",
      " Train: Loss: 0.004541 | Acc: 75.669% | Dur: 13.14S\n",
      " Test:  Loss: 0.032187 | Acc: 94.710% | Dur: 2.73S\n",
      "\n",
      "Epoch: 340\n",
      " Train: Loss: 0.005673 | Acc: 78.147% | Dur: 13.13S\n",
      " Test:  Loss: 0.035416 | Acc: 94.480% | Dur: 2.74S\n",
      "\n",
      "Epoch: 341\n",
      " Train: Loss: 0.005668 | Acc: 74.726% | Dur: 13.38S\n",
      " Test:  Loss: 0.035069 | Acc: 94.470% | Dur: 2.78S\n",
      "\n",
      "Epoch: 342\n",
      " Train: Loss: 0.005099 | Acc: 76.301% | Dur: 13.37S\n",
      " Test:  Loss: 0.032839 | Acc: 94.710% | Dur: 2.75S\n",
      "\n",
      "Epoch: 343\n",
      " Train: Loss: 0.000756 | Acc: 74.601% | Dur: 12.97S\n",
      " Test:  Loss: 0.032470 | Acc: 94.690% | Dur: 2.82S\n",
      "\n",
      "Epoch: 344\n",
      " Train: Loss: 0.002607 | Acc: 76.516% | Dur: 12.91S\n",
      " Test:  Loss: 0.028837 | Acc: 95.040% | Dur: 2.78S\n",
      "\n",
      "Epoch: 345\n",
      " Train: Loss: 0.004962 | Acc: 74.736% | Dur: 12.85S\n",
      " Test:  Loss: 0.031816 | Acc: 94.790% | Dur: 2.75S\n",
      "\n",
      "Epoch: 346\n",
      " Train: Loss: 0.006073 | Acc: 77.207% | Dur: 13.11S\n",
      " Test:  Loss: 0.030913 | Acc: 94.860% | Dur: 2.76S\n",
      "\n",
      "Epoch: 347\n",
      " Train: Loss: 0.004565 | Acc: 75.334% | Dur: 13.01S\n",
      " Test:  Loss: 0.034002 | Acc: 94.640% | Dur: 2.75S\n",
      "\n",
      "Epoch: 348\n",
      " Train: Loss: 0.004901 | Acc: 75.025% | Dur: 13.03S\n",
      " Test:  Loss: 0.030737 | Acc: 94.850% | Dur: 2.82S\n",
      "\n",
      "Epoch: 349\n",
      " Train: Loss: 0.005374 | Acc: 74.386% | Dur: 13.05S\n",
      " Test:  Loss: 0.033696 | Acc: 94.770% | Dur: 2.80S\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(350):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c8ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.759209213256835, 44.28160115051269, 50.065521484375, 54.132131088256834, 55.68267283630371, 57.88259049987793, 59.24815960693359, 61.21552741241455, 61.60835682678223, 59.85467024993896, 62.10626628112793, 61.1074493637085, 64.27416383361816, 65.85942449951172, 63.43105924224854, 65.84024417114257, 67.34741439819337, 66.43781379699708, 68.37329998779298, 66.642528175354, 66.77693518066407, 65.43033251953125, 67.77642176055909, 68.50190144348144, 67.92922073364258, 70.0279907913208, 65.82971710205078, 71.84295616149902, 66.97820288848877, 70.67465957641602, 68.83794281768799, 68.56817457580567, 70.72574715423583, 70.36296324920654, 70.61705590820313, 68.9280705871582, 70.59980050659179, 71.49171892547608, 70.4856842803955, 71.16313313293458, 70.18368190002441, 71.7357285079956, 69.45175349426269, 69.57228665161132, 70.30346116638184, 70.75873105621338, 70.54133492279053, 70.3129175567627, 72.58843574523925, 72.38545442199707, 71.02622521972657, 72.28420262145995, 68.93374617004395, 72.09254968261719, 71.4766541748047, 71.58340521240234, 69.87062213134766, 72.31495050811768, 72.44764370727539, 69.75993702697754, 70.04001876831055, 71.85250994110108, 69.35387725067139, 71.88036692810059, 71.46425408935546, 71.51001614379882, 71.42836177062988, 68.5809623260498, 74.35515406799317, 70.39238426208496, 71.52798805999755, 70.41732321166992, 68.31455612182617, 70.99911940002441, 70.74769506072998, 70.68129302215576, 69.12600582122803, 69.18421749877929, 68.74428250885009, 71.2158992767334, 70.92701832580566, 69.67010238647461, 70.11556769561767, 71.49820695495606, 71.09900675201416, 70.13093406677245, 69.72794532775879, 69.90241413879394, 69.7940771484375, 70.35201235198974, 69.2972236480713, 69.3491943435669, 71.56074011993408, 69.77308772277831, 72.55142872619629, 71.37436032104492, 71.248228225708, 69.0078802947998, 69.60236252593994, 67.56327960968018, 68.72435112762452, 70.61265544891357, 70.80774068450928, 70.95834889221192, 70.95792514038087, 71.72580363464355, 70.65522203826905, 70.82339591979981, 71.71106786346435, 71.73154650878907, 70.63089094543457, 71.62170881652833, 72.4326471786499, 74.34916804504394, 70.38715181732178, 72.43115315246582, 72.80870825958252, 72.95723665618897, 72.25555513000488, 72.1582533493042, 71.60651210021973, 72.25904066467285, 71.47861399078369, 73.2011986541748, 73.0640318145752, 72.17631423187255, 72.77492431640626, 76.40211393737793, 74.81583865356446, 73.73197541809083, 73.26872282409668, 73.37267951965332, 75.19403323364257, 73.15371733093262, 72.61762983703613, 75.75207890319824, 72.81064883422852, 74.93811663818359, 73.05749898529052, 75.40389253997803, 74.09995254516602, 74.44701586914063, 75.54260263061524, 75.2537485961914, 72.69103948974609, 75.51753504943848, 74.31611108398438, 74.966825881958, 73.00271160888671, 75.95747061157226, 73.59708235168458, 74.5494785232544, 73.49485295104981, 74.15879544067383, 74.92474243927002, 72.62369972229004, 74.88223223876953, 75.1108826675415, 73.82136594390869, 74.37932467651368, 71.90550979614258, 73.6470512084961, 71.77830600738525, 74.89965856933594, 74.24464778137207, 74.95191255950928, 71.68641673278809, 73.86227707672118, 74.62164320373535, 73.30417269897461, 73.85409188842773, 74.19801462554932, 72.96708158874512, 74.01806155395508, 76.43366786193847, 74.25858445739746, 72.86414918518066, 72.335482711792, 73.94818804168702, 71.40708209991455, 71.25556805419922, 72.69568585205079, 72.52887658691407, 73.89090969848633, 73.13549938964844, 73.69635420227051, 72.99924369812011, 72.16024560546875, 72.15550591278077, 71.03838217163086, 72.20493824005126, 71.58709590148926, 73.77908290863037, 71.12072557067872, 74.55296636199951, 72.30564762878419, 71.09867069244385, 72.64426981353759, 71.3002293624878, 73.9243921661377, 72.17200001525879, 71.45980563354492, 72.07121144866943, 73.44877830505371, 70.957134765625, 73.20592256164551, 73.71127140808106, 70.8994697265625, 72.18040529632569, 72.8539475479126, 73.03886254882812, 70.3589252319336, 72.23031188964843, 72.86873463439942, 74.08220413208008, 74.68162322998047, 73.27798993682862, 74.38682960510253, 74.02741104125977, 74.52792427062988, 72.66173196411133, 73.23989777374267, 72.01592751312256, 72.86382527160644, 72.65381607055664, 71.69154444885254, 74.68944186401367, 72.75810585021972, 75.12530911254883, 73.20726000976562, 75.19807011413575, 74.81334101867675, 73.44718215942383, 74.59901080322265, 74.70285227966309, 73.83636360931396, 74.91808665466309, 74.62533962249756, 74.80750291442871, 76.34036540985107, 73.78355123901368, 71.65289004516602, 75.28970231628418, 75.67992472076416, 75.7485205078125, 75.05513764953614, 73.92442686462402, 75.4485199432373, 75.60754971313476, 74.65906163024903, 75.05909384155274, 75.00891893005371, 76.51402261352538, 73.59132313537597, 75.83774232482911, 74.36836752319336, 74.56267010498047, 74.3983882598877, 76.23593236541748, 74.76099112701417, 76.17899018859863, 73.45785404968262, 74.58280612182617, 76.72164544677734, 71.61102527618408, 75.78389646911621, 74.54704313659668, 74.53989282226563, 73.10792811584473, 75.38084186553955, 73.86790696716308, 74.64862161254882, 74.4962998046875, 74.50687101745605, 74.91901376342773, 72.6431078338623, 73.8629518661499, 73.656009475708, 73.09625965881348, 72.67430192565918, 71.70617160034179, 73.80107224273682, 72.55555809020996, 74.38402435302734, 72.26879650878907, 74.46041259002685, 71.70447737121582, 72.92455418395996, 73.60918055725098, 72.01248979187012, 75.8898271484375, 73.88976719665527, 72.62365161132813, 73.05903108215333, 72.06988990783691, 71.97327850341797, 74.31461793518066, 74.04660095214844, 72.2635548248291, 71.89196087646485, 73.41054011535644, 73.9996076965332, 72.81321437835693, 71.83313756561279, 73.32934408569336, 71.53473321533203, 72.60227177429199, 73.02164225769043, 73.60216744232177, 72.11799662780761, 73.08247666931152, 74.2576357574463, 71.19027058410644, 71.24710452270507, 71.87380821990966, 73.88706939697266, 72.33791763305663, 76.05365786743164, 74.1464182434082, 73.52075853729248, 73.2577317276001, 73.01987265014648, 74.19000484466552, 73.85063394165039, 73.2156250076294, 74.97429818725585, 75.3508678970337, 74.60732344055175, 73.78075561523437, 72.71175288391113, 71.99932926940917, 73.33566462707519, 73.0681551513672, 71.78651870727539, 73.22222332763671, 73.7640806274414, 73.5737487411499, 74.36418481445313, 75.70634464263917, 75.66941912078858, 78.147412399292, 74.72568552398681, 76.30087358093262, 74.60060977172851, 76.51584272766114, 74.73644668579101, 77.20735607147216, 75.33404376220703, 75.02541355895995, 74.38595233917236]\n",
      "[0.009293059305268891, 0.008121158395494734, 0.00789737275668553, 0.008758401384159011, 0.004556005706592482, 0.005761709140271557, 0.008644763912473406, 0.005211522992776364, 0.008461644454878204, 0.006942053230441347, 0.0072134264877864295, 0.005950033664703369, 0.0036714648713870924, 0.003009045002411823, 0.006689612962761704, 0.007662258586105035, 0.0023385202702210874, 0.003614809440106762, 0.0021657267091225603, 0.005543632166726249, 0.006051048940541793, 0.005270826573274574, 0.00698017100898587, 0.005564491967765653, 0.00768658093043736, 0.007195677076067243, 0.001981761838708605, 0.002277147405001582, 0.006000175768015336, 0.0016770460167709663, 0.006433925458363124, 0.004448582627335373, 0.007524168613005658, 0.004829867153751607, 0.007068118878773281, 0.007449740049790363, 0.001047701403802755, 0.007039699019217978, 0.0025544345987086395, 0.006424471431848954, 0.005239779851874527, 0.007410278125685088, 0.003081801898625432, 0.006190651533555011, 0.007449202391566063, 0.006366728519906803, 0.006541853656574171, 0.002720980619897648, 0.006492834918353022, 0.005245434994600257, 0.0031988967438133396, 0.006152160921875312, 0.006600534429355543, 0.0015352061208413572, 0.0056092915486316295, 0.007151018600074613, 0.005723737332285667, 0.004409194904930738, 0.006903210464788943, 0.002842676274630488, 0.001901906819976106, 0.006423013550894601, 0.006819843637699984, 0.0037489953089733514, 0.007394965814084423, 0.0032115280628204346, 0.006927316286125962, 0.0064867090205756985, 0.003712093039434783, 0.0010145742978368486, 0.005970066299243849, 0.005942069754308584, 0.007289269748999148, 0.0023840063688706377, 0.0055238276111836335, 0.005147121390517877, 0.006888238142947762, 0.006576795359047091, 0.007257193935160734, 0.006658967052187238, 0.007231048175266811, 0.0023346369972034376, 0.005763878019488588, 0.006921140515074438, 0.0075596546640201495, 0.006398679650559717, 0.004323393106460571, 0.006735833323731714, 0.002355587117525996, 0.0060429767686493545, 0.00736898913675425, 0.007170289146656893, 0.005800702742167881, 0.005470395088195801, 0.0021745316228088066, 0.007416556684338316, 0.004391678133789374, 0.006740649743955962, 0.005159109830856323, 0.007165847992410465, 0.004682651587894985, 0.0042750537395477295, 0.005911170219888492, 0.006565596376146589, 0.005535273527612491, 0.004840837449443584, 0.006903684869104502, 0.00559118998293974, 0.00516386299717183, 0.004894824964659554, 0.0062195427563725685, 0.006769087849831094, 0.007299331986174292, 0.005157294930243979, 0.0058278252883833285, 0.001782663926786306, 0.006260118922408746, 0.006456789921741097, 0.002885943164630812, 0.005899052230679259, 0.006234224961728466, 0.0021823051632667073, 0.005857394666087871, 0.003278962811645196, 0.006535863389774244, 0.00558399180976712, 0.006624710803129235, 0.006245336970504449, 0.006126513286512725, 0.0070165614692532284, 0.0028389874769716847, 0.0059240667187437715, 0.0005005739857347645, 0.006896082844052996, 0.005274593830108643, 0.0035701193371597603, 0.003718904390627024, 0.002912081625996804, 0.006715502665967358, 0.006122382319703394, 0.005826146626959042, 0.0013463186974428138, 0.005707816201813367, 0.005117309336759606, 0.005141576333921783, 0.002003648299343732, 0.00406179987654394, 0.002065529932781142, 0.0014597329558158408, 0.0034461365062363296, 0.00405447002576322, 0.006131899600126305, 0.001555045648496978, 0.004810189714237135, 0.004518398825003177, 0.006030075404108787, 0.004080652886507463, 0.006292400311450569, 0.004710362882030254, 0.00584568478623215, 0.005780067979073038, 0.006429941070323087, 0.005884727653192014, 0.0042758857717319415, 0.006500932635093222, 0.00564990724836077, 0.0033190156124076066, 0.005889207732920744, 0.00661090442112514, 0.004471983228410993, 0.002352526601479978, 0.006155067560624103, 0.006673426044230558, 0.003029420059554431, 0.004121015874706969, 0.0035220031835594954, 0.002034701863113715, 0.004281842282840184, 0.004673396446266953, 0.006366818535084627, 0.006557872708962888, 0.004120695347688636, 0.0033356979185221146, 0.00634367003732798, 0.005767844769419456, 0.00423773819086503, 0.004528346718574057, 0.0066599772900951155, 0.007183747632162911, 0.0047749356955898055, 0.0058671935480468125, 0.005364470944112661, 0.006196583412131485, 0.0024653077125549316, 0.006290434574594303, 0.004005746269712643, 0.006168509016231615, 0.006548528160367694, 0.005168893507548741, 0.006155408766804909, 0.002279255159047185, 0.006209597295644332, 0.006493973488710365, 0.0064009501009571305, 0.006447940456623934, 0.001049532498023948, 0.004149918045316424, 0.0050221632937995754, 0.006117483791039914, 0.0045328383543053455, 0.005754650855550961, 0.003502995383982756, 0.006173544392293813, 0.006504996090519185, 0.0044027101628634395, 0.0012332793249159443, 0.006039277631409315, 0.0040785591213070614, 0.0015759262807515202, 0.005719758418141579, 0.006564364141347457, 0.005028159338600782, 0.005830683878489903, 0.004481703347089339, 0.005605284048586475, 0.002178459599310038, 0.005983487683899549, 0.001276725865140253, 0.00434270044978784, 0.004038887364523751, 0.00654688295052976, 0.0054578908852168494, 0.0038284361362457275, 0.0003068050255580824, 0.0010866074233638998, 0.004125332345767897, 0.0007292010954448155, 0.005440363470388919, 0.001219941113068133, 0.005726987001847247, 0.00568782918307246, 0.0015679681483580141, 0.00368890287924786, 0.006089309040380984, 0.00470175001086021, 0.004958160373629356, 0.005272315472972636, 0.004686483315059117, 0.004579021614425036, 0.0027727311363025587, 0.005116617193027418, 0.003887230036210041, 0.0036966475296993646, 0.006266919934019751, 0.005117180396099479, 0.005501657724380493, 0.0017001276113549058, 0.0022960826754570007, 0.004057639715622882, 0.005878814629146031, 0.001994884743982432, 0.005493711452094876, 0.004767762154948955, 0.0025155115492489872, 0.00562305718052144, 0.003049105405807495, 0.005738553343986978, 0.005528678699415557, 0.002867614432257049, 0.0055298142287195946, 0.0034860819578170776, 0.0025176757148333956, 0.004359378194322391, 0.0034813847468823803, 0.006073149491329582, 0.0050577591268383725, 0.0057630611925709, 0.005503555949853391, 0.0036001254101188816, 0.0024521451215354763, 0.003125106193581406, 0.004624842989201449, 0.004765992870136183, 0.005569348529893525, 0.005495657726209991, 0.0061619610202555755, 0.0064502510489249714, 0.005995409221065288, 0.005435472848464032, 0.0016193523698923538, 0.005578075136457171, 0.0013475068369690254, 0.0018969752958842687, 0.006316436796772237, 0.004213767088189417, 0.0012612418860805277, 0.005560505146882972, 0.0033011080659165674, 0.006685637089670921, 0.006675265273269342, 0.0009233011883132312, 0.00546215261731829, 0.004277541929361772, 0.005406169867028995, 0.003987001521246774, 0.004902354916747735, 0.0011239132406760234, 0.004801753832369434, 0.007196580268898789, 0.006157381193978446, 0.0016045536921948803, 0.005760588208023383, 0.0062407328158008805, 0.0005457363262468455, 0.006183999533555946, 0.005646611354788955, 0.003725132163690061, 0.00039351465446608406, 0.00604469253092396, 0.005884686294867068, 0.00618552066841904, 0.0027768368623694597, 0.006107323023737694, 0.006498992443084717, 0.004705007587160383, 0.0036610711594017185, 0.006438875684932787, 0.005217735864678208, 0.006428570163493253, 0.0025214498140374007, 0.005179488537262897, 0.0055018608667412585, 0.00038621384574442496, 0.0029808571752236814, 0.003814719769419456, 0.0018325444995140542, 0.005852012001738256, 0.000822513079156681, 0.005334848652080614, 0.004540543166958556, 0.005672533901370302, 0.005668023411108523, 0.005098527791548749, 0.0007563348935574902, 0.0026072662095634304, 0.004962093976079201, 0.0060728459942097565, 0.004564621618815831, 0.004901062469093167, 0.0053735290254865375]\n"
     ]
    }
   ],
   "source": [
    "print(train_acc_list)\n",
    "print(train_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81189086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.73, 55.03, 61.39, 70.76, 71.02, 72.12, 75.96, 76.23, 78.98, 79.8, 82.22, 82.39, 79.52, 82.98, 84.54, 82.91, 82.99, 84.24, 85.62, 85.6, 85.44, 86.44, 87.11, 87.81, 87.92, 87.07, 88.74, 87.62, 87.72, 88.83, 88.37, 89.21, 90.02, 89.47, 90.05, 90.3, 90.62, 90.49, 91.12, 90.9, 90.96, 90.94, 91.56, 91.57, 90.94, 91.28, 91.53, 91.3, 91.17, 91.31, 91.37, 91.37, 91.11, 91.54, 91.63, 91.49, 91.11, 91.43, 91.15, 91.4, 90.76, 91.1, 91.09, 90.81, 90.52, 89.89, 90.03, 90.15, 89.12, 90.14, 88.62, 89.03, 88.51, 89.71, 90.17, 89.95, 85.81, 90.05, 90.12, 88.5, 87.41, 88.77, 89.07, 87.51, 87.72, 83.98, 88.9, 89.52, 87.99, 89.39, 89.38, 87.86, 86.04, 89.18, 89.41, 86.93, 90.28, 88.88, 88.86, 88.1, 87.87, 88.54, 90.39, 89.78, 89.19, 89.94, 90.82, 88.88, 91.18, 88.52, 91.62, 91.35, 89.86, 91.37, 90.21, 91.79, 90.13, 90.99, 92.07, 91.96, 92.05, 92.17, 92.54, 92.6, 92.66, 92.75, 92.77, 92.66, 92.46, 93.2, 93.24, 93.26, 93.23, 93.03, 93.38, 93.48, 93.37, 93.98, 93.84, 93.58, 93.82, 94.01, 93.97, 93.92, 93.69, 94.11, 93.92, 93.97, 94.06, 93.98, 93.93, 93.92, 94.01, 94.02, 94.07, 93.92, 94.04, 94.01, 93.96, 94.18, 93.74, 93.7, 93.56, 93.53, 93.8, 93.57, 93.45, 93.44, 92.57, 93.56, 93.08, 92.49, 92.49, 92.94, 92.25, 92.63, 92.47, 92.12, 92.32, 91.65, 92.56, 91.9, 91.7, 92.48, 91.44, 91.89, 89.92, 91.56, 91.54, 91.7, 90.23, 92.15, 91.55, 90.74, 91.65, 91.94, 91.91, 90.75, 91.81, 90.79, 91.52, 89.61, 90.31, 91.33, 90.97, 91.92, 91.68, 90.63, 91.96, 91.9, 92.4, 90.86, 91.79, 92.2, 92.57, 92.41, 92.49, 92.93, 92.45, 92.83, 92.41, 93.61, 92.69, 92.68, 92.86, 93.4, 93.68, 93.32, 93.88, 93.85, 93.64, 93.7, 93.93, 94.03, 94.29, 94.14, 94.47, 94.28, 94.47, 94.51, 94.44, 94.49, 94.7, 94.67, 94.51, 94.64, 94.48, 94.52, 94.78, 94.72, 94.65, 94.65, 94.6, 94.46, 94.44, 94.62, 94.87, 94.67, 94.64, 94.86, 94.71, 94.47, 94.33, 94.47, 94.21, 94.07, 94.31, 94.11, 93.93, 93.93, 94.04, 93.67, 93.76, 93.66, 93.33, 93.14, 92.92, 92.79, 90.72, 92.6, 93.12, 90.64, 91.8, 92.0, 92.11, 92.12, 92.38, 92.85, 92.92, 92.01, 91.64, 92.54, 92.27, 91.54, 91.46, 91.88, 91.17, 92.3, 91.03, 91.54, 92.42, 91.84, 92.23, 92.38, 91.29, 90.52, 91.74, 92.22, 92.3, 91.91, 92.51, 92.79, 91.57, 92.66, 91.97, 93.26, 92.3, 93.26, 93.12, 93.41, 93.75, 93.29, 93.52, 93.29, 93.72, 94.02, 93.94, 94.13, 94.07, 93.93, 94.05, 93.91, 94.46, 94.32, 94.31, 94.7, 94.25, 94.61, 94.28, 94.71, 94.48, 94.47, 94.71, 94.69, 95.04, 94.79, 94.86, 94.64, 94.85, 94.77]\n",
      "[0.13287268877029418, 0.1304006338119507, 0.1133691668510437, 0.09068434238433838, 0.08312815427780151, 0.08708729743957519, 0.07579782605171204, 0.07258055806159973, 0.07069234251976013, 0.06694512963294982, 0.05893650054931641, 0.06240007281303406, 0.06963674426078796, 0.05285554528236389, 0.05753059983253479, 0.05443357825279236, 0.05643715262413025, 0.05738434791564941, 0.0505585253238678, 0.054230129718780516, 0.05166561007499695, 0.05087444186210632, 0.051004558801651, 0.045755508542060855, 0.04790882766246796, 0.04433632791042328, 0.04599456191062927, 0.044859856367111206, 0.04776220619678497, 0.040572136640548706, 0.04593806862831116, 0.041032299399375916, 0.04683758616447449, 0.04502202868461609, 0.043493247032165526, 0.0438878059387207, 0.037199997901916505, 0.038996455073356626, 0.03402052819728851, 0.041398444771766664, 0.0401736706495285, 0.03653912842273712, 0.0329909473657608, 0.037355121970176694, 0.044352743029594424, 0.038547408580780027, 0.03489358723163605, 0.03853871822357178, 0.038585954904556276, 0.03815447986125946, 0.037584123015403745, 0.03761344850063324, 0.04246545732021332, 0.0351304680109024, 0.03521658182144165, 0.036580872535705564, 0.04248717129230499, 0.0375669926404953, 0.040490147471427915, 0.03844397068023682, 0.039650195837020875, 0.03938212990760803, 0.03524183928966522, 0.039311456680297854, 0.04041037559509277, 0.04422582387924194, 0.040883660316467285, 0.039452704787254336, 0.03885438740253448, 0.03970904052257538, 0.04349164962768555, 0.04219217002391815, 0.04788201451301575, 0.03881639838218689, 0.0403554379940033, 0.04592297077178955, 0.055087614059448245, 0.04631364643573761, 0.04128233194351196, 0.04853757917881012, 0.04573861658573151, 0.04321826696395874, 0.043414455652236936, 0.04694936573505402, 0.05008484721183777, 0.05671505928039551, 0.047282516956329346, 0.04416384994983673, 0.04494655430316925, 0.04311303794384003, 0.04825175702571869, 0.04938630163669586, 0.05125047564506531, 0.0431643158197403, 0.04691492319107056, 0.04894667565822601, 0.04283430278301239, 0.052086430788040164, 0.04527693688869476, 0.04891888201236725, 0.049383634328842164, 0.047783729434013364, 0.04189479351043701, 0.04719771146774292, 0.04368667602539063, 0.043004578351974486, 0.04077138900756836, 0.04787952899932861, 0.04625965654850006, 0.047799679636955264, 0.043125247955322264, 0.04222393333911896, 0.045727378129959105, 0.03781099617481232, 0.04443166553974152, 0.037851917743682864, 0.044936558604240416, 0.04263827502727509, 0.03781793713569641, 0.04145089089870453, 0.04094310104846954, 0.03907554149627686, 0.03907397985458374, 0.039754778146743774, 0.0399343341588974, 0.03686745464801788, 0.040951257944107054, 0.036863160133361814, 0.036452305316925046, 0.03845731317996979, 0.03766351044178009, 0.0371330052614212, 0.03089427649974823, 0.036747655272483824, 0.037344029545783995, 0.03593480587005615, 0.0356877475976944, 0.033293166756629945, 0.03687595129013062, 0.03524163961410522, 0.035185042023658755, 0.031722208857536315, 0.033470061421394345, 0.03369108140468598, 0.03773168921470642, 0.03057895302772522, 0.03513563573360443, 0.03113507926464081, 0.03513524532318115, 0.032720184326171874, 0.0332324892282486, 0.037159311771392825, 0.03269462585449219, 0.03473915159702301, 0.03154263198375702, 0.03639332354068756, 0.032142990827560426, 0.030996868014335634, 0.0341373473405838, 0.03191467523574829, 0.03672018945217133, 0.035051971673965454, 0.03713603913784027, 0.03410486578941345, 0.03354101479053497, 0.034265133738517764, 0.03419898450374603, 0.03546025454998016, 0.040160107612609866, 0.03214111924171448, 0.035951876640319826, 0.03623583912849426, 0.037018173933029176, 0.03467053174972534, 0.036342039704322815, 0.03977064788341522, 0.034076347947120667, 0.04028429090976715, 0.04122271239757538, 0.03915443122386932, 0.038802212476730345, 0.04212861955165863, 0.04057019352912903, 0.04359956979751587, 0.04114851057529449, 0.038141071796417236, 0.04166935682296753, 0.03840822279453278, 0.038268467783927916, 0.04339832067489624, 0.04591786861419678, 0.04089003503322601, 0.04191988408565521, 0.04341530203819275, 0.04116404056549072, 0.04163185358047485, 0.04338114857673645, 0.04571638405323029, 0.039334753155708314, 0.04250597059726715, 0.040683573484420775, 0.04757324755191803, 0.0472650557756424, 0.04054827094078064, 0.04174417555332184, 0.03993331491947174, 0.034911778569221494, 0.04221946597099304, 0.039459621906280516, 0.03838126659393311, 0.04315478205680847, 0.041303449869155885, 0.04145418107509613, 0.034275591373443604, 0.03914000689983368, 0.03376865684986115, 0.043942540884017944, 0.03784594237804413, 0.037730827927589417, 0.03984968662261963, 0.04009068012237549, 0.03279047012329102, 0.03708206117153168, 0.042090049386024474, 0.039830750226974486, 0.03496477007865906, 0.03579109311103821, 0.03593967854976654, 0.033035844564437866, 0.03529327511787415, 0.0362547367811203, 0.03853420913219452, 0.03410346210002899, 0.034755200147628784, 0.030887106060981752, 0.03514676690101624, 0.03162708878517151, 0.03538617491722107, 0.03260507583618164, 0.03422325551509857, 0.03306698799133301, 0.03466555774211884, 0.030596300959587097, 0.029662629961967467, 0.036964142322540285, 0.03289181590080261, 0.03576957583427429, 0.03297663033008576, 0.030026254057884217, 0.03096255362033844, 0.032955920696258544, 0.03171801865100861, 0.031057798862457277, 0.036463302373886106, 0.035060998797416684, 0.03352194130420685, 0.031223392486572264, 0.028542572259902955, 0.03518103063106537, 0.032445883750915526, 0.030774596333503722, 0.03717753291130066, 0.03570965528488159, 0.03323180377483368, 0.035832592844963075, 0.038273027539253233, 0.03533346056938171, 0.03520732223987579, 0.0362723708152771, 0.03399573266506195, 0.03688541054725647, 0.03333692252635956, 0.032157689332962036, 0.035996446013450624, 0.03518417775630951, 0.040346816182136536, 0.037077730894088744, 0.038418376445770265, 0.04162308275699615, 0.03960349559783936, 0.03734395205974579, 0.041557642817497256, 0.04072200357913971, 0.03946160674095154, 0.04249134361743927, 0.0368880033493042, 0.03726150691509247, 0.03911068439483643, 0.03762097954750061, 0.04317424595355988, 0.039668294787406924, 0.042232954502105714, 0.038836488127708436, 0.04207048714160919, 0.043386057019233704, 0.03759638667106628, 0.04246403574943543, 0.03471086919307709, 0.04027695953845978, 0.043538108468055725, 0.0344720333814621, 0.040884315967559814, 0.03827262818813324, 0.042712199687957766, 0.03732635974884033, 0.044515961408615114, 0.040302222967147826, 0.04096667170524597, 0.04023412168025971, 0.04070121943950653, 0.03955451250076294, 0.04089146554470062, 0.04604291021823883, 0.03584805130958557, 0.04097062051296234, 0.0352182000875473, 0.03893764615058899, 0.038138660788536075, 0.03536925911903381, 0.036412394046783446, 0.03513297140598297, 0.03752991259098053, 0.037399673461914064, 0.03849791288375855, 0.03284896910190582, 0.03345014750957489, 0.03313811123371124, 0.03750232458114624, 0.035113352537155154, 0.03455689549446106, 0.03499947786331177, 0.03554643392562866, 0.03093941807746887, 0.03514152467250824, 0.03677147328853607, 0.03145685493946075, 0.03732587099075317, 0.033964723348617554, 0.03741692900657654, 0.03218668103218079, 0.03541558086872101, 0.03506911993026733, 0.03283905386924744, 0.03246981799602509, 0.028836989402770997, 0.03181564807891846, 0.030912721157073976, 0.03400214910507202, 0.03073683977127075, 0.03369641304016113]\n"
     ]
    }
   ],
   "source": [
    "print(test_acc_list)\n",
    "print(test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68658828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEjCAYAAADHWv01AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABpBUlEQVR4nO2dZ3hcxdWA36Muy3KVe+8NbGMbG9N7B9NDL4EACSSh5YOETkIogUAIJEDovddAMNhgwDQ3bNy7ce9dsvr5fszs6mq15Ura1a7seZ9nn71l5t5zy8y5c87MGVFVHA6Hw+EIR1qyBXA4HA5H6uKUhMPhcDgi4pSEw+FwOCLilITD4XA4IuKUhMPhcDgi4pSEw+FwOCLSKJWEiPxPRC5KthyOxCMiy0TkyGTL4YhMfcqje761R0QmiMhlDXW+jIY6kYjs9Kw2AUqACrt+haq+7PdYqnpcPGVzOPY0XHl0+KXBlISqNg0si8gy4DJVHReaTkQyVLW8oeRqSEREAFHVymTL4tizceXR4Zekm5tE5FARWSkiN4rIWuBZEWkpIv8VkQ0issUud/bkCTa3RORiEZkoIg/YtEtFJOKXjYjcJCKLRWSHiMwRkVND9v9KROZ69g+z27uIyDtWpk0i8qjdfoeIvOTJ311EVEQyPLLeLSLfAEVATxG5xHOOJSJyRYgMY0Rkuohst7IeKyJnisjUkHTXi8h7Ya7xbBGZErLtWhH5wC4fb69th4isEpEboj0jzzGy7X1eLiLrRORxEcm1+wLP8U8istGaEc7z5G0uIi/Y+/eziNwiImme/WHvu2WoiPwkIttE5HURyfHkO9Heq60i8q2IDPbsu9Fe3w4RmS8iR/i5zj2Zhi6PIefOFpGHRWS1/T0sItl2X4E971YR2SwiXwfeHz/PWUT2E5G1IpLu2XaqiPxkl0eKyBRb5taJyN99ypwmVXXKJhF5Q0Ra2X2BuuByez1rROR6P9dr99eoBzyn7iYi39hr/lRECkKu9Vt7r2aIyKGefReLqXN22GdzHrFQ1Qb/AcuAI+3yoUA5cB+QDeQCrYHTMc3gfOBN4D1P/gmYLx+Ai4Ey4FdAOvBrYDXmiz3cuc8EOmIU5C+AQqCDZ98qYF9AgN5AN3vcGcBDQB6QAxxo89wBvOQ5fndAgQyPrMuBQZiWWyZwAtDLnuMQjPIYZtOPBLYBR1kZOwH97b3ZDAzwnOtH4PQw19gE2AH08WybDJxtl9cAB9nlloFz+3huDwMfAK3sc/kQuCfkOf7dynqIvbf97P4XgPdtvu7AAuDSaPfd865Mss+sFTAXuNLuGwasB0bZZ3SRTZ8N9ANWAB09z6VXMt73VP+R3PLoPfddwPdAW6AN8C3wZ7vvHuBxTPnJBA6y74rv5wwsBo7yrL8J3GSXvwMusMtNgf183rtrrMyd7f16Ang1pC54FVNv7A1s8Hm9YesBz/1eDPS1z2cCcK/d1wnYBBxv8x1l19tYGbZTVSY7AINiXmOKvJSlQE6U9EOBLVFeykWefU3sg2nvU5bpwBi7PBb4fZg0o+3DzQiz7w5iK4m7YsjwXuC89iV7KEK6fwN32+VBwBYgO0Lal4Db7HIfjNJoYteXA1cAzWrxzART6ffybBsNLPU8x3Igz7P/DeBWTGVRAgz07LsCmBDtvnvelfM96/cDj3vux59D0s/HKKjeGAVyJJCZjPe8sfySWR5Dzr0YON6z7xhgmV2+C/OR0Tskv+/nDPwFeMYu59v3uZtd/wq4Eyio5b2bCxzhWe+AUZIZVNUF/UPe36d9XG+0emACcItn/TfAJ3b5RuDFkPRjMR9QecBWjMLP9XuNSTc3WTaoanFgRUSaiMgT1iyxHfMAW3ibiiGsDSyoapFdbBouoYhc6DFPbAX2AgJNtS6YBxdKF+BnrbttdkWIDMeJyPe22bwVo/VjyQDwPHCuiAhwAfCGqpZESPsKcI5dPhfz5Re4N6fbc/4sIl+KyGgf19AGU+Cneu7dJ3Z7gC2qWuhZ/xnTAigAsuy6d18nuxztmsHzfDGtrsCz7QZcH5DHytQF81W5CPOVdwewXkReE5GOPq7T0YDlMYSO1HxHAs/sb8Ai4FNrLrnJHr82z/kV4DRr0jkNmKaqgfNdivkynycik0XkRB/ygnkH3/W8f3MxHQDaedJ4y7/3mqJdb33KxJkhZeJAjLWkEGM9uRJYIyIfiUj/WBeYKkoiNBTt9Zhm5ChVbQYcbLdLfU4iIt2A/wBXA61VtQUwy3PcFRgzUCgrgK5i/QwhFGIqzwDtw6QJXp99Qd8GHgDaWRk+9iEDqvo95ivvIEzF/2K4dJZPgQIRGYpRFq94jjNZVcdgmrnvYb74Y7ER2IVpnrawv+bqcYACLUUkz7PeFWNq2Ij5uuoWsm+VXY54zTFYgWlZtfD8mqjqqwCq+oqqHmjPqxgTiiM2DVIew7Camu/IagBV3aGq16tqT+Ak4LqA78Hvc1bVOZiK+DhM+fGWiYWqeg6mTNwHvBXyLkdiBXBcyDuYo6qrPGm6hLumaNdL/crEiyHy5KnqvQCqOlZVj8K0eOZh6sOopIqSCCUfUyFttU6g2+N03DzMS7QBQEQuwbQkAjwF3CAiw8XQ2yqWSRg7/r0ikiciOSJygM0zHThYRLqKSHPgjzFkyMLYLjcA5WKcekd79j8NXCIiR1inWKcQbf8C8ChQrqoTI53EtnrewnyBtQI+s9ecJSLniUhzVS3D2CgrIh3Hc7xKzAv1kIi0tcfqJCLHhCS9057jIOBE4E1VrcAoortFJN/e0+swJjGIfN9j8R/gShEZZfPlicgJ9hz9RORwq5SLMe9TzOt0hCVR5TGUV4FbRKSNdcTehn1HxHRQ6G1b0YF3tqIOz/kV4HcYRfdmYKOInC8ibex7vtVu9vO+PI55r7vZ47QRkTEhaW61rbFBwCXA67Gul9j1QCReAk4SkWNEJN3WVYeKSGcRaSciJ1vlVwLs9HONqaokHsY4ZDZiHDufxOOg9kviQYyTah3GkfSNZ/+bwN2YF2kH5iu7la3kTsLYP5cDKzHNNlT1M8xD/wmYCvw3hgw7MC/pGxifwrkYZ3Bg/yTMi/QQxnH1JdW/Nl7EKLZorYgAr2BstW+GmMouAJZZ08GVwPkAVtHtFJGuEY53I6bJ/73NOw7zhRlgrb2m1cDLGAfzPLvvt5hW1xJgopXtGXvNYe97rItT1SkYB+mj9ryLMDZxMIr4Xsw7tBbzhfinWMd0hOVhElAew/AXYAqmLM0EptltYPxq4zAV23fAv1R1ArV/zq9i/C6fq+pGz/Zjgdlixo/8A9PJoxjMmBL70ROOf2DK76cisgNzf0aFpPkS826OBx5Q1U9jXa+PeiAsqroCGIO5BxswLYs/YOr6NEyrcDWmE8whGH9GVMQ6NhyNBDFdTtdjeiQtTLY8AcR0s3tJVTvHSOpw7BGISHdgKcah3mjHmqRqS8IRmV8Dk1NJQTgcjt2XBhtx7ag/YkbGCnBKciVxOBx7Cgk1N4nItcBlGGfxTIyNrQnGht8d00f6LFXdkjAhHA6Hw1FnEmZuEpFOGAftCFXdCzOg6mzgJmC8qvbBOHJuSpQMDofD4agfiTY3ZQC5IlKGaUGsxnQRPdTufx4zevDGaAcpKCjQ7t27J0xIR+Nj6tSpG1W1TeyUjnC4MuUIJVKZSpiSUNVVIvIApsvoLuBTVf1URNqp6hqbZk2gz30oInI5cDlA165dmTJlSrhkjj0UEfk5dipHJLp37+7KlKMakcpUIs1NLTH9dXtghprnicj5fvOr6pOqOkJVR7Rp4z4YHQ6HIxkksgvskZjgbxvsyN53gP2BdSLSAcD+r0+gDI5GxnPfLGXF5qLYCR1xZXtxGTe9/RPfLt4YO7FjjyKRSmI5sJ8dji7AEZjgVx9gIhJi/99PoAyORsTmwlLu+HAOlz4/2RvB0tEAVFYqr01ewfy1O5ItiiPFSJiSUNUfMLGDpmG6v6YBT2KG0B8lIgsxsc7vTZQMjviwcWcJa7btSvh5NheWArC1qIwef/yY296fnfBzOgw5mSaga3GZmzTRUZ2EjrhW1dtVtb+q7qWqF6hqiapuUtUjVLWP/d+cSBlShae+XsKrk5bXOf8X89dz1SvT4iiRQVV56LMFzF69Lbjt+W+XMeIv49i0s4T7PpnHiL+MY/Q9n8flfOu3F1NaHr4i2rjTRD1fv8P8v/i98003FNkZpiooLnMxEB3ViakkxEzpd5V1RDvqyF8+mssf35mJqnL3R3OY+vNmFq3fwbg56/hk1lq63/QRn85eGzH/Jc9O5qOf1rCzJL4hYDYXlvKP8Qs54ZGJXP3KNF75YTm3fzCbjTtLGP6Xcfx7QlVI+28XbYxYifywZBPdb/qI9TuKw+4HY9IY+dfxXPv6dABKyyurmZTemLwiQk5HohERsjPS2FxYSmWlM/M5qvDTBfZszEjpyWLmTX4W053VvUl14Ownv+eHpZv5z9dLa+x74bufWbRhJyu37OK2Ewfy/vRV9GrTlBHdqwKirt9eTNM2fuZvicx/vlrC3R/P5e1fj+bbRZuC2//70xr++9OaiPnOfeoHAN6/6gDaNsumQ/Pc4L7nv1sGwHeLNzFmaKdw2dlZahTcRzPX8GBZBSPvHsfpwztz+0mDAJizZnvNPCXlNM120WMaAlXTeiurqOTPp+xFZroL7ebwoSTszE83i8itmPkBngEqReQZ4B+7o7moslJ558dVjBnasU4FZcmGnWzcWcrIHjWjXf+wNPLtmrhoIxMXmd4lk5ZuZtH6nQAcOaBqKMn6HSX0DFESn85ey6Slm7nlxIE1jvnVgg1c+Mwk0gSevnhf5q3ZwX2fmOjdp//7u1pfG8CYx75hn64tePc3B/D9kk089sUiyivMN8O2XWV8PHMNRw5oR1ZG1b2btWobTbKqJjJ778dVbC8u59lvlnHbiQNRhWWbCmuc6+Of1nDWvl1qbHfEn9IKYwZ8bfIKissqePjsfZIskSMV8PWJJiKDMa2J4zGzqr2MmRLvc8x8t7sVH/60mhvenMHabbu4+vA+tc5/+INfArDs3hMA6tR8DygIgHFzq3oJn/3k9zx+/nCe+noJN58wgAEdmnH5i1MByM/JpEebPA7p24Yflmzi6EHtufujuUYGhWtfn87WorKY5z5pSEc+nLE6apofl2/li/nrueTZydW2/2/mWr5bsokmWemM7NGKZy/elwnzN3DJc9XT3fTOzODy0o2FfLVgA8VllfzuiD5sKSylb7umPDVxKfPXud42yeC96au5/4wh1RS9Y88kppIQkamYmZqeBm7yzKn8g2d2tt2K7btMRbpmW2T7uh9WbC6iQ/Mc/vrxvNiJa8GVLxmlcOq/vuV3R1QpsYfGLaiW7l/nDWPt9qprCKcgerdtSrOcDKYt38qZwzvTp11TDu7bhg9nrOb2kwaydGMhL3wX3oEcqiAAvltizFdFpRVMmL+BvW4fS2FpdGdoQKkCnDOyS9CMdfrwzjTJcqamZPHhjNWcPtxND7Kn4+cz4UzbC+kVj4IAQFVPS5BcCae8opJ7/jeX6Su2oqrMW7udeWuNTTzDmpgCJpRwlFVU8q8JiygqjexIPuj+L3hk/EKe+cb4HwZ2aFZt/2UH9qi27q3w/fLDkk0R9/3m5Wls2xW55fD4+cMYd90hNM/NBOCIAW25/OBe9G/fjIk3HsbF+3fnxmP7I56ZjP/v2H4RjhaeaArinJHVJ8C78dj+1fwcTkEkl7IK1x3W4c/cdJmI3K+qWyEYbuN6Vb0loZLFgcKSciYt28xh/aps+kWl5WwtKmNrURlPfLmEJ75cUi3Prw/tRZqtFMujmIk+nb2O+z+Zz8Ydpdx20kAqKpXlm4tYvbX6eIJHPl8UXH747KGompbAXWMGcWDvArq0akKftk0Z1Kk5TbLSWb6pkMnLtrDKc5w/HNOPkwZ35K7/zmHc3HUA5GWlU1haEdXHEeCVX42iuKyCXz5XFatn5h1Hk59jlEOgj7z3eju3bGLOk53BEf3bMW7uOp68YDhHD2rP0C4teP/H1bw+pW69kT655iA6t2xC0+yMat2CD+nrwq+kEtHef8eeg5+WxHEBBQFg5344PmESxZH/e/snLnl2Mj9bh+jVr0xj4G1j2f/ez9mwsyRsnn9PWMxjX5hun29PW0lZRSXbi8v4ZNZaLnj6Bz6ZZXr/BMYsfGMdzfd/Mo/DHpjAebYHEFDtC/ye0/amb7t8+rXP54sbDuWgPm0QES7avzv79y6geW4mmelpPHz2Pjz0i6HVZBrQIZ+urZvw7/OHBbe9eeX+vnv97N2pOYf3b8dvD+8NQI+CvKCCANi7c3MACppmh83fo8AojNZNswDYv1cB950xGIBmOVUy3HLCAK44pCcjurWkQ/Oc4PYWTarOdcXBPenfvllQ9lcuq5oOuEdBnq/rcSSGUP9DhVMSDvy1JNJFJDtgarJzLIevTVKMxdb5e9v7s1m/o4S5ni6Wv3rBXwTMPjf/j9E9Wwdt7V8v3MjSe6p05Px1O/h83jqe+GpJjby/P6IPFZVKk6yMGqaVaIzs0YoFfzmOSlUWb9jJoI6mEs9MT+PaI/vy0czV9GyTx9WH9+be/1X3d7TKywqOXA4QUAjXH92Ps0d2Jc/TywjgyoN7MaJbq7C9sQBuOKYfo3u1Zni36vsn3XwEWelpVCpUqtZQMpWVygvfLeP04Z257f3ZfDRzDb86uGe1NPv3LmDJX4+nuLyC3BC5HA3L/r1aM2H+huC6a0k4wMfMdCLyf8DJmPERCvwS+EBV70+8eIYRI0ZoXcIad7/powRIU5OCplmkiQRHCgd47NxhnDC4Q8LOW1GpTFu+hV5tmlJYUk6rvCzysjP4YMZqVm4p4v5P5gNVvaySSWWlUlpRGTRt1RcRmaqqI+JysDgjIscC/8BMtPWUqt4bsl/s/uOBIuBiVZ0WLa+IvA4EHEItgK2qOlREumNios23+75X1StjyRiuTO0oLmPvOz4Nrt98/IBqSn3brjJEoJmnFerYfYhUpvyMk7hfRGZiAvQJ8GdVHZsAGePKrFXbwm5vnZfFJvul/eCZQ7j+zRn1PtfGnaVcOLobd43ZCzAV4lcLNyTcxp6eJuxrB9q1yssKbj95SEcAjh7YPmXCLKSlCTlpu39LQUTSgccwcclWYgahfqCqczzJjgP62N8o4N/AqGh5VfUXnnM8CHhf8MWqOrS+sueHVP7llcqAWz/hpCEduP+MIQy50yiQeH50bC8uo0lmerCziCP18PVkVPV/qnqDql7fGBQEVMUBCmXstQezd6fm/Ou8YdW6931+/SH88oAeYfMEePbifXnyguHkW3v6L0ZUDfLy9spJSxMO7dcW8TolkkDvtk3Zq1PzpMqwBzISWKSqS1S1FHgNM6+KlzHAC2r4Hmhhw+bHzGtbIWcBryb6QioqK9lVVsEbU1ZW266qnPzoRJ76egm7bO+1jTtL2FEcewyOl8pKZfAdn3Lzu7PiJrMj/vgZJ7Ef8E9gAJCFaQYXqmqzqBmTxBfz1vPe9FW8P736YLB+7fI5ZZ9OFDTN5sPfHhjcfv8ZgxGgZ5um3HbSQG47aSDHPvwV80JCJp88pCOH9Te9pKbd1pYdxeXk52QEe/h0apmLwwF0ArzdvlZiWgux0nTymfcgYJ2qLvRs6yEiPwLbgVtU9etwgoXO9hiLSD6JSoWfVm7jp5Xb+HbxJjYXljJ9xVYKmmYz5ZYja6S/9b1ZHNC7NcfuVd30usPGIXt72kr2792aJlkZ/OqFKbx55ehgC9mRfPw4rh/FxG96ExgBXAj0TqRQ9SF0ZG+At38TvjfQWSNqhnx488rRbNpZyhfz19O+WQ7H7tW+WqsgMz0taN5p0SSTrUVlDGifH6crcDRywjUfQ2vbSGn85D2H6q2INUBXVd0kIsOB90RkkKrWCISlqk9iwvUzYsSImF7ppRtrhkkBKK+sGj8xceHGYDiPSK33F7//mRe//5lptx7FAfd+zkuXjeSzOeupsMfJzkjj969ND6Z/bdKKakri+yWbOPvJ7/nqD4fRtXWTWGLv9lRWKrvKKshroJhmfs1Ni4B0Va1Q1WeBwxIrVt2YtnxL2O2dWuTWKkhcfk4m3QvyuOSAHhy3d4eoZqO7xuxFpxa5rvvmboaI3C8izUQkU0TGi8hGn9PvrgS8Xx6dgdAYJ5HSRM0rIhnAacDrgW2B8Pt2eSqwGOjrQ86YhLbGA2zzjNxv26xmR0dV5U/vzmTmyup+wR+WbGJXWQU3vzuLx79cHAxymR3SmeHtaSu55+O5wfWXfzBjaX5cEb58xwtVpftNH/HQZwtiJ04iD49bwKDbx7K9lua9uuJHSRSJSBYw3Raca4GUrBHnhokiCvCXU/dK2DlPHtKRb2463Dnedj+Otl/jJ2Iq777AH3zkmwz0EZEettycjZmN0csHwIVi2A/YpqprfOQ9EpinqkEngYi0sQ5vRKQnxhlesz92PfFGFijxzAeyckvNyag2F5byyg/LufCZH6ptL40wgjszveZHmLdLecDXEfqht2rrLv4xbmHcZjAMhOH/14RFMVIml/dtXLVNO0tjpIwPfj6vL8Aok6uBazFfOqcnUqi6sGlnSUQHmHfEtcPhk0BXn+OBV1V1s5+OCKpaLiJXA2Mx/rtnVHW2iFxp9z8OfGyPuwjTBfaSaHk9hz+bmg7rg4G7RKQcqACuTERk5t++8mNwOVJlHyDgythSVFZtMqu5a4yfL9TfJ2GtbEYJ/OerJcGYY7khLY7LX5jC7NXbWbt9F5cf3KverfktheY8qd7FNzMYNqhhwqZEVRL2C+VuVT0fKAbubBCp6sDjX1ZNjjO8W0um/ryFP5+yV40Xy+HwyYciMg/YBfxGRNpgykBMVPVjjCLwbnvcs6zAVX7zevZdHGbb25jIzAkjTWDKz1Wmnlgxnbw+ixMemRhcfvKrxeGSVwtC6eUPb87g28VVsclCHemBQaOvTlrB+Lnr+eKGQ+tlp99UaHwqzXJTW0lk2LhBsZR13M4XbaeqVtjmbJbtkpeyeGdse+bifVm0fkeNEcIOh19U9SYRuQ/YbstBITW7su4R5GSmVwsxU1Ye3byzbnt4B3ZtB3CHhgXxKp8dxWXVojSv31HCoNvH8v5VBzCkS4uox129dRczVmzl28Wb+PMpVaboLUWmivOGmkkVrnt9OmWVyj/P2ScYPqUkwjTA8caPIX0Z8I2I3Coi1wV+CZar1uzyRBttnpvpFISjXojImUC5VRC3AC8BHZMsVlJITxPSPFqirDJy5fThjNWc8tg3cTlvaPDKQFTmikqtNjK8ep7IUZErK5Uflmxi/3s/59cvTwvOof7wuAU8PXEpO0tMHRKICvCvCYv4bM46Zq3aFlRY5RWVfLXAhC658JlJPP/tMgDmr93BHR/MTtjUr+/8uCo4x0vA3FTsqfNUlSnLNsfNP+PFj8pcbX9pQMr28ywuc2GNHXHlVlV9U0QOBI4BHsCOjE6uWA2PUL1vbqBiDMdvX/0x4r76ctd/53D3x3O5+5S9I6bZvit86P5VW3dxwL2f19iuqjw8zgw5+ftZQ4CqwJyBsDYBFt19HPePnc+TXy3h3d/sz1cLNvDVgg1ctH93fvncZFZt3cVlB/UIRlCuK8VlFRz6twncNWYQRw9qX2N/wNG/yxNN4Z1pq4LRI44d1J7HLxheLxm8+AnLkbJ+CC9bd5mm4odXHxgjpcPhi0AJPAH4t6q+LyJ3JFGepCEi1bqBR+oaWxuaZKVTFGMyqlACPami+US8ZucAXy/cwGXPh4/9ttHTQ+i6N6pC9Lzyw/IaaXeVVfCd9ZFUhnyxp1s/QaQ5aLYVlTF/3Y6IQTS9LN1YyNrtxdzzv3lsKSpl1qrqvTYDLYnV24qpqFTS06Ra9+BPZq+NeY7a4GfE9RfUHNCDqh4eV0nqwYYdJXy/ZDOjerQKhr12OOrJKhF5AtPt9D4RycbnuKLdDaMf4mvGyM5Iq7WSCBBpwCxQbezA+u3FZKSnccHTkyKmLwyjVL5fspnvl9TsILZtVxlrthlFVRrilwl83ZdHMMVd9sJkJi/bwrw/HxszyOXyzUWA8Y3c+PbMGvuzrJK49b1Z3PreLH740xG89H11paaqcQsL5OelvwHTP/wPwK3AdMBXSFYRaSEib4nIPBGZKyKjRaSViHwmIgvtf8s6S49pmu179zigpg3T4agHZ2G6oh5r51Nphb9xErsdlZVa7Ys7HmRnJKbXoXeK3pF/Hc+wP38WNX1xuX9FdeB9XwTvg7c3JXj8BB6z9xEPTuCpr814j3m2+284Z/P24jIuemZSUAGttQ75SMokLa165T/qr+NrpAl1+q/fXsxFz0yqNhDSLzGVhKpO9fy+UdXr8G+X/Qfwiar2B4ZgQhrfBIxX1T7AeLteZ8bPXR9c7tuuaX0O5XAEUdUizOjlY+zYhbaqGt5bupuzvTjyFL11JSczMY2yVWEG90Wjrr7MLxdUzbsx5M5Pg2M/SjxKZ/GGQv7ykRk5nmFbGsVlFWwtqq5w3/9xFV8u2MCjdhbL7XbK4UiTPvlxTodm/deExXy5YANvTVsZPkMUYj4p++Uf+BWIyDFATW9KzXzNMAN9ngZQ1VL7RTYGeN4mex44pdZSe1i5pSi4/Prlo+tzKIcjiIj8HngZaGt/L4nIb5Mr1e5DvOYVCWWFpz7ww646mry8eOeR31UaXumkp5mq9m9j5zP0rs9Yv6Oq+26gFRJoQQRMZpHmp/fTgalSlStenMJlzxvTXMDyVJfeT356N02lKvhYObAUuNRHvp7ABuBZERlij/N7oJ0NQYCqrhGRsMOh/Uas9PaVbumZU8HhqCeXAqNUtRDAjpn4DhMR2VFPsjMS05IoKq3gywX+53KpjbnJD7vKKpi7ZnuNXl5ZtiXxjv2S37SzlIPv/4LLD+5Fdxu0cPy89ZRXVAZ7aIXOMAmmkq/wUdFXVCpjZ68D4MXvloU9ll/8mJt6qGpP+99HVY9W1Ymx8mEU0DBMz5B9gEJqYVpS1SdVdYSqjmjTJvIDX711F/k5GYy95mC/h3Y4/CBU9XDCLid3gpDdiET5JAAueiayozqUkjhPylVcVsHC9TtZZKdODhCI7RYwA1387CSKyyp5ZPzCaj2ilm8uCk4/sCmskvA3KNGrSG59f3awR1pdhlH4MTddJSItPOstReQ3Po69ElipqoEoX29hlMY6O8EK9n99hPy+WLqxkP16tqafC9XtiC/PAj+IyB226+v3WNOpo/7EI6RE+2Y59T7GrjgriV1lFcFBb14yQoIYekel/9/bPwWXv1kceTAgwEs//OzLZDQ4wmDD0K67fvDT5vuV9SUAoKpbgF/FyqSqa4EVIhKYl/cIYA4mquVFdttFwPu1EdhLRaWybFMhPdukZFBaRyNGVf+OCby3GdgCXKKqDydVqN2ISHNP1IYzPDNL1pV4D8LdVlTGZ3PWVdtWUanBbquxWG/jWLWKYDq/7f3ZfL1wY53lu+d/8/hmUe3y+5E8TTwdbm3QP7/G/98CL4vIT8BQ4K/AvcBRIrIQM5fvvZGzR2fTzhLKKrTeIxwdjgDejhqYkDQvAS8CP9ttfo5xrIjMF5FFIlLDxGpDhD9i9/8kIsNi5bUtmlUiMt3+jvfs+6NNP992LEl5tkdwytaGzDiE54/3HPAlYXwcv311mu8xCwHfgV+fSl1oXssAhn4c12OBN0TkcYwD+0rgEz8HV9XpmNnsQjnCr4DRWL/DfI20za858YnDUUe8HTWgahSZ2OWe0TLbj6jHMB9AK4HJIvKBqs7xJDsOM+9DH0x38n8Do3zkfUhVHwg530BMCPFBmNhS40Skr6rWq/Yb1rUF05Zvrbbt7lP3itt81F1aNWH26vDzv/glMyNyxRvqE4hEvM1NpWHGQXw80/8I6K27AuHKExdksKBp7epLP6r4Rsx4hl9jwhuPB/6v1pIlgHW2aeaUhCNehHTUCCwH1qMqCMtIYJGqLrGRk1+jZvTYMcALavgeaGH9c37yhjIGeM3OULcUM0fFyFpcclj+ftbQautNszPo2CJ+87j/8bgBtUpf0LSm8SIzLXL1tWKzv66w8TY31Tcya2AMRSLDlUcyZUXCj5LIBf6jqmeo6unAU0BK1MqBOPTt4uDAcjjiRCdghWd9pd3mJ02svFdb89QznkgFfs4HmG7lIjJFRKZs2LAhXJIgoY7WNKFaJNj6kptVu95NXVrVNCmHm9EuQOjcE5FIRO+m+hAYMV5bk1BtyKpl92M/qcdjFEWAXGBcrc6SIJZvKiIrIy0uvRwcjjgRruYKrbEipYmW999AL4xvbw3wYC3OZzb67FYONe39Jlx41Cy1wu+xDu/flgfOHMI+XWpG78mMUtmF8w2EI97mpvq2TAJTkqbS7Hh+lESOqgYNfHY5JTzFP28qokvL3BqxTByOJLISM8VvgM6YUPt+0kTMq6rrVLVCVSuB/1BlUvJzvlqTHlKmQueUiPfxw3HlIb145uJ9OWN4Z8KdOprjusRnZR0uyOC+3eseTq6+g/PWbi+mfbOcWpuEEokfJVEY0vtiOGZKx6SzpaiU1rV0wjgcfhGRdBHpKCJdAz8f2SYDfUSkh4hkYZzKH4Sk+QC40PZy2g/YZqMQRMwbGFtkORWY5TnW2SKSLSI9MM5w/6PJIhBq708TCVtR1xU/Cscb0TUc0cxNoZV1pPmv14WZOnXf7nWfsCxUOZ0z0s8rU52BHZv5UqJ1YWiMWfvC4ceFfg3wpogEvk46AL+o9ZkSwK6yipTSuI7dBxun6XZgHRAo+QoMjpZPVcttQMCxQDrwjKrOFpEr7f7HMXNYH49xMhdhxmNEzGsPfb+IDLUyLAOusHlmi8gbmDFI5cBV9enZNPnmIymrqKzhk8iIc0vCz6E2eyLPhkserSURavaJNABt9db4fu+GKqd2zWr/EZudkZYw68hrl+9X6zx+Jh2aLCL9gX6YZzVPVevfyTkOFJaU08WNkXAkht8D/VQ1+hDYMKjqxxhF4N32uGdZMT0FfeW12y+Icr67gbtrK2c42tiegqE2/bQQJSFStxAPweNF0RJnDO/MW1NXxnQ+RzU3hcgf6Uirt9ZsSdSG3xzai39NqAobHuq4blJLBz0Yx3J6HBVyZrpQVqF0apFbp8CKft3c/YCBwD7AOSJyYa3PlAB2lVbUupeEw+GTFcC2ZAuRLDLSojuu2/gw80brjx9QEuE+mG84uh+/O7w3fzllr6jHjzaKuWZLAl4P8xUd6rj+66mRp0YNR0aIDKGTFeVm+R/vEDCfZaWnEaV3b60JPMva9moK5o+VQERuBw7FKImPMQOBJgIv1OmMcaSorKJOmtrh8MESYIKIfAQEY0jYcB27PTUc1yK1NoFE+xgO1K1pImGnAr3u6H7VttXWcf3I+IXV1g/oXcConq2jynv6sM6cO6orfxs7L2q6ajLEuCdNavHlnpORTllFedxbEhnpAmXRfThR8/tIcwZmwqAfVfUSEWmHGSuRdIpcS8KROJbbXxb+w9DstoSam+o7mWkgTEW4utCvLvJb6d1ywgAuHN09Zrq6jHFIjyFDbeqnnKx0dpQYJRFPn0RAmdY1jIkfJbFLVStFpNxOJLSeGKEJGoKKSqW0vJImmYkbvu7Yc1HVO5MtQyqRLvEeJ2GVRDDaSc19sYg2TsJLrzZNo5pa8nMy2FFcHjQ9BRo2Nxzdlwc+XRD12LFkrY2SCMyxkZWRFtdOAhn2wSVSSUyxocL/g4lrs5M4dLGrL0WlZmIOZ25yxBMReVhVrxGRDwnzwayqJydBrKQjdRhxHS11UOGEbUn4VBI+Dfexvsqb5WQaJREyZiI0KF96mtSYUjSWWag25qZAZZ6dnhbXLrCB4/qNRFsjf6wEqhqYO+JxEfkEaKaqP0XL0xAEHqgzNznizIv2/4GoqfYw5q3dkZBxEuEOKT7rsmgB/rzEqsjzbTC9UCd2aLfZcEoilgLy1k/7dG3BjyFBE70ElFJ2ZnpEmcPJ4KV/+/zgfNsBAs51v/crlFrZalR1WZ3OkgAKrZJwLQlHPFHVqfb/y2TLkmpU80nU0ykRrXIN15IIF2o7tAdW5HNF359tv/YDPolIOqVJVnqNKK9el0S4CjzX05Lo2zY/hpIw/9F6N6WLUBHFI3TD0f247IUp1bYFxrzU1dyUmIlmGwBnbnIkEhHpIyJvicgcEVkS+CVbrmRSa3NTlOQBHVEfx7Vfk0wsZRIIy92hefQYcJnpaSy794SIMoRzpHt9IdEU49WH9Q7e32g+iVgKL5z6CJjl9jglsSvYknCOa0dCeBYTVK8cOAzT5fvFqDl2cxLnuA6/L17yeOvGD64+gG6tqw/AbdcshycvGM7Dv9gn6nHCmsY8soaz+XuVSLQ6unPL3OD1ZGVE9knEUnjhRpZneMZf1AVfueoYwyahFDlzkyOx5KrqeEBU9WdVvQM4PMkyJRW/s6uFcufJgxjSuXnIscx/uLow3GlCN522T9ho6GHxKp3BnVtwakjeSlWOHtSe5k1M5NVIprRwystbmYfrQeX9eo/mG/FaqbKjtCRitZ7CiV7Vu6luzy+mkrAxbNYBnwEf2d9/63S2OFLkHNeOxFIsImnAQhG5WkROBdomW6iGZGRIoLto9dPfzxpSY1ugldC9II9D+1W/dcGWRJjK0FdLohZhQUK/vkOP7/c44cRKj9GSyPDctGjmpvLKSsorjCD5OZkRlUFMJRG2JZFW7b+2+MkViGEzSFX3tr+oQc4agl1lAZ+EMzc5EsI1mJD4vwOGA+cDF/nJmKA5rv8mIvNs+ndtt3REpLuI7PLMff146PnqyouXVZ/grnrlWr0yOm1Y54jHCTdhUbTeTX7NTd4Ks1OUWfNCLTSBbIE8oc7mSKeP1ZIIN27DWzFHu66yCg3OatcsJyNiqyPWvQmn8AKKKqOO9kI/SiIlY9g4c5MjUdi5ps9S1Z2qulJVL1HV0+1Uo37yPoYJXzMQE+tsYEgy7xzXl2N8H7HyfgbsZT/QFgB/9BxvsaoOtb8r63jZNcjOqF62auu4DtjCww3Ei2Z6CVuX2W352VUfhd7Z6h48awi/ObRX2OOFfn2Htl4q6tFVy6uAorUkBnduXkMhNslKZ69OzQAor6ikrMIqidzMiIoqWmPghL07hHdc20x1HXvhR0kEYtj8UUSuC/zqdLY4Uma1bl099g5HOEQkw4baHi51M8InZI5rVf1UVctt/u8xkws1KLHuxrMX78tbV44Orge6f0q0uE9hfRKRT9TO9kAKmLL26doCiK7AQhVSaGUZKYx4Tbmi7w9XF2WkC3PvOpa3f71/WGV0UB8zQ2BZRSWlHiUR0dwUQYhHz92Hf56zT404WFB1vXVVEn5sNSkZwyYQRjg07r3DUU8mAcOAH4H3ReRNoDCwU1XfiZE/3JzTo3ykiTTHdWhegF8Cr3vWe4jIj8B24BZV/TqGjHUiUkV/1WHmC/6w/tX9DoGw1OGyqf3mrW3pjVShRw8mGGrqqp6nMmQSu0j+klgtqbCO67Q00uwgtsoQs9YBvQvYq6Nx6Pdumx8cg9E0OyNiHKlIz8CMrRC6t645uVLAYZ0wJZGqMWyCSsJNXepIDK2ATZgeTYH5pxWIpSTCvZDxmOPaZBS5GdMt92W7aQ3QVVU32Vkj3xORQaq6vYZgIpdjzFt07Vr7Doreouatq/9wTP+w6XMyqyrN0Po1kL+2jbXAaUOzRTtKaOUYMKMFzEOhX/iXH9yTjTtLuHj/7vxt7Pyqc8QQNZy5yVuph57nn+fsQ05mOn3aHUzfdvk0yxnBc98uIz87o8agveDxYvgq9urUnL+eujd/endmcF96PX0SEZVEvGLYWDvrFGCVqp4oIq0wX0HdMTNsnaWqW2oreEVQSThzkyOutLXm1FnUrLj92CXqM8d1VrS8InIRcCJwhJ24CFUtwYYyV9WpIrIY6Ispc9VQ1SeBJwFGjBjhy8byymWjOPepH4Da+yQCLYni8spg3sP7t+XOkwfRPDeT4/ZqzyUH9CAzXVi6sZDr3phRq+ND1cMRifxwQuU+e2QXVm3dxcAOzbjm9ek1Wif5OZncc1rNvjl1aUl48Vb8HZvnBO9P33b5AOzfu4D9excAtf/q91aD3VuHn4itrpFlo7Uk4hXD5vfAXKCZXb8JGK+q99reGzcBN9b2oAEnj2tJOOJMOtAUfy2CcATnqQZWYeapPjckzQfA1SLyGsactE1V14jIhkh5ReRYTDk5RFWLAgcSkTbAZlWtEJGeGGd43EaGByqtLq1yax27KfDFXlJWEWyF9CzICzqc/33+8GDafbq25MsFG3h/eqg+NQR9EF1asmRDIcfv3b7a/mhuhXAtiT8dP4Dxc9cBNXs3eZl885Es3VjIWU98F/aF2LtTi+ByrHEIO0vKg8uxWlARXTgRt0vYZQBbVca/JRGPGDYi0hk4ATO1YsDZPQYziRHA88AE6qAkKirVRKZ0SsIRX9ao6l11zZzAOa4fBbKBz2wl8L3tyXQwcJeIlAMVwJWqWn16tHoy4/ajyUpPY9uu2s1aHDA3eVsS0bTs388ayn2nR+9d36ttXrXQGH7MVZFaAB1tF9jBnVtEzNsmP5stRaX2XDX3927blNzMdHaVVcTsRLOjuEpJ5GVH75VZn3otNGvAmV3XiYz8zEzXB7gH0yUvGNxEVf3MKfEw8H9AvmdbO1VdY4+xRkTqNECpvFJ9hwp2OGpBvb86EjTHde8I6d8G3q6PvLFonmtGImdnpHHsoPZ8MnutryaVMaOsoXluJht2mMn9wvW+CZCeJqSnRa88/XRE6tQil1VbdwXXI9WNAzo045NrDqJP2/zwCUKIpJC84TSiEVASxwxqxy0nhPaKrk6tK3TPfQlVMIGWUnod60s/ueoUw0ZETgTWB1oktUVELheRKSIyZcOGDTX2l1dUxjXmusNhOSLZAqQqaWnC3adGn3fay1WH9ebZi/fl4D4FwYq0rkMSurQyX/2RgvB5D3vaMP8hO/q3b+ZjFLP5j5QscO5YsZF2FJuW2PVH96s2xiMctfX/eJVvqJxVSqJWh6w6no80dY1hcwBwsogsw/T3PlxEXgLW2T7h2P/14TKr6pOqOkJVR7Rp06bG/vJKdd1fHXEn3qaa3Y2As/WgPgUx06anCYf1b4tIVRi/aC2JaJyzb1eevWTfGnGXwtUAOSET/dR3HoyAzOGCEUKVEonVkvjFvqZPQscoo8MDRPQ9RJTRm7d6qnLbxzeRLYk6xbBR1T+qamdV7Y5xwH2uqudjnHaB8AYXAe/XRfDyCnVOa4ejgcnLzmDCDYdy/xm1i8wTMIHUtSWRliYc1q9tnYIM5tZidrhwVHXXDb8/aPOPUR9dfnAvlt5zPE2zYw9Py/Y5NWuVjN6WRHU5KuvpuPYjyTXUMYZNBO4FjhKRhcBRdr3WlFdqnTWjw+GoO90L8mqE7PDyyTUH8dHvDqy2LVC517UlEYvQw4rA8G4tGX/9IeTnZNbr2MGWREjlu/Du48y57Xpo5TzzjqNrHMuvkhMRzhnZJex2gNE9W1fb7r38UF0QaEkkogusN4bNHzBzW19Sl5Oo6gRMLyZUdRNxsPtWVFbWOfStw+FIHP3bN6uxrcrcFN9zeevcowe2498TFnNovzb85tBedQ5tHvFcIeuB3kwtcjNZv6OkRuVcX+V04uCOvDppRbVt3nEhXqLpXhtcNiGD6TJsl7zhIiKqCfoEqCPlFeoc1w5HIyHYBTaB1cg+XVvWmDkuHgQd19Zwcd6orkxaWuW6evPK0Xy1YAPLNxeFyV13DuhdwLJ7T+Af4xayYP0OPvppTTQpg0uhijgQDqSuLYlo9ppJ9j8Qw+YCETkt8KvT2eJIeaW64H4ORyOhvr2bYpFI5RMY79GhuXE4333q3nx23SHB/d1a53HB6O7VKudXfhUu5Fbd+P2RfRjVw8ztMaRLCzq1yOX6o/tWS+M9d+i9qKhnCCM/Af7qGsMmoZRXui6wDkcyGXfdwazeWuwr7QmDO/C/WWv5/ZF94ipDpB5H8aRPu3z+cfbQGgEMQ9nlDcoXZ52VZ+fN6dgih29uqtm51KsXarQkEjiYrr4xbBKK693kcCSX3m3z6e1zIFp+TibP/3Jk7IQpypihscde7CoNH7k1HpyyTye27irjvFHhAzOqp0oObUkE5txJRBTY+sawSSgVbpyEw+GwJL1CorqSiDVYrrakpwmXHtgj4v7KMC2JvTs150/HDyA7M43T/vUtB/oY2xKOaEqiXjFsEk2Z6wLrcDhS6DsxYG76x9lD464kYuFtPQTMS7lZ6YzuZbrK1sehH62WTaHbX5OKykoynbnJ4dijOXO4maCvZ0HNyXYamkBLom1++NAh8eaJC6qi6Fb3SdjeTHGqHqMpiZSOYeO6wDpSFRE5VkTmi8giGw4/dL+IyCN2/08iMixWXhFpJSKfichC+9/Ss++PNv18ETkm8VeYOpw5ogvL7j2Bts0apmKORsAvECvCa7w4ZlB73rTTxY7q2Sq4PRCw8IL9usflPNFChad0DJth3VrGTVM6HPHCDkB9DBNNYCUwWUQ+UNU5nmTHYeZ96IOZT+LfwKgYecPOwyIiAzFhbwYBHYFxItLXztPtaED+ftZQXv5heXBK0oZg3+6tapiS2uRnx3W8iJ8usCnJjceGnzLR4UgyI4FFqroEwE4sNAbwKokxwAt2gOr3ItLCBrvsHiVvpHlYxgCv2RnqlorIIivDdwm8RkcYurRqwk3H7X71kvP8OhzxpRPgjaWw0m7zkyZa3mrzsFAVZNPP+YDY4fcdjnA4JeFwxBc/XcYjpalLd3PfeWKF33c4wtEozE1Tp07dKCI/h9lVAGxsaHkikCqypIockFhZuiXouPVlJeAN39kZCJ24OVKarCh514lIBzubo3ceFj/nq0EjKFOpIgfsObKEL1Oq2mh/wJRky5BqsqSKHKkmSwNecwawBOiBqfRnAINC0pwA/A/TCtgPmBQrL/A34Ca7fBNwv10eZNNl23xLgPTG/sxSRQ4nizaOloTD0VhQEzn5amAsJmrBM6o6W0SutPsfx8xhfTywCCjChuCPlNce+l7gDRG5FFgOnGnzzBaRNzDO7XLgKnU9mxxxxCkJhyPOqOrHGEXg3fa4Z1mBq/zmtdsjzsOiqncDd9dDZIcjIo3dcf1ksgXwkCqypIockFqyOPyRKs8sVeSAPVwWsXYuh8PhcDhq0NhbEg6Hw+FIII1SScSKjZOA8z0jIutFZJZnW4PH0hGRLiLyhYjMFZHZIvL7JMqSIyKTRGSGleXOZMniqD+uTLkyFZFkd+mqQxewdGAx0JOqboIDE3zOg4FhwCzPtvup3iXxPrs8kOpdEhdTjy6JIXJ0AIbZ5XxggT1fMmQRoKldzgR+wHTnbHBZ3K/ez9KVKXVlKtKvMbYkgrFxVLUUCMS3SRiq+hUQGvBwDCaGDvb/FM/211S1RFWXYro5xmVKLlVdo6rT7PIOYC4mBEMyZFFV3WlXM+1PkyGLo964MoUrU5FojErCd6yaBFPvWDr1QUS6A/tgvjaSIouIpIvIdMzo389UNWmyOOpFqjwbV6ZSsEw1RiWRktOpeki4fCLSFHgbuEZVtydLFlWtUNWhmFAQI0Vkr2TJ4qgXqf5sXJlKgiwBGqOSqFOsmgSwzsbQIR6xdPwiIpmYl/llVX0nmbIEUNWtmNDVxyZbFkedSJVn48qUJZXKVGNUEpOBPiLSQ0SyMBOufJAEOT4ALrLLFwHve7afLSLZItIDM7HMpHicUEQEeBqYq6p/T7IsbUSkhV3OBY4E5iVDFke9cWXKlanIxNsT3hA/TNybBRhv/s0NcL5XgTVAGUZ7Xwq0BsYDC+1/K0/6m61s84Hj4ijHgZjm5E/AdPs7PkmyDAZ+tLLMAm6z2xtcFveLy/N0ZcqVqbA/N+La4XA4HBFpjOYmh8PhcDQQTkk4HA6HIyJOSTgcDocjIk5JOBwOhyMiTkk4HA6HIyJOSexmiMihIvLfZMvhcOwOuPLklITD4XA4ouCURJIQkfNt7PjpIvKEDey1U0QeFJFpIjJeRNrYtENF5HsR+UlE3g3EkxeR3iIyzsafnyYivezhm4rIWyIyT0RetqNKHY7dFleeEodTEklARAYAvwAOUBPMqwI4D8gDpqnqMOBL4Hab5QXgRlUdDMz0bH8ZeExVhwD7Y0awgolkeQ0m3nxP4IAEX5LDkTRceUosGckWYA/lCGA4MNl+lORignZVAq/bNC8B74hIc6CFqn5ptz8PvCki+UAnVX0XQFWLAezxJqnqSrs+HegOTEz4VTkcycGVpwTilERyEOB5Vf1jtY0it4akixYzJVqTt8SzXIF7zo7dG1eeEogzNyWH8cAZItIWgnPYdsM8jzNsmnOBiaq6DdgiIgfZ7RcAX6qJeb9SRE6xx8gWkSYNeREOR4rgylMC2aM0YqqgqnNE5BbgUxFJw0TCvAooBAaJyFRgG8bOCiY88OP2pV0CXGK3XwA8ISJ32WOc2YCX4XCkBK48JRYXBTaFEJGdqto02XI4HLsDrjzFB2ducjgcDkdEXEvC4XA4HBFxLQmHw+FwRMQpCYfD4XBExCkJh8PhcETEKQmHw+FwRMQpCYfD4XBExCkJh8PhcETEKQmHw+FwRMQpCYfD4XBExCkJh8PhcETEKQmHw+FwRMQpCYfD4XBExCkJh8PhcETEKQmHw+FwRMQpCYfD4XBExCkJh8PhcETEKQmHw+FwRMQpCYfD4XBExCkJh8PhcETEKQlHyiEi3UVERSQj2bI44o97vnXD3rPeDX3epCgJEdnp+VWKyC7P+nl1ON4EEbksEbI6HLs7rjw6opEUTa6qTQPLIrIMuExVxyVDloZARDJUtTzZcjgc4djTyqOjdqSUuUlE0kTkJhFZLCKbROQNEWll9+WIyEt2+1YRmSwi7UTkbuAg4FH75fNohGO/KSJrRWSbiHwlIoM8+3JF5EER+dnunygiuXbfgSLyrT3nChG52G6v9rUkIheLyETPuorIVSKyEFhot/3DHmO7iEwVkYM86dNF5E/22nfY/V1E5DEReTDkWj4UkWvCXOPjIvJAyLb3ReQ6u3yjiKyyx58vIkf4fC4dReRtEdkgIktF5HeefXeIyFsi8ro97jQRGeLZP8Deq60iMltETvZz3y3nichyEdkoIjd78tX6PfFznY7qJLI8hpyno4h8ICKbRWSRiPzKs2+kiEyxZWadiPw92vnDHPsmEXkrZNs/ROQRu3yxiCyx7+5S8dlyEpHmIvK0iKyxZeovIpLuOeY3IvJP+17P85a1GNcbth7wnPpIEVkoIlts3SCevL8Ukbl231gR6Wa3i4g8JCLrrTw/ichefq4TAFVN6g9YBhxpl68Bvgc6A9nAE8Crdt8VwIdAEyAdGA40s/smYL5+op3nl0C+Pe7DwHTPvsfsMTrZY+9v03UFdgDnAJlAa2BouHMCFwMTPesKfAa0AnLttvPtMTKA64G1QI7d9wdgJtAPEGCITTsSWA2k2XQFQBHQLsw1HgysAMSutwR2AR3tcVcAHe2+7kAvH88nDZgK3AZkAT2BJcAxdv8dQBlwhr1HNwBL7XImsAj4k817uL2f/WLc9+72/v0HyLX3ogQYUJ/3xP1Sozx6nm+GXf8S+BeQAwwFNgBH2H3fARfY5abAfrV5zkA3W14CsqUDa4D9gDxgu+d97AAM8nmf3rP3Iw9oC0wCrvDUBeXAtbYM/ALYBrTycb1h6wFPnfJfoAWmbtoAHGv3nYIpawMw9cstwLd23zGYMtzCHnMA0MH3O5FiL+XcwM3yPLQye9G/BL4FBoc5RtSXMkz6FvaGN8dUgruAIWHS/RF4N8Ixqp2T8Eri8BhybAmcF5gPjImQbi5wlF2+Gvg4QjoBlgMH2/VfAZ/b5d7AeuBIILMW92oUsDzMfXnWLt8BfO/Zl4YphAfZ31qsgrP7X7V5ot337vb+dfZsmwScXZ/3xP18Pe+El0fP880AugAVQL5n/z3Ac3b5K+BOoCDkGL6fMzARuNAuHwUstst5wFbgdOyHnM971A7z0ZLr2XYO8IVdvhjzYSch7+8FPq43Wj2gwIGe9TeAm+zy/4BLPfvSMMqxG+bjbAFGMab5vc7AL6XMTZgLetc2H7diXtIKzEN5ERgLvCYiq0XkfhHJ9HNQ24S71zbhtmMKApiv8gKMRl8cJmuXCNv9siJEjuttc3Cbvb7m9vyxzvU8phWC/X8xXCI1b8drmBcW4FzgZbtvEebL8A5gvYi8JiIdfVxDN6Bj4JlYuf+EeSY1rlNVK4GVmNZLR2CF3RbgZ0zLIdp9D7DWs1yE+ZIMyBT398RRg4a4zx2Bzaq6w7Mt8I4AXAr0BeZZk9KJdnttzv8K1cvEKwCqWoj5yr8SWCMiH4lIfx8yd8O0ENZ47s0TmBZFgFW2PHqvKVAmol1vrDonWpn4h0eezZiPxk6q+jnwKKblvk5EnhSRZj6uE0gxnwSmsjlOVVt4fjmqukpVy1T1TlUdiDFLnAhcaPNpxCMazgXGYL6im2O+ZMDcxI1AMdArgjzhtgMUYpq6AdqHSROUS4z/4UbgLKClqrbANEEDNsVo53oJGCPG1j8A09SNxKvAGdYeOQp4OyiM6iuqeiDmhVLgvijHCbACWBryTPJV9XhPmqDNVETSMOaJ1fbXxW4L0BVYRfT77kemurwnjtqRqPLoZTXQSkTyPdsC7wiqulBVz8FUwPcBb4lIXi2f85vAoSLSGTgVqyTs8ceq6lGYVtI8jIkzFiswLYkCz31ppqqDPGk6ef0F9poCZSLi9RK9Hogl0xUhzypXVb+11/mIqg4HBmGU7h/8HjjVlMTjwN0eh0sbERljlw8Tkb2tc2g7ptlbYfOtw9jKI5GPeaibMBX7XwM77FfuM8DfrUMpXURGi0g25iv8SBE5S0QyRKS1iAy1WacDp4lIEzF9ly+NcW35GDvlBiBDRG4DvNr8KeDPItLHOpoGi0hrK+NKYDLm6+ltVd0V6SSq+qM9x1PAWFXdCiAi/UTkcHtdxRhTT0Wk43iYBGwX4/TOtfdnLxHZ15NmuIicJqbf+zWYe/098ANGmf6fiGSKyKHAScBrMe57LOr6njhqR6LKYxBVXYExG90jxhk9GFOWXrbnOV9E2tj3ZavNVlGb56yqGzAmsGcxHzxz7bHbicjJIpKHeWd3RjpGyPHWAJ8CD4pIMzEO/l4icognWVvgd/a9PxPzcfdxrOslSj0Qg8eBP4rtkCPGsX6mXd5XREbZllYhpvz7LxO1tU/F+0d1G2gacB3GLrcD0+z6q1bZ/Obbi1wHPEKV42s0xua2BXgkzDmaAu/bY/6M+eJQoLfdn4txZq/CfN1/RZWz+SBMZbcdo60vstsLMC/KDuAbjBkn1CfR27OeDjxtj7MG+L+Qa0/HOJuW2mNOprpN/nx7zMN83NNbbdozPdsGYyr8HZim6H+pcmKfB8yOcryOmBbKWnuPv/fIfQfwFvC6PfaPwDBP3kEYR902YA5wqmdf2PtOiGPTpp2AtXPX9T1xv5Qpj9WeL6bl+V/7Xi4GrvSkfQnjS9sJzAZOqctzxvgDFPiDZ1sHz7u51b5jA7Wq3O+McrzmwL8xptVt9r0P+MwuxtQJj9p9C4CjPXmjXW/EeoCadcpzwF9CrnEmVXXVM3b7EcBP9h5uxCikpn7fiUAvGEeKIyIHYwpMd61u408qInIH5sU9P1Zah2NPQEw3+cvUmHYbPalmbnKEwTYTfw88lUoKwuFw7P44JZHiiMgATFO4A8Y043A4HA2GMzc5HA6HIyKuJeFwOByOiDgl4XA4HI6INIp47gUFBdq9e/dki+FIIaZOnbpRVdskW47GiitTjlAilalGoSS6d+/OlClTki2GI4UQkZ+TLUNjxpUpRyiRypQzNzmqUVEZHJjDuu3FrN9RTEVlw3ZuWLBuB29NXcmKzUW4jhUNw7aiMh77YhGzVm1LtiiOFKNRtCT2RFSV6qFf4s+2ojL++flCVm3dxeqtu1iyoZCS8kqa5WYytEsLxs1dB0D7ZjnccEw/DuxdQPvmOXGX49vFG7njg9ksWLezxr5zRnblntP2jvs5HdWRNPjb2PmoKnt1ap5scRwphFMSSWDttmLuHzuPLYWlfDF/A5npwtAuLchMT2PJhkLKKyvp1jqPa47sw4AOzSho6iecUWzMMHt4b/oqZq7axltTV7Kj2EyYl5eVTnF5JS2bZNGnbVOmr9gKQK82eSzeUMgNb84gPU3o07Yp1x7Vl2MGhYtn6I+lGwtJE7jypWms3FzEjpLqk/b1bJNH55ZNaJKZzklDOtT5PA7/NMvJpHvrJsxatT3ZojhSDKckGpBPZq3lywUbeHXS8mrbyyqUnSUVVFSWkZedzuINxWzcWcoFT08Kpvnw6gPZu3Pdv/BmrdrGif+cSNv8bNbvKAFABI7bqz1/On4AXVo1iZh36cZCHv18ER/OWM28tTu44sWp/O2MwRzWvy2Z6Wk0z/UXIXrumu08MHY+4+etD25rk58NAtce2ZceBXkM6dKCVnlZdb5OR93p0qoJa7cXJ1sMR4rhlESCKS6r4IMZq3lr6komLd0c3H7MoHacN6obvdo2pWWTTJpkVX8UE+av590fV/H+9NUAnPToRPbp2oL/XDiCjTtLaJefQ8sYlamqsqmwlL9+NJd3fjSRiAMK4pYTBnDy0I60zY9tPupRkMeDZw3hwbOGMH/tDq59fTp/eOsnAAZ0aMbfzhjM9W/M4Kx9u3DeqK7kZKYHfQkiQnFZBbe8N4u3pq6sdtw/Hd+fyw+uS1RkRyLISk+jtNxFfXFUxymJBDJx4Ub+NnYeM1ZWOQOfvWRfOrXIpW+7/Cg54dB+bTm0X1sKS8oZN9d8ef+4fCsj/lI1P/2EGw5lU2EpSzcW0qdtU/q2yyc3K50thaUs2VjI6f/+tsZxexTk8eqv9quzb6Ff+3ze+vVoBt42FjCtg9+99iNLNhTy5//O4c0pK+jZJo+PZ66119GGCfM3VDvG/acP5qx9u9Q4tiO5ZGemUVrhlISjOjGVhJ0J6uPdJbBcYUk5938yj0qF/Xu1prSiks/nrad1XjZHDmjL/r0LYh8kCv+buYYfV2zlya+WBLddf1RfLj6gOz9vKqq1U/Bf5w1n484SXp+8ghe+W8aWorLgvkMfmFAj/RnDO/PFvPVsKiwNbjuoTwGH9G3DoI7NGd3LT2j66IS2elZtqZreYt7aHcxbWzXpVqiCADh1WKca2xzJx7UkHOHw05I4GzMt3tuYeY3nJlimhPHW1JXc8OaM4PqL31fvFvzMN0tJE5hww2F0bR3ZRh+JR8Yv5O+fLai27faTBnLJAT0A6tRrJCsjjY4tcrn2qL5ce1RfXv7hZz6YvpofPKYrL6EmHYCnLhpBdkZ6rc/tl6cuGkHHFrlc89p0Znq6UAb8HyN7tGJwp+aUVyq3njiQ9LTE9tpy1I2sDKckHDWJqSRU9Xw7H+o5wLMiopgZnl7V6vO0pjSfzl5bTUH0a5fPMXu1J02gVV4W70xbxfQVW6lUuOqVaTx41hA6t8xlw44S8nMyyclMq/EFvX5HMQvW7qRzy1z+NWERb0ypWUEf1q9tjW314bxR3ThvVDcufnZS2K/0UHq1yUuIgjhyQNugGSzQA+vD3x5IcVkFV7w4lR4FeezXsxVXvjSNXm3yuOXEgXGXwRFfsjPSnbnJUQNfPglV3W5bErmY6SlPBf4gIo+o6j8TKF9c+OM7M4M9it66cjQjureqkebC0d1ZvXUX7/64ir+Nnc/RD31Vbf9BfQp48dJRlJRXUFahNM3O4JJnJzN79XYO7tuGrxaEr7C71aFF4odmOVU9io4e2I77Th/Me9NXceeHc+jUIpc7Tx7EqJ6tyM/x1/OotvzznGEsWLeDnSXl1bro5mSm8/wvRwJQVlHJbw/vzcX7d0+IDI74kpWRRkmZm+nVUR0/PomTgF9iJud+ERipqutFpAkwF0hpJaGqQQXx7MX7hlUQATq2yOW8UV3529j5NfZ9vXAj4+as4x/jFzJz1bZqTfP1YboN/uPsoQzr2jJhA+JuO2kgXVs14erDe5OVnkZamnB4/7bc+eEcfn1oL44c2C4h5w2Qm5XOkC4toqbJTE/j+qP7JVQOR/zIynCOa0dN/LQkzgQeUtVqn9aqWiQiv0yMWPHhsS8WBSv8I/q35dB+sePBtWiSxYQbDg3rFL7shapYN17brddRC5Cbmc6YoYl1zhY0zeaGY6pXwN1a5zH9tqNo0cSNM3DUnuyMNMoqlMpKJc35jRwWP7GbbgeCo7pEJFdEugOo6vgEyVVvlm0srNYiuPmEAb6/6rsX5PHsxfvW6nzH7dWeL244FIAbj03e17NTEI66kpVhqgPXmnB48aMk3gS8b02F3ZbSHP7ghODyGcM707NN01rlP6x/W5bdewJt82OHxEhPE84c0ZkeBXksu/cELra9mRyOxkRWuqkOSlwPJ4cHP0oiQ1WDne7tckp/ri7dWIg3cOlfTtmrzscKND7aeJTFZQdWVwLz/nwsh/dPrA/A4Ug02ZmmF5zrBuvw4kdJbBCRkwMrIjIG2Jg4kerPFzY20C0nDGDWnceQk1n/LqDv/mZ/DupjBtr1blvVKnnp0lFkpruI647GT3a6Mzc5auLHcX0l8LKIPAoIsAK4MKFS1YMF63Zw13/nkJOZxmUH9az38drm57BuewlZ6WlkWGdewO5/YO8CDuxTvxHaDkeqkJFu3u9ypyQcHvwMplsM7CciTQFJ9QF0z3+7DIDj945PiOn/XDiCcXPX0bZZTnCksAjMuP1ocjJdC8Kx+5BhWxJlFW6iJ0cVvgbTicgJwCAgJ9BDSFXvSqBcdeLvn87n5R+W07ddU+4+JT4T1bRvnsP5+3UDIM1ee0Wl+g6P7XA0FrJsS6LMtSQcHmJ+CovI48AvgN9izE1nAt0SLFetqahUHvl8EQBDOrcgNyv+oSiaZhud6rqQO8IhIr1EJNsuHyoivxORFkkWyzcZaaY6KHctCYcHP/aS/VX1QmCLqt4JjAZSLs6zd7KUXm1r193VL7ecOJDLD+7JEQNcTyZHWN4GKkSkN/A00AN4Jbki+SfTjZNwhMGPkgjUvkUi0hEow7z8KcUCz6jn/u2jz9VQV1rlZfGn4we43kyOSFSqajkmttnDqnot0GjmX81Mc45rR0381HYf2ibz34BpwDLg1QTKVGuWbyrikucmA3DFwT05pG/s8BsORwIoE5FzgIuA/9ptvpxXInKsiMwXkUUiclOY/f1F5DsRKRGRG2qT1y/Oce0IR1QlISJpwHhV3aqqb2N8Ef1V9TY/BxeRa0VktojMEpFXRSRHRFqJyGcistD+t6zvRfy4Yktw+fz9uiUsqJ7DEYNLMObYu1V1qYj0AF6KlUlE0oHHgOOAgcA5IhIaW30z8DvggTrk9UVmwHFd6VoSjiqiKgk7G92DnvUSVd0WJUsQEemEealHqOpeQDpmAqObMIqnDzDerteLjTurZmHLz3EzsjqSg6rOUdXfqeqr9uMnX1Xv9ZF1JLBIVZfYiAavAWNCjr1eVSdjzL21yuuXgBm1zI24dnjwY276VEROl7p9nmcAuSKSATQBVmNe4Oft/ueBU+pw3GpsLiwJLgd6IDkcDY2ITBCRZiLSCpiBmaTr7z6ydsIMUg2w0m7zg++8InK5iEwRkSkbNtSc/ySgJMornbnJUYUfJXEdJqBfiYhsF5EdIrI9ViZVXYVpGi8H1gDbVPVToJ2qrrFp1gD1nrptk6clkeGcyo7k0VxVtwOnYab6HQ4c6SNfuA8wvzW177yq+qSqjlDVEW3a1PTbZbhxEo4wxKxRVTVfVdNUNUtVm9n1ZrHy2eb2GExPqI5Anoic71ewWF89XmavjqmzHI6GIENEOgBnUeW49sNKqncr74xpdSc6bzUy05zj2lETPzPTHRxue+gkRGE4Eliqqhvscd4B9gfWiUgHVV1jC9T6CMd/EngSYMSIERHf2opKZfbqbVx6YA9+d0SfWJfjcCSSu4CxwDeqOllEegILfeSbDPSxju5VGN/duT7PWZ+81cjMcF1gHTXxY8D/g2c5B+MomwocHiPfckzMpybALuAIYApQiOkieK/9f7+WMldj264yKhU6t8x1oTIcSUVV38Qz14qqLgFO95GvXESuxiiYdOAZVZ0tIlfa/Y+LSHtM+WkGVIrINcBAO/98jbx1kT8j2JJwSsJRhZ8Afyd510WkC3C/j3w/iMhbmLEV5cCPmJZBU+ANEbkUo0jOrIPcQbYUGX9ESzcjmyPJiEhnzJzvB2D8AhOB36vqylh5VfVj4OOQbY97ltdiTEm+8taFLDdOwhGGunQFWgn4msVHVW/HTH/qpQTTqogLW62SaNHEtSIcSedZTBiOwIfP+XbbUUmTqBYEzE1uZjqHFz8+iX9S1VsiDRiK6d6XEqzeaqKGuJaEIwVoo6rPetafs2ahRkFuZjrpacLOktChGI49GT/9RadgfBBTge+AG1XVdy+lRDN29lryczKqzRbncCSJjSJyvoik29/5wKZkC+UXEaGiUnnsi8VUuLESDosfc9NbQLGqVoAJAyAiTVS1KLGi+WNzYSn92+eT5wbROZLPL4FHgYcwre9v7bZGx+Rlm9mvZ+tki+FIAfzUrOMx3Vl32vVc4FNMd9aks7OknFZ5ztTkSD6quhw4OWbCRkBWhhuU6jD4URI5qhpQEKjqTtutNSXYWVxO11YpI45jDyTEb1cDVf1dA4oTF4rLKpItgiNF8KMkCkVkmKpOAxCR4ZhxDynBzpJyF9TPkWymJFuAeHHtkX15aNwCdpU6JeEw+KldrwHeFJHAUP8OmOlMU4KdJeXkZTkl4Ugeqvp87FSNg+P3bs9D4xZQ5JSEw+JnMN1kEekP9MMEE5unqinRR66iUikqrXBOa4cjTgTmht/lzE0OS0zvlIhcBeSp6ixVnQk0FZHfJF602CzfbDpYdWqRm2RJHI7dg9xMqyRcS8Jh8dOF4VequjWwoqpbgF8lTKJaMHHRRgAGdowZlNbhSDgicoCfbalME2u6deYmRwA/SiLNO+GQnS4xJfqcfjZnHZ1a5DKgg1MSjpTgnz63pSw5mWnkZKaxcWdJ7MSOPQI/xvyxmIB8j2O6+V0JfJJQqXxSUlZBp5a5pKe5Oa0dyUNERmPGDbURkes8u5phIrM2GkSE7q3zWLqxMNmiOFIEP0riRuAK4NcYx/WnwFOJFMov5ZUatKE6HEkkCxPdOAPI92zfDpyRFInqQe+2Tflx+dZki+FIEfz0bqoE/m1/KUVZRSXN3BgJR5JR1S+BL0XkOVX9GUBE0oCmdjrTRsWIbi35709rWLR+J91bN3FTAu/h+Ond1EdE3hKROSKyJPBrCOFiUVpe6V5gRypxj4g0E5E8YA4wX0T+ECtTqjGwY3MAjvz7l1z1yrQkS+NINn5q2GcxrYhy4DDgBeDFRArll/JKDU6U4nCkAANty+EUzCRAXYELkipRHfCGuRk7e10SJXGkAn5q2FxVHQ+Iqv6sqncQe+rSBqGsopKMdOe0dqQMmSKSiVES79tBp40u5nbb/Oxki+BIIfwY9IutfXWhnUt3FdA2sWL5o6y8kkzXknCkDk8AyzCTcn0lIt0wzutGRZrrLejw4KeGvQZoAvwOGI6ZkvGiBMrkm7JKdUrCkTKo6iOq2klVj1fDzxgTrcPRaPEVu8ku7gQuSaw4taOsopJMZ25ypAgi0g74K9BRVY8TkYHAaODp5EpWd3Iy3UfYnk6jfgPKK1xLwpFSPIcZfNrRri/AtMQbLa3znH9iT6dR17ClFc4n4Ug+IhJokReo6htAJYCqlgONOghSQdOUiMDjSCJ+xkmkbNAyZ25ypAiT7H+hiLTG9mgSkf2AbUmTqh5cvH93AGas3MbXCzdQXlGZXIEcScPPZ3hKBi2rqFRUcS0JRyoQ+FK5DvgA6CUi32DGFP02aVLVgztOHhRcvuDpSfxv1tqEnOdfExZx7evTE3JsR3yI6LhO9aBlZfbLxo2TcKQA3jLyLmYgnQAlwJHAT8kSLF5s3RW/ecbembaSjPQ0Th7Skfs/mQ/AQ78YGrfjO+JLtM/w0KBlgZ/voGUi0sKG9JgnInNFZLSItBKRz0Rkof1vWRfBS62ScCOuHSlAOqas5AN5mDKTjuk6nh8lXxAROVZE5ovIIhG5Kcx+EZFH7P6fRGSYZ98yEZkpItNFJCHzbd/63qzgh1l9ue6NGfzu1R/jcixH4onYkohT0LJ/AJ+o6hkikoUpNH8CxqvqvbYw3ISJNFsrysrNC+vMTY4UYI2q3lXXzHaOlseAo4CVwGQR+UBV53iSHQf0sb9RmFA5ozz7D1PVjXWVIRzdWjfh501FwfUlGwrp196Xzqs1qoqIsGJzET8s3cwZwzsn5DyO2uOnhq1T0DIRaQYcjO0jrqqldoa7MUBg4vjnMSEMak2xVRKuH7cjBaivzXMksEhVl6hqKfAappx4GQO8YAfpfQ+0EJEO9TxvVN759f787/cHBdd/++o0KisTE2Vk5qpt9Lvlfxx0/xfc8OYMit0c2ymDnxq2rkHLegIbgGdF5EcRecoqmnaqugbA/ocN8SEil4vIFBGZsmHDhhr7S+xLlJ2RdPeIw3FEPfN3AlZ41lfabX7TKPCpiEwVkcvrKUuQ1k2zGdChGb3bNgVgwbqdbC4qjdfhq/WYevDTBZSUV60/MHY+qo0u7NVuiR8lUdegZRnAMODfqroPUIgxLflCVZ9U1RGqOqJNmzY19pe4loQjRVDVzfU8RLiWSGgZi5bmAFUdhjFJXSUiB4c9SYwPr0iMu+4Q/nWecYG8PnkFH89c4ztvNWFVee6bpcH1fe8eF1z+ckF1eZ6auJTZq7fz3DdL+ef4hUlTGEWl5cxatW2PVlh+athA0LI8ahe0bCWwUlV/sOtvYZTGukAz2f6vr63QUKUkXEvCsRuwEujiWe8MrPabRlUD/+sxvatGhjtJrA+vaLSxkWH/NnY+v3l5Gif9cyLfLq6dC+SrhRu548MqN8uWoug9pk7850Tu+HAOD362gJ9WNtxwk50l5cHlp79eyon/nMg3izY12Pn9snZbcYOMX4mpJOoatExV1wIrRKSf3XQExqfxAVUBAi8C3q+L4FXmJteScDR6JgN9RKSH7eBxNqacePkAuND2ctoP2Kaqa0QkT0TyAaw592hgVrwFbJVXfeT1zFXbOPc/P0RIHZ6t9TBVjXnsG979cSWrt+6q8zH88L+Za9jr9rHMWb2dwx6YwIOfLQBg8YadCT2vXyorlV2lFWwrKmO/e8Zz98dzE35OPyOu24nI0yLyP7s+EP9RYH8LvCwiPwFDMcHP7gWOEpGFmN4c99ZF8IDjOtuZmxyNHBu+42pM3Ke5wBuqOltErhSRK22yj4ElwCLgP8Bv7PZ2wEQRmYEZ+f2Rqn4Sbxl7tWka9oPsu8XmC/vg+7/g96+F79b63DdLOevx75j285Z6yXDt6zM4+P4v6nWMSMxevY3pK7by1tSVADzx1WKWbiwM7t9SVIqqsq2ojM2Fpdz41k8UlZZHOlxc2LarjPKKSqYt38L67cVMWrqZ+8fOZ8Btn7Bii+l19uV8/2bDuuJnPonnMLPT3WzXFwCv4yOypapOB0aE2VVfR59zXDt2K1T1Y4wi8G573LOswFVh8i0BhiRcQOD7Px7Bf75ewr8mLA5ue3PqCkb3as3yzUUs31zEQ2cN5aFxC2idl0XTnEzOGN45aGKatKym66ZXmzwqlWoVcjTKE9C7amdJOSc8MhGAfbq2AOD96dWtfbNWbeeiZyfz1YINHNqvDRPmb2Bgx2YM79aSVnlZdGyRGxdZVJX//rSG8spKrn19RsR0c9cYi3+T7MTXf9FGXGfYL5wCVX1DRP4I5qtHRJLeP805rh2OhqVlXhYXjO5WTUm8M20VG3dWmZEWbdjJPz9fFFw/aUjkXrr5ORm8e9UB5Gdn8P2SzXRskcO05Vvo3Safkx6dWC3djuKqr/bb35/FFYf0Yvy89VywX7d6XdOUZZs54/Hvgus/Lt8aNt24uVXTuE6wX+9FpRWc+E8j57J7T6iXHMVlFRz10Jes2OzPnPaHt8wg/lmrtlNZqQmdKCpaDZvSQcuc49rhaHha5Fb5Jnq1yQPgK0/PpO0h4Tv63RLe8nXpgT346LcH0SwnExFhdK/WdGudx6n7dGbvzs2ZcfvR/OWUvQD44OoDq+V9/ruf2f/ez7n1vVms315c62sYfMdY7v5oDj+t3FpNQXhpkhW7Xrnvk3m1Pnck+t/6iW8FEcqXCxNrcoqmJFI6aFmxc1w7HA1OblY6M247msV/PZ7FG2qaiCJVuqHceuJAurZuEnF/89xMzt+vG8vuPYHOLSObck557Bu2hSimRet3ROyyWlGpbC8u5z9fL+W0f30bNs2p+3Rizl3HcvVhvQH44U+xreNbi0q5+NlJzPTZC2vjzhLu/mgOJeV1M8p08pi3Lnl2MnNWb+ecJ79nzbZdVMTZJBfNJ5HSQct2lZqb2yTbj1vF4XDEi+ZNMmud56PfHciu0gpyMtPJyaxd6z809M51R/Xl77bX0eptxQy581MO6lNAv3b5DOvWkt+8PI1HztmHWau20SwngyMGtKO4rIILn5nErScMDB4n4N9onZfFi5eO4hdPfseO4nJO3ceMUbzhmH7ccEw//PDBjNVMmL+B4rIKXrt8NAATF25k1uptXHlIL3YUl7FkQyFDurTgya8W89ePTStk+eYihnRpEfG4D5w5hBverOmb+Oamw+l+00fB9eMf+RqA0fd8zkF9Crjv9MHsf+/nAPz5lL3qZZaLVsMGgpaFGrsiq/8GpND2LMit5QvncDjiwzMXj+CXz1WPJ9ivXT7z1+0A4MzhnXnT9hYa0L5Zvezmc+46BlWoVCU/J5PisopqvpGvF27k64UbYaIZrHfHB7PZXGh8JQ98uiCY7v/ervltO6RLCwZ2bMbMO46JKcfpwzrz9rSVNbbf9v5swIxduOW9mfzqoJ6c/7TpItw2P5uJizbyzrRVPHfJvkEFATB29jrGzl4XTHfTcf3pXpDHY58v4uyRXTlqYDt+WLIpeB+7t27Cv88fHlXGrxduDCoIMMEZTxrcgRZN6jaBVDQlUa+gZYmmqLSC3Mx00hPosHE4HJE5pG/NiDpn7duF2au3MXbWWv7v2P40y83k83nr6+1YbZJVvar6v2P7c/3R/Tjrie+YGqZrbUBB+KGyFqOpY13Gsk1FLNu0nJe+Xx7cdt0bMxjdszUAFz87OWLe5y4ZycCOzQB4+uJ9g9sP6F0QVBInDO7AgA4mzRtXjOasJ/yZ9x76bAGdWuayYvMurj68N+2a5fjKB9GVRErXvoUl5eQ1QPcvh8MRnvQ04eiB7fh0jvkSHnfdIUFn9r2nDSYrI41bTxzIrScOjHaYep2/Ph+JfzllLz6csZqbjx8QM+2M244GMfPYvDl1JbmZ6eyyftF/nD2U3782PWr+75bUHLE9snsrZq/eRqE1nQcURChjhnakZV4Wi9fv5NxRXavy92gV7FVVUan0+tPHYfODGe0e6GZ84uAOcVMS9R7LkEgKS8prfF04HI6G5ckLRzBv7Xam/bw1GAgQICujYb4xAw7qB88cwoyVW3nhu58jpj1+7/bcftIg/vDWT3y1YAPn79eN833a6r1+mEDF/M2ijXy3eBMnDe7I2m3F7N2pOec+5W8UeprA4xcMp0lWOv1v/YQTBkfuKiwiHNK3DYf0jRxKJT1N+P0RfVi0fidnDO/MJc9Vb7F4x6F4n5Mfos0nUd+gZQmlsLTCVzc1h8ORWPq3b0b/9uG/ghPNacM6M3nZFg7oXcBJQzrSu23ToH/gw6sPZNXWIq58aRotm2Ry7+mDaZaTyZMXDKewpP6jpQ/oXcABvQsAuMI6pwFaNMnkb2cM4VcvVPfXeMd7XHtk32Cok2m3HhWXuuzao/oCxi8CJpTKy5eNoqS8klMe+yaYrnXT7Fodt9F+iheVlpPnejY5HHs0Z+/bhdOGdQqOl7pwdHcO7F1A89xMWjfNZu/OzVl6z/GIVLVs6tLDyg/5OZnM+/OxZGekUVJeSUHTbC47qAfz1+7ghmP6sXDdDi5+djIDOjTjN7Z7LdSMi1Vf2jfPqXbN3rk5ptxyZK2P12hr2Y7Nc9381g7HHo6I1BhQ27NN0xppGoqA8snJTK9RIRc0zeLEwR246rDeCe9wE6oU7zt9b4Z3a0VBLVsR0IiVxN/ObJBwNQ6HwxEXsjPSefTcYbETJoBf7Ns1dqIIuOHKDofD4YiIUxIOh8PhiIg0hmn5RGQDEK5vWwFQu+mxEkeqyJIqckBiZemmqrWbXs0RpBGUqVSRA/YcWcKWqUahJCIhIlNUNdx8FQ1OqsiSKnJAasni8EeqPLNUkQOcLM7c5HA4HI6IOCXhcDgcjog0diXxZLIF8JAqsqSKHJBasjj8kSrPLFXkgD1clkbtk3A4HA5HYmnsLQmHw+FwJBCnJBwOh8MRkUapJETkWBGZLyKLROSmBjjfMyKyXkRmeba1EpHPRGSh/W/p2fdHK9t8EYk93ZV/ObqIyBciMldEZovI75MoS46ITBKRGVaWO5Mli6P+uDLlylREVLVR/TDTqi4GegJZwAxgYILPeTAwDJjl2XY/cJNdvgm4zy4PtDJlAz2srOlxkqMDMMwu5wML7PmSIYsATe1yJvADsF8yZHG/ej9LV6bUlalIv8bYkhgJLFLVJapaCrwGjEnkCVX1KyB0fo0xwPN2+XngFM/211S1RFWXAouszPGQY42qTrPLO4C5QKckyaKqutOuZtqfJkMWR71xZQpXpiLRGJVEJ2CFZ32l3dbQtFPVNWBeNCAw4W+DyCci3YF9MF8bSZFFRNJFZDqwHvhMVZMmi6NepMqzcWUqBctUY1QS4QKxp1I/3oTLJyJNgbeBa1R1e7JkUdUKVR0KdAZGisheyZLFUS9S/dm4MpUEWQI0RiWxEujiWe8MrE6CHOtEpAOA/V9vtydUPhHJxLzML6vqO8mUJYCqbgUmAMcmWxZHnUiVZ+PKlCWVylRjVBKTgT4i0kNEsoCzgQ+SIMcHwEV2+SLgfc/2s0UkW0R6AH2ASfE4oYgI8DQwV1X/nmRZ2ohIC7ucCxwJzEuGLI5648qUK1ORibcnvCF+wPGYXgiLgZsb4HyvAmuAMoz2vhRoDYwHFtr/Vp70N1vZ5gPHxVGOAzHNyZ+A6fZ3fJJkGQz8aGWZBdxmtze4LO4Xl+fpypQrU2F/LiyHw+FwOCLSGM1NDofD4WggnJJwOBwOR0ScknA4HA5HRJyScDgcDkdEnJJwOBwOR0ScktjNEJFDReS/yZbD4dgdcOXJKQmHw+FwRMEpiSQhIufb2PHTReQJG9hrp4g8KCLTRGS8iLSxaYeKyPci8pOIvBuIJy8ivUVknI0/P01EetnDNxWRt0Rknoi8bEeVOhy7La48JQ6nJJKAiAwAfgEcoCaYVwVwHpAHTFPVYcCXwO02ywvAjao6GJjp2f4y8JiqDgH2x4xgBRPJ8hpMvPmewAEJviSHI2m48pRYMpItwB7KEcBwYLL9KMnFBO2qBF63aV4C3hGR5kALVf3Sbn8eeFNE8oFOqvougKoWA9jjTVLVlXZ9OtAdmJjwq3I4koMrTwnEKYnkIMDzqvrHahtFbg1JFy1mSrQmb4lnuQL3nB27N648JRBnbkoO44EzRKQtBOew7YZ5HmfYNOcCE1V1G7BFRA6y2y8AvlQT836liJxij5EtIk0a8iIcjhTBlacEskdpxFRBVeeIyC3ApyKShomEeRVQCAwSkanANoydFUx44MftS7sEuMRuvwB4QkTussc4swEvw+FICVx5SiwuCmwKISI7VbVpsuVwOHYHXHmKD87c5HA4HI6IuJaEw+FwOCLiWhIOh8PhiIhTEg6Hw+GIiFMSDofD4YiIUxIOh8PhiIhTEg6Hw+GIyP8DJ+0ooe1FLoEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model(train_loss_list,train_acc_list,test_loss_list,test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82018dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './project1_model.pt'\n",
    "torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project1_model import project1_model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = project1_model().to(device)\n",
    "model_path = './project1_model.pt'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
